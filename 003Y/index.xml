<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>1</fr:month>
      <fr:day>30</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/003Y/</fr:uri>
    <fr:display-uri>003Y</fr:display-uri>
    <fr:route>/003Y/</fr:route>
    <fr:title text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title>
  </fr:frontmatter>
  <fr:mainmatter>
    <html:p>
  Rough itinerary,
  <html:ul><html:li>Vanilla policy gradient
      <html:ul><html:li>Policy gradient theorem + proof</html:li>
          <html:li>Deterministic policy gradient theorem + (maybe)proof</html:li></html:ul></html:li>
      <html:li>Actor critic method
      <html:ul><html:li>A2C: Variance reduction method</html:li>
        <html:li>(Maybe) A3C: Asynchronous update</html:li></html:ul></html:li>
      <html:li>Trust region policy optimization</html:li>
      <html:li>Soft Actor Critic</html:li>
      <html:li><fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">Proximal Policy Optimization</fr:link></html:li>
      <html:li><fr:link href="/003X/" title="Group Relative Policy Optimization" uri="https://kellenkanarios.com/003X/" display-uri="003X" type="local">Group Relative Policy Optimization</fr:link></html:li></html:ul></html:p>
    <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="policy-gradient" theme="boxy-light" crossorigin="anonymous" async="" />
  </fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>John Schulman</fr:author>
              <fr:author>Filip Wolski</fr:author>
              <fr:author>Prafulla Dhariwal</fr:author>
              <fr:author>Alec Radford</fr:author>
              <fr:author>Oleg Klimov</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2017</fr:year>
              <fr:month>8</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/</fr:uri>
            <fr:display-uri>schulmanProximalPolicyOptimization2017</fr:display-uri>
            <fr:route>/schulmanProximalPolicyOptimization2017/</fr:route>
            <fr:title text="Proximal Policy Optimization Algorithms">Proximal Policy Optimization Algorithms</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="doi">10.48550/arXiv.1707.06347</fr:meta>
            <fr:meta name="external">https://arxiv.org/abs/1707.06347</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{schulmanProximalPolicyOptimization2017,
 title = {Proximal {{Policy Optimization Algorithms}}},
 author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
 year = {2017},
 doi = {10.48550/arXiv.1707.06347},
 urldate = {2025-06-27},
 number = {arXiv:1707.06347},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/UUPSNPZG/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
 primaryclass = {cs},
 eprint = {1707.06347},
 month = {August}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>6</fr:month>
              <fr:day>30</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0082/</fr:uri>
            <fr:display-uri>0082</fr:display-uri>
            <fr:route>/0082/</fr:route>
            <fr:title text="Evidence Lower Bound">Evidence Lower Bound</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><html:strong>Setup:</html:strong>
We are given a joint distribution <fr:tex display="inline"><![CDATA[p(X, Z; \theta )]]></fr:tex>, where <fr:tex display="inline"><![CDATA[X]]></fr:tex> and <fr:tex display="inline"><![CDATA[Z]]></fr:tex> are random variables with their joint parametrized by <fr:tex display="inline"><![CDATA[\theta ]]></fr:tex>. We are also given a dataset <fr:tex display="inline"><![CDATA[\mathcal {D}]]></fr:tex> containing realizations of <html:strong>only</html:strong> <fr:tex display="inline"><![CDATA[X]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[Z]]></fr:tex> is a <fr:link href="/0083/" title="Latent Variable" uri="https://kellenkanarios.com/0083/" display-uri="0083" type="local">latent random variable</fr:link>.
  </html:p>
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  "><html:strong>Goal:</html:strong> We want to learn <fr:tex display="inline"><![CDATA[p(x; \theta )]]></fr:tex>.</html:div>
<html:p> However, <fr:tex display="block"><![CDATA[p(x; \theta ) = \int  p(x, z; \theta ) \mathrm {d}z = \int  p(x | z; \theta )p(z) \mathrm {d}z,]]></fr:tex> which is computationally intractable for most continuous <fr:tex display="inline"><![CDATA[Z]]></fr:tex>.</html:p>
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  "><html:strong>Question:</html:strong> Can we update <fr:tex display="inline"><![CDATA[\theta ]]></fr:tex> for a given realization of <fr:tex display="inline"><![CDATA[x]]></fr:tex> and <fr:tex display="inline"><![CDATA[z]]></fr:tex>?</html:div>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  "><html:strong>Answer:</html:strong> Yes!!!</html:div>
<html:p><html:strong>Motivation:</html:strong> Suppose we had some <fr:tex display="inline"><![CDATA[q(z)]]></fr:tex> that we could sample realizations of <fr:tex display="inline"><![CDATA[Z]]></fr:tex> from. Then we want to find some reasonable <fr:tex display="inline"><![CDATA[\ell _{\theta }]]></fr:tex>, such that
<fr:tex display="block"><![CDATA[\theta ^* \approx  \arg \max _{\theta } \mathbb {E}_{x, \sim  p(x; \theta ^*), z \sim  q(z)}[\ell _{\theta }(X, Z)]]]></fr:tex>

    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>30</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
<fr:tex display="inline"><![CDATA[q(z)]]></fr:tex> can potentially (and will) depend on the sample <fr:tex display="inline"><![CDATA[x_i]]></fr:tex>. This is OK because we are given <fr:tex display="inline"><![CDATA[x_i]]></fr:tex>'s, which we can then just pass to <fr:tex display="inline"><![CDATA[q]]></fr:tex> to get the corresponding <fr:tex display="inline"><![CDATA[z_i]]></fr:tex>.
</fr:mainmatter></fr:tree>

Why is this the case? Because then (for well-behaved <fr:tex display="inline"><![CDATA[\ell _{\theta }]]></fr:tex>)
<fr:tex display="block"><![CDATA[\theta _{t + 1} = \theta _{t} - \alpha  \nabla _{\theta _{t}}\ell _{\theta _{t}}(x_i, z_i) \to  \theta ^*]]></fr:tex>
i.e. SGD is a reasonable thing to do, where the gradient is for a <html:strong>single</html:strong> sample of <fr:tex display="inline"><![CDATA[x_i]]></fr:tex> and <fr:tex display="inline"><![CDATA[z_i]]></fr:tex> which we have readily available from our dataset <fr:tex display="inline"><![CDATA[\mathcal {D}]]></fr:tex> and our distribution <fr:tex display="inline"><![CDATA[q]]></fr:tex> that we can sample from. This idea comes up a lot in optimization, such as the <fr:tex display="inline"><![CDATA[\log ]]></fr:tex> trick in the original <fr:link href="/003Y/" title="The History and Evolution of Policy Gradient Algorithms" uri="https://kellenkanarios.com/003Y/" display-uri="003Y" type="local">policy gradient</fr:link>.
</html:p><html:p>Similar to the <fr:tex display="inline"><![CDATA[\log ]]></fr:tex> trick, the <html:em>Evidence Lower Bound</html:em> (ELBO) comes from a simple multiply and divide trick. Namely,
<fr:tex display="block"><![CDATA[\begin {align*}
  \log  p(x; \theta ) &= \log  \int  p(x, z; \theta ) \mathrm {d}z \\
  &= \log  \int  p(x, z; \theta ) \frac {q(z)}{q(z)} \mathrm {d}z \\
  &= \log  \mathbb {E}_{z \sim  q(z)} \frac {p(x, Z; \theta )}{q(z)} \\
  &\geq  \mathbb {E}_{z \sim  q(z)} \log  \frac {p(x, Z; \theta )}{q(z)}
.\end {align*}]]></fr:tex>
The last inequality follows directly from <fr:link href="/002G/" title="Jensen's Inequality" uri="https://kellenkanarios.com/002G/" display-uri="002G" type="local">Jensen's Inequality</fr:link>. However, note that this is only a lower bound and therefore not exactly what we want. For example, if we were to pick <fr:tex display="inline"><![CDATA[q \sim  N(0, 1)]]></fr:tex> and then optimize the ELBO for <fr:tex display="inline"><![CDATA[\theta ]]></fr:tex> we would not necessarily find a good <fr:tex display="inline"><![CDATA[\theta ]]></fr:tex> for the likelihood <fr:tex display="inline"><![CDATA[\log  p(x; \theta )]]></fr:tex>. Luckily, we can actually get the difference <fr:tex display="inline"><![CDATA[\log  p(x; \theta ) - ]]></fr:tex>ELBO in closed form! Namely,
<fr:tex display="block"><![CDATA[\begin {align*}
  \log  p(x; \theta ) - \mathrm {ELBO} &= \log  p(x; \theta ) - \mathbb {E}_{z \sim  q(z)} \log  \frac {p(x, Z; \theta )}{q(z)} \\ 
&= \int  \left (\log  p(x; \theta ) - \log  \frac {p(x, z; \theta )}{q(z)}\right )q(z) \\ 
&= \int  \left (\log  \frac {p(x; \theta )q(z)}{p(x, z; \theta )}\right )q(z) \\ 
&= \int  \left (\log  \frac {p(x; \theta )q(z)}{p(z | x; \theta )p(x; \theta )}\right )q(z) \\ 
&= \int  \left (\log  \frac {q(z)}{p(z | x; \theta )}\right )q(z) \\ 
&= \mathrm {KL}\big (q(z) \mid \mid  p(z | x; \theta )\big )
.\end {align*}]]></fr:tex>
Thus, to tighten the lower bound we can optimize <fr:tex display="inline"><![CDATA[q_{\phi }(z)]]></fr:tex> to try to match <fr:tex display="inline"><![CDATA[p(z | x; \theta )]]></fr:tex>. As a sanity check, lets make sure these match when <fr:tex display="inline"><![CDATA[q(z) = p(z \mid  x; \theta )]]></fr:tex>. 
<fr:tex display="block"><![CDATA[\begin {align*}
  \mathbb {E}_{z \sim  q(z)} \log  \frac {p(x, Z; \theta )}{q(z)} &= \int  \log  \frac {p(x, z; \theta )}{q(z)}\mathrm {d}z \\
  &= \int  \log  \frac {p(x; \theta )p(z \mid  x; \theta )}{p(z \mid  x; \theta )}\mathrm {d}z \\
  &= \int  \log  p(x; \theta )\mathrm {d}z \\
  &= \log (x; \theta )
.\end {align*}]]></fr:tex>
You may be wondering how we use this algorithmically to find <fr:tex display="inline"><![CDATA[\theta ^*]]></fr:tex>? This will be the main idea of <fr:link href="/0088/" title="Expectation Maximization" uri="https://kellenkanarios.com/0088/" display-uri="0088" type="local">expectation maximization</fr:link>. Feel free to check it out!
</html:p></fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>6</fr:month>
              <fr:day>5</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/007M/</fr:uri>
            <fr:display-uri>007M</fr:display-uri>
            <fr:route>/007M/</fr:route>
            <fr:title text="Upcoming Blogs">Upcoming Blogs</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:ul><html:li><fr:link href="/0005/" title="Contrastive Reinforcement Learning" uri="https://kellenkanarios.com/0005/" display-uri="0005" type="local">Contrastive Reinforcement Learning</fr:link></html:li>
  <html:li><fr:link href="/003Y/" title="The History and Evolution of Policy Gradient Algorithms" uri="https://kellenkanarios.com/003Y/" display-uri="003Y" type="local">The History and Evolution of Policy Gradient Algorithms</fr:link></html:li>
  <html:li><fr:link href="/006Q/" title="A Dynamic Duo: Tree Search + RL" uri="https://kellenkanarios.com/006Q/" display-uri="006Q" type="local">A Dynamic Duo: Tree Search + RL</fr:link></html:li>
  <html:li><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link></html:li>
  <html:li><fr:link href="/007O/" title="RL as Probablistic Inference" uri="https://kellenkanarios.com/007O/" display-uri="007O" type="local">RL as Probablistic Inference</fr:link></html:li>
  <html:li><fr:link href="/007P/" title="Constrained MDPs?" uri="https://kellenkanarios.com/007P/" display-uri="007P" type="local">Constrained MDPs?</fr:link></html:li>
  <html:li><fr:link href="/006Q/" title="A Dynamic Duo: Tree Search + RL" uri="https://kellenkanarios.com/006Q/" display-uri="006Q" type="local">A Dynamic Duo: Tree Search + RL</fr:link></html:li></html:ul>
            <html:p>Request more below!</html:p>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="blog-requests" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>1</fr:month>
              <fr:day>30</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003X/</fr:uri>
            <fr:display-uri>003X</fr:display-uri>
            <fr:route>/003X/</fr:route>
            <fr:title text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link href="/shaoDeepSeekMathPushingLimits2024/" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" uri="https://kellenkanarios.com/shaoDeepSeekMathPushingLimits2024/" display-uri="shaoDeepSeekMathPushingLimits2024" type="local">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</fr:link></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:title text="Math">Math</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tex display="block"><![CDATA[
    \begin {align*}
    {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\
    &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )
    \end {align*}
  ]]></fr:tex>
  where
  <fr:tex display="block"><![CDATA[
        \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1
    ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
