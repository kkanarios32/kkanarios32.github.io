<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Matteo Hessel</fr:author>
      <fr:author>Manuel Kroiss</fr:author>
      <fr:author>Aidan Clark</fr:author>
      <fr:author>Iurii Kemaev</fr:author>
      <fr:author>John Quan</fr:author>
      <fr:author>Thomas Keck</fr:author>
      <fr:author>Fabio Viola</fr:author>
      <fr:author>Hado van Hasselt</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2021</fr:year>
      <fr:month>4</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/hesselPodracerArchitecturesScalable2021/</fr:uri>
    <fr:display-uri>hesselPodracerArchitecturesScalable2021</fr:display-uri>
    <fr:route>/hesselPodracerArchitecturesScalable2021/</fr:route>
    <fr:title text="Podracer architectures for scalable Reinforcement Learning">Podracer architectures for scalable Reinforcement Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2104.06272</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2104.06272</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{hesselPodracerArchitecturesScalable2021,
 title = {Podracer Architectures for Scalable {{Reinforcement Learning}}},
 author = {Hessel, Matteo and Kroiss, Manuel and Clark, Aidan and Kemaev, Iurii and Quan, John and Keck, Thomas and Viola, Fabio and van Hasselt, Hado},
 year = {2021},
 doi = {10.48550/arXiv.2104.06272},
 urldate = {2025-02-11},
 number = {arXiv:2104.06272},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/YM29JDFM/Hessel et al. - 2021 - Podracer architectures for scalable Reinforcement Learning.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Supporting state-of-the-art AI research requires balancing rapid prototyping, ease of use, and quick iteration, with the ability to deploy experiments at a scale traditionally associated with production systems.Deep learning frameworks such as TensorFlow, PyTorch and JAX allow users to transparently make use of accelerators, such as TPUs and GPUs, to offload the more computationally intensive parts of training and inference in modern deep learning systems. Popular training pipelines that use these frameworks for deep learning typically focus on (un-)supervised learning. How to best train reinforcement learning (RL) agents at scale is still an active research area. In this report we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way. Specifically we describe two architectures designed to make the best use of the resources available on a TPU Pod (a special configuration in a Google data center that features multiple TPU devices connected to each other by extremely low latency communication channels).},
 primaryclass = {cs},
 eprint = {2104.06272},
 month = {April}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
