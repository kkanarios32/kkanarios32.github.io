<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Matthew McLeod</fr:author>
      <fr:author>Chunlok Lo</fr:author>
      <fr:author>Matthew Schlegel</fr:author>
      <fr:author>Andrew Jacobsen</fr:author>
      <fr:author>Raksha Kumaraswamy</fr:author>
      <fr:author>Martha White</fr:author>
      <fr:author>Adam White</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2022</fr:year>
      <fr:month>2</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/mcleodContinualAuxiliaryTask2022/</fr:uri>
    <fr:display-uri>mcleodContinualAuxiliaryTask2022</fr:display-uri>
    <fr:route>/mcleodContinualAuxiliaryTask2022/</fr:route>
    <fr:title text="Continual Auxiliary Task Learning">Continual Auxiliary Task Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/2202.11133</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{mcleodContinualAuxiliaryTask2022,
 title = {Continual {{Auxiliary Task Learning}}},
 author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen, Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},
 year = {2022},
 urldate = {2024-10-16},
 number = {arXiv:2202.11133},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/GUALHQEZ/McLeod et al. - 2022 - Continual Auxiliary Task Learning.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.},
 primaryclass = {cs},
 eprint = {2202.11133},
 month = {February}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
