<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>John Schulman</fr:author>
      <fr:author>Filip Wolski</fr:author>
      <fr:author>Prafulla Dhariwal</fr:author>
      <fr:author>Alec Radford</fr:author>
      <fr:author>Oleg Klimov</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2017</fr:year>
      <fr:month>8</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/</fr:uri>
    <fr:display-uri>schulmanProximalPolicyOptimization2017</fr:display-uri>
    <fr:route>/schulmanProximalPolicyOptimization2017/</fr:route>
    <fr:title text="Proximal Policy Optimization Algorithms">Proximal Policy Optimization Algorithms</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.1707.06347</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/1707.06347</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{schulmanProximalPolicyOptimization2017,
 title = {Proximal {{Policy Optimization Algorithms}}},
 author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
 year = {2017},
 doi = {10.48550/arXiv.1707.06347},
 urldate = {2025-06-27},
 number = {arXiv:1707.06347},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/UUPSNPZG/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
 primaryclass = {cs},
 eprint = {1707.06347},
 month = {August}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>1</fr:month>
              <fr:day>30</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003X/</fr:uri>
            <fr:display-uri>003X</fr:display-uri>
            <fr:route>/003X/</fr:route>
            <fr:title text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link href="/shaoDeepSeekMathPushingLimits2024/" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" uri="https://kellenkanarios.com/shaoDeepSeekMathPushingLimits2024/" display-uri="shaoDeepSeekMathPushingLimits2024" type="local">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</fr:link></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:title text="Math">Math</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tex display="block"><![CDATA[
    \begin {align*}
    {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\
    &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )
    \end {align*}
  ]]></fr:tex>
  where
  <fr:tex display="block"><![CDATA[
        \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1
    ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>1</fr:month>
              <fr:day>30</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003Y/</fr:uri>
            <fr:display-uri>003Y</fr:display-uri>
            <fr:route>/003Y/</fr:route>
            <fr:title text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>
  Rough itinerary,
  <html:ul><html:li>Vanilla policy gradient
      <html:ul><html:li>Policy gradient theorem + proof</html:li>
          <html:li>Deterministic policy gradient theorem + (maybe)proof</html:li></html:ul></html:li>
      <html:li>Actor critic method
      <html:ul><html:li>A2C: Variance reduction method</html:li>
        <html:li>(Maybe) A3C: Asynchronous update</html:li></html:ul></html:li>
      <html:li>Trust region policy optimization</html:li>
      <html:li>Soft Actor Critic</html:li>
      <html:li><fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">Proximal Policy Optimization</fr:link></html:li>
      <html:li><fr:link href="/003X/" title="Group Relative Policy Optimization" uri="https://kellenkanarios.com/003X/" display-uri="003X" type="local">Group Relative Policy Optimization</fr:link></html:li></html:ul></html:p>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="policy-gradient" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
