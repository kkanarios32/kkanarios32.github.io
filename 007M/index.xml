<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>6</fr:month>
      <fr:day>5</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/007M/</fr:uri>
    <fr:display-uri>007M</fr:display-uri>
    <fr:route>/007M/</fr:route>
    <fr:title text="Upcoming Blogs">Upcoming Blogs</fr:title>
  </fr:frontmatter>
  <fr:mainmatter>
    <html:ul><html:li><fr:link href="/0005/" title="Contrastive Reinforcement Learning" uri="https://kellenkanarios.com/0005/" display-uri="0005" type="local">Contrastive Reinforcement Learning</fr:link></html:li>
  <html:li><fr:link href="/003Y/" title="The History and Evolution of Policy Gradient Algorithms" uri="https://kellenkanarios.com/003Y/" display-uri="003Y" type="local">The History and Evolution of Policy Gradient Algorithms</fr:link></html:li>
  <html:li><fr:link href="/006Q/" title="A Dynamic Duo: Tree Search + RL" uri="https://kellenkanarios.com/006Q/" display-uri="006Q" type="local">A Dynamic Duo: Tree Search + RL</fr:link></html:li>
  <html:li><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link></html:li>
  <html:li><fr:link href="/007O/" title="RL as Probablistic Inference" uri="https://kellenkanarios.com/007O/" display-uri="007O" type="local">RL as Probablistic Inference</fr:link></html:li>
  <html:li><fr:link href="/007P/" title="Constrained MDPs?" uri="https://kellenkanarios.com/007P/" display-uri="007P" type="local">Constrained MDPs?</fr:link></html:li></html:ul>
    <html:p>Request more below!</html:p>
    <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="blog-requests" theme="boxy-light" crossorigin="anonymous" async="" />
  </fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2024</fr:year>
              <fr:month>10</fr:month>
              <fr:day>29</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0002/</fr:uri>
            <fr:display-uri>0002</fr:display-uri>
            <fr:route>/0002/</fr:route>
            <fr:title text="Blog">Blog</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>This is my blog, in which I write about a variety of topics including computer science, mathematics, and more. For more short-form blogs, see the <fr:link href="/007L/" title="Marginalia" uri="https://kellenkanarios.com/007L/" display-uri="007L" type="local">marginalia</fr:link>. See <fr:link href="/007M/" title="Upcoming Blogs" uri="https://kellenkanarios.com/007M/" display-uri="007M" type="local">upcoming blogs</fr:link> and feel free to request! This blog also has a corresponding <fr:link href="/0002/atom.xml" type="external">Atom feed</fr:link>.</html:p>
            <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>3</fr:month>
                  <fr:day>4</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/005F/</fr:uri>
                <fr:display-uri>005F</fr:display-uri>
                <fr:route>/005F/</fr:route>
                <fr:title text="Rebuilding My (Neo)Vim Config From Scratch">Rebuilding My (Neo)Vim Config From Scratch</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>I have been using LazyVim for some time now, but I have now run into issues multiple times where understanding how LazyVim is doing something is far more difficult than if I had written my own setup. I allocated one day for this adventure and really just wanted to make sure I had support for <fr:tex display="inline"><![CDATA[\TeX ]]></fr:tex>, python, forester, and C/C++. Due to my (self-imposed) time constraint, I do not have the associated resources linked for each of the things discussed below. At some point, I hope to come back and more thoroughly cover each of the components.
</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Sane Defaults">Sane Defaults</fr:title></fr:frontmatter><fr:mainmatter>
To my surprise, a lot of the features that I had come to take for granted were actually options set up internally by Lazyvim. For example, I was shocked with 8 space indents!! and I could not even copy from one terminal instance to another... Due to this, I went and found all of the options I liked from Lazyvim and added them to my new configuration in <html:code>configs/options.lua</html:code>.
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Installing a Plugin Manager">Installing a Plugin Manager</fr:title></fr:frontmatter><fr:mainmatter>
For this, we will be using the defacto standard <html:code>lazy.nvim</html:code>. This is actually straightforward and kind of "just works". Just follow the installation guide in their documentation.
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Setting up Auto Complete">Setting up Auto Complete</fr:title></fr:frontmatter><fr:mainmatter>
This is one of the main motivations for me making the switch. It seems <html:code>nvim-cmp</html:code> has finally been replaced with a new <html:code>blink.cmp</html:code>, so that is what we will be using.


  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Rebuilding My (Neo)Vim Config From Scratch › Language Server Protocol"><fr:link href="/005F/" title="Rebuilding My (Neo)Vim Config From Scratch" uri="https://kellenkanarios.com/005F/" display-uri="005F" type="local">Rebuilding My (Neo)Vim Config From Scratch</fr:link> › Language Server Protocol</fr:title></fr:frontmatter><fr:mainmatter>
It turns out there is a lot that goes into getting LSP setup correctly.
<html:ol><html:li>First we must actually install the language servers. To do this the easiest way, we use the <html:code>mason.nvim</html:code> and <html:code>mason.nvim-lspconfig</html:code> plugins. At some point, I might actually figure out how to set up lsp myself without lspconfig but that point is not now.
</html:li>
<html:li>Through <html:code>nvim-lspconfig</html:code>, we can set up each of the servers we want to have LSP support. I just set up clangd, pyright, and texlab.</html:li></html:ol>
This was a bit ridiculous. The first of many challenges was around import resolution in python. To remedy this, I needed to write a function to find the virtual environment directory and then set the <html:code>pythonPath</html:code> to the venv python binary. Previously, I think I was just using pylsp and installing it as a pip package to each python venv. I much prefer the new way, and I think pyright is overall a much better lsp.
</fr:mainmatter></fr:tree>


  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Rebuilding My (Neo)Vim Config From Scratch › Forester Completion"><fr:link href="/005F/" title="Rebuilding My (Neo)Vim Config From Scratch" uri="https://kellenkanarios.com/005F/" display-uri="005F" type="local">Rebuilding My (Neo)Vim Config From Scratch</fr:link> › Forester Completion</fr:title></fr:frontmatter><fr:mainmatter>
  Another necessary completion source for me is the one provided by <html:code>forester.nvim</html:code>. Similar to vimtex, the reference completion support is VERY useful. Obviously, I need completion when I am writing this blog!!! This was a little more involved. The first difficulty was that the completion source provided by the <html:code>forester.nvim</html:code> plugin was for <html:code>nvim-cmp</html:code>. It turns out this is a prevalent enough problem that the author of <html:code>blink.cmp</html:code> wrote an additional plugin <html:code>blink.compat</html:code> to allow for <html:code>nvim-cmp</html:code> completion sources. While this sounds all fine and good, <html:code>nvim-compat</html:code> expects plugins that return the completion source themselves, whereas in <html:code>forester.nvim</html:code> the completion source is just one submodule of a more feature-rich plugin. To get around this, I needed to look into the <html:code>blink.compat</html:code> code and find how they are registering the sources and just do it myself.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Snippets">Snippets</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
  Going all the way back to the <fr:link href="https://castel.dev/post/lecture-notes-1/" type="external">Gilles Castel blog post</fr:link>, I have always been partial to snippets that auto-expand. I had them set up prior to Lazyvim but with Lazyvim I had resigned to using friendly-snippets with native nvim snippets. Since I was already redoing everything, this time around I decided not to compromise. Once upon a time (right when it came out I think?) I tried out Luasnips, but it seems that they now have far more extensive features. They are also natively supported by <html:code>blink.cmp</html:code>!! It feels necessary that I plug the <fr:link href="https://github.com/iurimateus/luasnip-latex-snippets.nvim" type="external">awesome repo</fr:link> that ports the original Ultisnips snippets to Luasnip. With this, I was able to easily add my own forester snippets!!!
  </html:p>
  <html:p>
  A fun little thing that I had been hoping to do for awhile and is finally now possible - I can load latex snippets when inside math environments in forester!!!! To do this, I looked into the forester treesitter grammar and found the corresponding nodetypes for math envs. It was then straightforward to detect whether we were in a math env and to load the associated latex snippets.
  </html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Anki">Anki</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
  There is a very cool add-on to anki called <fr:link href="https://git.sr.ht/~foosoft/anki-connectdeck-actions" type="external">AnkiConnect</fr:link> that has an associated plugin <html:code>anki.nvim</html:code>. Basically, AnkiConnects allows you to make requests to Anki and receive/send useful information from/to your decks. Unfortunately, <html:code>anki.nvim</html:code> built-in commands didn't seem all that useful to me. However, it provided the necessary infrastructure for me to accomplish my desired workflow.
  </html:p>
  <html:p>
  Namely, I made my own command that queries Anki for the deck names, which you can then pick from using telescope pickers. When you select one it will create a new flashcard in a specified flashcard directory under a directory created based on the deck name. You can then send this card to that deck using the existing <html:code>AnkiSend</html:code> command.
  </html:p>
  <html:p>I also continued the snippet fun here. When writing Anki cards, you can write latex code between [latex] [/latex] delimiters. I wrote a quick function to detect whether we are in these delimiters and if we are then to load the latex snippets from the previous section. I also added some basic anki filetype plugins to insert things like these delimiters.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:title text="Formatters and Linters">Formatters and Linters</fr:title></fr:frontmatter><fr:mainmatter>
  <html:code>compat.nvim</html:code>, <html:code>mason.nvim</html:code>, black, isort.
</fr:mainmatter></fr:tree>
<html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="nvim" theme="boxy-light" crossorigin="anonymous" async="" /></fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>6</fr:month>
              <fr:day>9</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/007P/</fr:uri>
            <fr:display-uri>007P</fr:display-uri>
            <fr:route>/007P/</fr:route>
            <fr:title text="Constrained MDPs?">Constrained MDPs?</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="constrained-mdps" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>6</fr:month>
              <fr:day>9</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/007O/</fr:uri>
            <fr:display-uri>007O</fr:display-uri>
            <fr:route>/007O/</fr:route>
            <fr:title text="RL as Probablistic Inference">RL as Probablistic Inference</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="rl-prob-inference" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>3</fr:month>
              <fr:day>31</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/006Q/</fr:uri>
            <fr:display-uri>006Q</fr:display-uri>
            <fr:route>/006Q/</fr:route>
            <fr:title text="A Dynamic Duo: Tree Search + RL">A Dynamic Duo: Tree Search + RL</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><html:em>One thing that should be learned [...] is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.</html:em> - <fr:link href="/richardsutton/" title="Richard Sutton" uri="https://kellenkanarios.com/richardsutton/" display-uri="richardsutton" type="local">Richard Sutton</fr:link></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>31</fr:day></fr:date><fr:title text="What is Planning?">What is Planning?</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
    In <fr:link href="/suttonReinforcementLearningIntroduction2018/" title="Reinforcement learning: An introduction" uri="https://kellenkanarios.com/suttonReinforcementLearningIntroduction2018/" display-uri="suttonReinforcementLearningIntroduction2018" type="local">Reinforcement learning: An introduction</fr:link>, they define planning as <html:em>a computational process that takes a model as input and outputs a policy</html:em>. Like everything Sutton writes, I agree with it for the most part. I have struggled with this question for a long time. In the RL community, you often here this vague term "planning" thrown around in all sorts of different situations. I think the key distinction between traditional methods is shown when looking at these methods directly. 
  </html:p>
  <html:p>
    As an example, in <fr:link href="/suttonReinforcementLearningIntroduction2018/" title="Reinforcement learning: An introduction" uri="https://kellenkanarios.com/suttonReinforcementLearningIntroduction2018/" display-uri="suttonReinforcementLearningIntroduction2018" type="local">Q-learning</fr:link> you interact with the environment and learn via updating your Q-function. You then immediately recover an action via <fr:tex display="inline"><![CDATA[a \in  \arg \max _{a} Q(s, a)]]></fr:tex> This does not require that you input a model rather you only input the current state and receive the corresponding action. However, it can be argued that this requires learning a world model <fr:link href="/richensGeneralAgentsNeed2025/" title="General agents need world models" uri="https://kellenkanarios.com/richensGeneralAgentsNeed2025/" display-uri="richensGeneralAgentsNeed2025" type="local">General agents need world models</fr:link>.
  </html:p>
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>31</fr:day></fr:date><fr:uri>https://kellenkanarios.com/006R/</fr:uri><fr:display-uri>006R</fr:display-uri><fr:route>/006R/</fr:route><fr:title text="Alpha-Zero">Alpha-Zero</fr:title></fr:frontmatter><fr:mainmatter><html:p>The major break through of planning was in <fr:link href="/schrittwieserMasteringAtariGo2020/" title="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" uri="https://kellenkanarios.com/schrittwieserMasteringAtariGo2020/" display-uri="schrittwieserMasteringAtariGo2020" type="local">alpha-zero</fr:link>, or more accurately at the time was just alpha-go. However, the core idea remained the same. The idea is to learn some notion of a "good" state in a game like chess or go and then leverage this information in combination with some planning.</html:p></fr:mainmatter></fr:tree><html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="mcts" theme="boxy-light" crossorigin="anonymous" async="" /></fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>1</fr:month>
              <fr:day>30</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003Y/</fr:uri>
            <fr:display-uri>003Y</fr:display-uri>
            <fr:route>/003Y/</fr:route>
            <fr:title text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>
  Rough itinerary,
  <html:ul><html:li>Vanilla policy gradient
      <html:ul><html:li>Policy gradient theorem + proof</html:li>
          <html:li>Deterministic policy gradient theorem + (maybe)proof</html:li></html:ul></html:li>
      <html:li>Actor critic method
      <html:ul><html:li>A2C: Variance reduction method</html:li>
        <html:li>(Maybe) A3C: Asynchronous update</html:li></html:ul></html:li>
      <html:li>Trust region policy optimization</html:li>
      <html:li>Soft Actor Critic</html:li>
      <html:li><fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">Proximal Policy Optimization</fr:link></html:li>
      <html:li><fr:link href="/003X/" title="Group Relative Policy Optimization" uri="https://kellenkanarios.com/003X/" display-uri="003X" type="local">Group Relative Policy Optimization</fr:link></html:li></html:ul></html:p>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="policy-gradient" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>1</fr:month>
              <fr:day>27</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003D/</fr:uri>
            <fr:display-uri>003D</fr:display-uri>
            <fr:route>/003D/</fr:route>
            <fr:title text="Deepseek v1 through R1: RL is back!">Deepseek v1 through R1: RL is back!</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>In this blog, we will aim to understand the key contributions of <fr:link href="/deepseek-aiDeepSeekR1IncentivizingReasoning2025/" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" uri="https://kellenkanarios.com/deepseek-aiDeepSeekR1IncentivizingReasoning2025/" display-uri="deepseek-aiDeepSeekR1IncentivizingReasoning2025" type="local">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</fr:link>. It will serve as the complement to my group meeting presentation possibly consisting of more in-depth explanations. Time permitting, we might go over the engineering innovations introduced in <fr:link href="/deepseek-aiDeepSeekV3TechnicalReport2025/" title="DeepSeek-V3 Technical Report" uri="https://kellenkanarios.com/deepseek-aiDeepSeekV3TechnicalReport2025/" display-uri="deepseek-aiDeepSeekV3TechnicalReport2025" type="local">DeepSeek-V3 Technical Report</fr:link>.</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Background">Background</fr:title></fr:frontmatter><fr:mainmatter>
    By request of my advisor, I will cover the basics of LLMs prior to the innovations in the Deepseek lineage. For those familiar with LLMs, please skip this section.
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>3</fr:day></fr:date><fr:uri>https://kellenkanarios.com/004J/</fr:uri><fr:display-uri>004J</fr:display-uri><fr:route>/004J/</fr:route><fr:title text="Word Embeddings">Word Embeddings</fr:title></fr:frontmatter><fr:mainmatter><fr:tex display="block"><![CDATA[\mathrm {Tok}(\mathbf {x})=\begin {bmatrix} 132\\ 17 \\ 87\\ 83\\ 184\end {bmatrix}]]></fr:tex></fr:mainmatter></fr:tree>
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:uri>https://kellenkanarios.com/004G/</fr:uri><fr:display-uri>004G</fr:display-uri><fr:route>/004G/</fr:route><fr:title text="Self-Attention">Self-Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>
    TLDR: Learned weighting of token embeddings. Essentially, learning which words to "attend" to in the input sequence. Have matrices
<fr:tex display="inline"><![CDATA[\mathbf {Q} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {q}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {q}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_q}]]></fr:tex>, 
<fr:tex display="inline"><![CDATA[
\mathbf {K} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {k}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {k}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_k}
]]></fr:tex>
<fr:tex display="inline"><![CDATA[
\mathbf {V} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_v}
]]></fr:tex></html:p><html:p><html:strong>Intuition 1:</html:strong> Convex re-weighting of input tokens.
  Note that
<fr:tex display="block"><![CDATA[
\begin {align*}
  \begin {bmatrix}
    p_1 & p_2 & p_3
  \end {bmatrix} \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(2)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(3)} & \text {---}
  \end {bmatrix}
  \end {bmatrix} = p_1 \mathbf {v}^{(1)} + p_2 \mathbf {v}^{(2)} + p_3 \mathbf {v}^{(3)}
\end {align*}
  ]]></fr:tex>
<fr:tex display="block"><![CDATA[
\begin {align*}
  \begin {bmatrix}
    p_{11} & 0 & 0 \\
    p_{21} & p_{22} & 0 \\
    p_{31} & p_{32} & p_{33}
  \end {bmatrix} \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(2)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(3)} & \text {---}
  \end {bmatrix}
  \end {bmatrix} = 
  \begin {bmatrix}
  p_{11} \mathbf {v}^{(1)}  \\
  p_{21} \mathbf {v}^{(1)} + p_{22} \mathbf {v}^{(2)} \\
  p_{31} \mathbf {v}^{(1)} + p_{32} \mathbf {v}^{(2)} + p_{33} \mathbf {v}^{(3)}
  \end {bmatrix}
\end {align*}
  ]]></fr:tex>
  <html:strong>Intuition 2:</html:strong> Context dependent re-weighting.
      If <fr:tex display="inline"><![CDATA[\mathbf {p} = \mathbb {S}(\mathbf {Q} \mathbf {K}^T)]]></fr:tex> then
      <fr:tex display="block"><![CDATA[
        \begin {align*}
          p_{ij} = \frac {\mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}{\sum _{j} \mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}
        \end {align*}
      ]]></fr:tex></html:p>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
    Suppose that <fr:tex display="inline"><![CDATA[\mathbf {x} = \text {I play with the ball}]]></fr:tex>. Then 
<fr:tex display="block"><![CDATA[
    \begin {align*}
      \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``ball"})
    \end {align*}
  ]]></fr:tex>
  A feasible query for "ball" would be a verb describing the action of the ball, so maybe
  <fr:tex display="block"><![CDATA[
  \begin {align*}
      W_q \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``play"})
  \end {align*}
  ]]></fr:tex>
  and a key for "play" would be what you are playing with like a ball, so 
  <fr:tex display="block"><![CDATA[
  \begin {align*}
      W_k \mathbf {x}^{(2)} = \mathrm {Embed}(\text {``ball"})
  \end {align*}
  ]]></fr:tex>
  i.e.
<fr:tex display="block"><![CDATA[
  \begin {align*}
    \mathrm {Query}(\text {``quantum"}) \cdot  \mathrm {Key}(\text {``mechanics"}) \approx 
    ||\mathrm {Query}(\text {``quantum"})|| \cdot  ||\mathrm {Key}(\text {``mechanics"})||
  \end {align*}
]]></fr:tex>

</fr:mainmatter></fr:tree>
 
<html:p><fr:tex display="block"><![CDATA[
  \begin {align*}
\left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} &= \mathbb {S}\left (\begin {bmatrix}
\mathbf {q}^{(4)} \cdot  \mathbf {k}^{(1)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(2)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(3)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(4)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(5)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(6)}
\end {bmatrix}
\right ) \\
&= \begin {bmatrix}
0 & 0.2 & 0.3 & 0.5 & 0 & 0
\end {bmatrix}
  \end {align*}
]]></fr:tex>
<fr:tex display="block"><![CDATA[
\left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} \mathbf {V} = 0.2 \mathbf {v}^{(2)} + 0.3 \mathbf {v}^{(3)} + 0.5 \mathbf {v}^{(5)}
]]></fr:tex></html:p></fr:mainmatter></fr:tree>
    
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v1 through R1: RL is back! › RLHF"><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link> › RLHF</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tex display="block"><![CDATA[\mathrm {loss}\left (\phi \right )=E_{\left (x,y\right )\sim  D_{\pi _{\phi }^{\mathrm {RL}}}}\left [r_\theta (x,y)-\beta \log \left (\pi _{\phi }^{\mathrm {RL}}(y\mid  x)/\pi ^{\mathrm {SFT}}(y\mid  x)\right )\right ] + \gamma  E_{x\sim  D_{\mathrm {pretrain}}}\left [\log (\pi _{\phi }^{\mathrm {RL}}(x))\right ]]]></fr:tex>
      </fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v2">Deepseek v2</fr:title></fr:frontmatter><fr:mainmatter>
    Paper 
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v3">Deepseek v3</fr:title></fr:frontmatter><fr:mainmatter>
TODO. Kinda wanna look into the architectural / training innovations from this paper.
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek R1">Deepseek R1</fr:title></fr:frontmatter><fr:mainmatter>


  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v1 through R1: RL is back! › How is R1 different then previous iterations of models?"><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link> › How is R1 different then previous iterations of models?</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ul><html:li>In R1-Zero, they do <html:strong>ZERO</html:strong> SFT on the base model - directly apply reinforcement learning.</html:li>
    <html:li>Use PPO like policy optimization but do <html:strong>NOT</html:strong> learn a reward model.</html:li>
    <html:ul><html:li>Use very simple reward: 
        <html:ul><html:li><fr:tex display="inline"><![CDATA[+1]]></fr:tex> for correct answer</html:li> 
          <html:li><fr:tex display="inline"><![CDATA[-0.5]]></fr:tex> for incorrect answer</html:li> 
          <html:li><fr:tex display="inline"><![CDATA[-1]]></fr:tex> for inability to answer.</html:li></html:ul></html:li></html:ul></html:ul>
</fr:mainmatter></fr:tree>


<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:uri>https://kellenkanarios.com/003X/</fr:uri><fr:display-uri>003X</fr:display-uri><fr:route>/003X/</fr:route><fr:title text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title></fr:frontmatter><fr:mainmatter><html:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link href="/shaoDeepSeekMathPushingLimits2024/" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" uri="https://kellenkanarios.com/shaoDeepSeekMathPushingLimits2024/" display-uri="shaoDeepSeekMathPushingLimits2024" type="local">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</fr:link></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:title text="Math">Math</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tex display="block"><![CDATA[
    \begin {align*}
    {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\
    &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )
    \end {align*}
  ]]></fr:tex>
  where
  <fr:tex display="block"><![CDATA[
        \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1
    ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link href="/schulmanProximalPolicyOptimization2017/" title="Proximal Policy Optimization Algorithms" uri="https://kellenkanarios.com/schulmanProximalPolicyOptimization2017/" display-uri="schulmanProximalPolicyOptimization2017" type="local">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>


  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v1 through R1: RL is back! › Post-training"><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link> › Post-training</fr:title></fr:frontmatter><fr:mainmatter>
    <html:ul><html:li><html:em>Reinforcement Learning for all Scenarios:</html:em> Seems like they do RLHF after the pure RL stage.</html:li>
        <html:ul><html:li>Do traditional helpfulness harmfulness RLHF with trained reward model.</html:li></html:ul></html:ul>
  </fr:mainmatter></fr:tree>



  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:title text="Deepseek v1 through R1: RL is back! › Distilling Models with R1"><fr:link href="/003D/" title="Deepseek v1 through R1: RL is back!" uri="https://kellenkanarios.com/003D/" display-uri="003D" type="local">Deepseek v1 through R1: RL is back!</fr:link> › Distilling Models with R1</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ul><html:li>To distill, they do only SFT with R1 generated COT.</html:li>
      <html:li>They show that distillation outperforms doing pure RL approach on smaller model</html:li></html:ul>
  </fr:mainmatter></fr:tree>


</fr:mainmatter></fr:tree>

  <html:hr />
<html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="deepseek-r1" theme="boxy-light" crossorigin="anonymous" async="" /></fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2024</fr:year>
              <fr:month>10</fr:month>
              <fr:day>29</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0005/</fr:uri>
            <fr:display-uri>0005</fr:display-uri>
            <fr:route>/0005/</fr:route>
            <fr:title text="Contrastive Reinforcement Learning">Contrastive Reinforcement Learning</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>In this blog post, we aim to demistify <fr:link href="/eysenbachContrastiveLearningGoalConditioned2023/" title="Contrastive Learning as Goal-Conditioned Reinforcement Learning" uri="https://kellenkanarios.com/eysenbachContrastiveLearningGoalConditioned2023/" display-uri="eysenbachContrastiveLearningGoalConditioned2023" type="local"><html:em>Contrastive Reinforcement Learning</html:em></fr:link>. This term often gets thrown around in the dark inner circles of the reinforcement learning community. However, for those that are not familiar with contrastive learning, what does contrastive even mean? For those that are, how can reinforcement learning be contrastive? Throughout this blog post, we will answer these questions and many more.</html:p>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2024</fr:year>
                  <fr:month>10</fr:month>
                  <fr:day>31</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/0009/</fr:uri>
                <fr:display-uri>0009</fr:display-uri>
                <fr:route>/0009/</fr:route>
                <fr:title text="Contrastive Learning">Contrastive Learning</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>Prior to understanding contrastive reinforcement learning, it is important to have an at least rudimentary understanding of contrastive learning. Historically, contrastive learning has been used to learn representations. The fundamental idea behind contrastive learning is to encourage the representations of similar outputs to be similar in representation space.</html:p><html:p><html:strong>Supervised setting:</html:strong> For now, assume we are in the supervised setting (we have access to lables). Suppose that we are learning a representation in <fr:tex display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Our model is a classifier on dogs and cats. If we have two dogs <fr:tex display="inline"><![CDATA[y_1]]></fr:tex> and <fr:tex display="inline"><![CDATA[y_2]]></fr:tex> then we want the learned representation map <fr:tex display="block"><![CDATA[\phi : \{\text {dogs}, \text {cats}\} \to  \mathbb {R}^d]]></fr:tex> to be such that <fr:tex display="inline"><![CDATA[\phi (y_1)]]></fr:tex> and <fr:tex display="inline"><![CDATA[\phi (y_2)]]></fr:tex> are "close" in <fr:tex display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Now the notion of "close" is to be determined by the user. An example could be to minimize the inner product between their representation maps i.e. we could learn a feature map parametrized by <fr:tex display="inline"><![CDATA[\theta ]]></fr:tex> with the following objective <fr:tex display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle .]]></fr:tex> Similarly, we want dissimilar outputs to be far apart in representation space. If <fr:tex display="inline"><![CDATA[y_3]]></fr:tex> is a cat, then we can introduce a regularization to encourage this i.e.
<fr:tex display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle  - \sum _{i \in  \{1, 2\}} \langle  \phi _{\theta }(y_i), \phi _{\theta }(y_3) \rangle .]]></fr:tex></html:p><html:p><html:strong>Unsupervised setting:</html:strong> Now suppose that we get rid of labels and are just given <fr:tex display="inline"><![CDATA[n]]></fr:tex> dog samples <fr:tex display="inline"><![CDATA[\mathcal {D}]]></fr:tex> from some distribution <fr:tex display="inline"><![CDATA[p_{\mathcal {D}}]]></fr:tex>. We now want to be able to learn <fr:tex display="inline"><![CDATA[p_{\theta }]]></fr:tex> to somehow estimate this distribution. An approach is to learn to distinguish the sample dogs given from random noise. To do so, we generate <fr:tex display="inline"><![CDATA[n]]></fr:tex> random images <fr:tex display="inline"><![CDATA[\mathcal {R}]]></fr:tex> according to some distribution <fr:tex display="inline"><![CDATA[p_{\mathcal {R}}]]></fr:tex>. We can now return to the supervised learning setting, where we treat <fr:tex display="inline"><![CDATA[\mathcal {D}]]></fr:tex> and <fr:tex display="inline"><![CDATA[\mathcal {R}]]></fr:tex> as two classes. If we recall standard supervised learning practice, given a sample <fr:tex display="inline"><![CDATA[x]]></fr:tex>, we then want to find <fr:tex display="block"><![CDATA[p(\mathcal {D} \mid  x) = 1 - p(\mathcal {R} \mid  X).]]></fr:tex> 
As an explicit example, we will use logistic regression. Namely, we will model <fr:tex display="inline"><![CDATA[p(x) = p(\mathcal {D} \mid  x)]]></fr:tex> as <fr:tex display="block"><![CDATA[p_{\theta }(x) = \frac {1}{1 + e^{-G_{\theta }(x)}}.]]></fr:tex> However, <fr:tex display="inline"><![CDATA[p_{\theta }(x)]]></fr:tex> is estimating <fr:tex display="inline"><![CDATA[p(\mathcal {D} \mid  x)]]></fr:tex>, where we care about <fr:tex display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex>. To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if <fr:tex display="inline"><![CDATA[p_{\theta }(x) = p(\mathcal {D} \mid  x)]]></fr:tex> then <fr:tex display="inline"><![CDATA[G_{\theta }(x) = \log  \frac {p(x \mid  \mathcal {D})}{p(x \mid  \mathcal {R})}]]></fr:tex>. Since we generated the samples from <fr:tex display="inline"><![CDATA[\mathcal {R}]]></fr:tex>, we have the explicit distribution i.e. <fr:tex display="inline"><![CDATA[p(x \mid  \mathcal {R}) = p_{\mathcal {R}}(x)]]></fr:tex>. Therefore, we can restrict <fr:tex display="inline"><![CDATA[G_{\theta }]]></fr:tex> to explicitly learn <fr:tex display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex> by considering <fr:tex display="block"><![CDATA[G_{\theta }(x) = \log  p_{\theta }(x \mid  \mathcal {D}) - \log  p_{\mathcal {R}}(x),]]></fr:tex> considering the cross entropy loss we get the <fr:link href="/000E/" title="NCE loss" uri="https://kellenkanarios.com/000E/" display-uri="000E" type="local">NCE loss</fr:link></html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:uri>https://kellenkanarios.com/000E/</fr:uri><fr:display-uri>000E</fr:display-uri><fr:route>/000E/</fr:route><fr:title text="NCE loss">NCE loss</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em><fr:link href="/gutmannNoisecontrastiveEstimationNew2010/" title="Noise-contrastive estimation: A new estimation principle for unnormalized statistical models" uri="https://kellenkanarios.com/gutmannNoisecontrastiveEstimationNew2010/" display-uri="gutmannNoisecontrastiveEstimationNew2010" type="local">NCE</fr:link></html:em> loss aims to minimize the following objective <fr:tex display="block"><![CDATA[\mathcal {L}_{N} = - \sum _{t} \log  \left [h(x_t; \theta )\right ] + \log \left [1 - h(y_t; \theta )\right ],]]></fr:tex> where <fr:tex display="inline"><![CDATA[x_t]]></fr:tex> are samples from the data distribution and <fr:tex display="inline"><![CDATA[y_t]]></fr:tex> are randomly generated samples and <fr:tex display="block"><![CDATA[\begin {array}{r c l}{{h({\bf  u};\theta )}}&{{=}}&{{\frac {1}{1+\exp \left [-G({\bf  u};\theta )\right ]},}}\\ {{G({\bf  u};\theta )}}&{{=}}&{{\ln  p_{m}({\bf  u};\theta )-\ln  p_{n}({\bf  u}).}}\end {array}]]></fr:tex></html:p></fr:mainmatter></fr:tree><html:p>In <fr:link href="/gutmannNoisecontrastiveEstimationNew2010/" title="Noise-contrastive estimation: A new estimation principle for unnormalized statistical models" uri="https://kellenkanarios.com/gutmannNoisecontrastiveEstimationNew2010/" display-uri="gutmannNoisecontrastiveEstimationNew2010" type="local">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</fr:link>, they show under mild conditions that the estimator <fr:tex display="inline"><![CDATA[p_{\theta }(x \mid  D) \to  p_{\mathcal {D}}(x)]]></fr:tex> in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is <fr:link href="/000F/" title="Consistent estimator" uri="https://kellenkanarios.com/000F/" display-uri="000F" type="local">consistent</fr:link>.</html:p><html:p><html:strong>Time series:</html:strong> Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner. For a <fr:tex display="inline"><![CDATA[k]]></fr:tex> step prediction, this would just be your model predicting what will happen in <fr:tex display="inline"><![CDATA[k]]></fr:tex> steps then seeing if it matches what occured <fr:tex display="inline"><![CDATA[k]]></fr:tex> steps in the future in the sample trajectory. However, if your sample space <fr:tex display="inline"><![CDATA[\mathcal {X}]]></fr:tex> is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.</html:p><html:p>Fast forwarding to contrastive RL, current work is primarily considered with a particular contrastive objective.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:uri>https://kellenkanarios.com/000B/</fr:uri><fr:display-uri>000B</fr:display-uri><fr:route>/000B/</fr:route><fr:title text="InfoNCE">InfoNCE</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em><fr:link href="/oordRepresentationLearningContrastive2019/" title="Representation Learning with Contrastive Predictive Coding" uri="https://kellenkanarios.com/oordRepresentationLearningContrastive2019/" display-uri="oordRepresentationLearningContrastive2019" type="local">InfoNCE</fr:link></html:em> loss aims to minimize the following information-theoretic objective <fr:tex display="block"><![CDATA[\mathcal {L}_{N} = - \mathbb {E}_{\mathcal {X}} \left [\log  \frac {f_k(x_{t + k}, c_t)}{\sum _{x_j \in  \mathcal {X}} f_k(x_j, c_t)}\right ]]]></fr:tex></html:p></fr:mainmatter></fr:tree><html:p>Now we need to unpack this very ominous loss. To start, what are <fr:tex display="inline"><![CDATA[x_k]]></fr:tex> and <fr:tex display="inline"><![CDATA[c_t]]></fr:tex>?</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:uri>https://kellenkanarios.com/000C/</fr:uri><fr:display-uri>000C</fr:display-uri><fr:route>/000C/</fr:route><fr:title text="Maximize Mutual info">Maximize Mutual info</fr:title><fr:taxon>Theorem</fr:taxon></fr:frontmatter><fr:mainmatter><html:p><fr:tex display="inline"><![CDATA[\mathcal {L}_N]]></fr:tex> from <fr:link href="/oordRepresentationLearningContrastive2019/" title="Representation Learning with Contrastive Predictive Coding" uri="https://kellenkanarios.com/oordRepresentationLearningContrastive2019/" display-uri="oordRepresentationLearningContrastive2019" type="local">Representation Learning with Contrastive Predictive Coding</fr:link> maximizes a lower bound on the <fr:link href="/000V/" title="Mutual Information" uri="https://kellenkanarios.com/000V/" display-uri="000V" type="local">Mutual Information</fr:link> between <fr:tex display="inline"><![CDATA[x_{t + k}]]></fr:tex> and <fr:tex display="inline"><![CDATA[c_t]]></fr:tex>.</html:p>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:taxon>Proof</fr:taxon></fr:frontmatter><fr:mainmatter>All we must do is plug <fr:tex display="inline"><![CDATA[\frac {p(x \mid  c)}{p(x)}]]></fr:tex> back into the objective.
  <fr:tex display="block"><![CDATA[\begin {align*}
    \mathcal {L}_{\mathbb {N}}^{\text {opt}}&=-\,\mathbb {E}\log \left [\frac {\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}}{\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}+\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}}\right ] \\
    &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\
    &\approx \mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}(N-1)\,\mathbb {E}\,\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\
    &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k} \mid  c_t)}N\right ] \\
    &\geq  \mathbb {E} \log  \left [\frac {p(x_{t + k})}{p(x_{t + k} \mid  c_t)}N \right ] \\
    &= - I(x_{t + k}, c_t) + \log  N
 \end {align*}
  ]]></fr:tex>
</fr:mainmatter></fr:tree>
 

</fr:mainmatter></fr:tree><fr:tex display="block"><![CDATA[
\operatorname *{max}_{f(u,v)}\mathbb {E}_{(u,v^{+})\sim  p(u,v)}\left [\log \sigma (\underbrace {f(u,{\green  v^{+}})}_{\phi (u)^{T}\psi ({\green  v^{+}})})+\log (1-\sigma (\underbrace {f(u,{\red  v^{-}})}_{\phi (u)^{T}\psi ({\red  v^{-}})}))\right ]
]]></fr:tex><fr:tex display="block"><![CDATA[
\begin {align*}
&\operatorname *{max}_{f}\mathbb {E}_{(s,a)\sim  p(s,a),s_{f}^{-}\sim  p(s_{f})}\left [\mathcal {L}(s,a,s_{f}^{+},s_{f}^{-})\right ] \\
\end {align*}
]]></fr:tex><fr:tex display="block"><![CDATA[
\mathcal {L}_1(\theta ) = \log \sigma (f_{\theta }(s_1,a_1,{\color {green} s_{8}})) + \log (1-\sigma (f_{\theta }(s_1,a_1, {\color {red} s_3})))
]]></fr:tex><fr:tex display="block"><![CDATA[
\begin {align*}
\widehat {\mathcal {L}}(\theta ) &= \frac {1}{n} \sum _{i = 1}^{n} \mathcal {L}_i \\
&= \frac {1}{n} \sum _{i = 1}^{n} \Big [\log \sigma (f_{\theta }(s_i,a_i,{\color {green} s_{f}^{+}})) + \log (1-\sigma (f_{\theta }(s_i,a_i, {\color {red} s_{f}^{-}})))\Big ]
\end {align*}
]]></fr:tex><fr:tex display="block"><![CDATA[
\mathcal {L}(\theta ) = \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ]
]]></fr:tex><fr:tex display="block"><![CDATA[f^*(s, a, s_g) = \log \left (\frac {p^{\pi (\cdot  \mid  \cdot )}(s_g \mid  s, a)}{p(s_g)}\right )]]></fr:tex>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Proof</fr:taxon></fr:frontmatter><fr:mainmatter>
    We want to maximize
<fr:tex display="block"><![CDATA[
    \begin {align*}
\mathcal {L}(\theta ) &= \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] \\
&= \int  \log \sigma (f_{\theta }(x)) P_X(x) + \int  \log (1-\sigma (f_{\theta }(y))) P_Y(y) \\
&= \int  \log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)
    \end {align*}
    ]]></fr:tex>
    Since we are maximizing <fr:tex display="inline"><![CDATA[f(s)]]></fr:tex>, we can just maximize the integrand i.e.
<fr:tex display="block"><![CDATA[
      \begin {align*}
        \frac {\mathrm {d}}{\mathrm {d}f(z)} \Big [\log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)\Big ] = 0
      \end {align*}
    ]]></fr:tex>
    Solving,
    <fr:tex display="block"><![CDATA[
        \begin {align*}
          P_X(z)\big (1 - \sigma (f(z))\big ) - P_Y(z)\sigma (f(z)) = 0 &\iff  \sigma (f(z)) = \frac {P_X(z)}{P_X(z) + P_Y(z)} \\
          &\iff  f(z) = \log \left (\frac {P_X(z)}{P_Y(z)}\right )
        \end {align*}
      ]]></fr:tex>
  </fr:mainmatter></fr:tree>
 


   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Proof</fr:taxon></fr:frontmatter><fr:mainmatter>
The first step is to prove that the average Q-values are close to the task-conditioned Q-values. Below, we will use <fr:tex display="inline"><![CDATA[R_{c}(\tau )\triangleq \sum _{\ell =0}^{\infty }\gamma ^{\ell }r_{\ell }(s_{\ell },a_{\ell })]]></fr:tex>:

<fr:tex display="block"><![CDATA[
\begin {align*}
\left |Q^{\beta (\cdot |\cdot ,a)}(s,a,e)-Q^{\beta (\cdot |\cdot ,\epsilon ^{\prime })}(s,a,e)\right |&=\left |\int \beta (\tau \mid  s,a,e)R_{e}(\tau )d\tau -\int \beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right |\\ 
&=\left |\int \beta (\tau \mid  s,a,e)-\beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right | \\
&=\left |\int \beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )R_{e}(\tau )d\tau \right | \\
&\leq \int \left |\beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )\right |d\tau \cdot \operatorname *{max}_{\tau }|R_{e}(\tau )d\tau | \\
&\leq \int \beta (\tau \mid  s,a,e)\left |1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right |d\tau \cdot  1 \\
&=\mathbb {E}_{\beta (\tau |s,a,e)}\left [\left |1-{\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}}\right |\right ] \\
&\leq  \epsilon .
\end {align*}
  ]]></fr:tex>
</fr:mainmatter></fr:tree>
 

</fr:mainmatter>
            </fr:tree>
            <html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="contrastive-rl" theme="boxy-light" crossorigin="anonymous" async="" />
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
