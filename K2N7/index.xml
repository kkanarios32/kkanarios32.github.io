<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors />
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>7</fr:month>
      <fr:day>14</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/K2N7/</fr:uri>
    <fr:display-uri>K2N7</fr:display-uri>
    <fr:route>/K2N7/</fr:route>
    <fr:title text="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</fr:title>
    <fr:taxon>Definition</fr:taxon>
  </fr:frontmatter>
  <fr:mainmatter><html:p>A <html:em>gated linear unit</html:em> is an activation function combined with an elementwise multiplication i.e. 
<html:ul><html:li><fr:tex display="inline"><![CDATA[\mathrm {ReGLU} = \mathrm {ReLU}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li>
  <html:li><fr:tex display="inline"><![CDATA[\mathrm {SwiGLU} = \mathrm {Swish}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li></html:ul>
where <fr:tex display="inline"><![CDATA[\mathrm {ReLU}]]></fr:tex> is the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> activation and <fr:tex display="inline"><![CDATA[\mathrm {Swish}]]></fr:tex> is the <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> activation respectively.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The matrix <fr:tex display="inline"><![CDATA[\mathbf {V}]]></fr:tex> introduces additional learnable parameters. Therefore, an architecture that uses gating typically reduces their other parameters by a factor of <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>.
</fr:mainmatter></fr:tree>
</fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>13</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/W4WB/</fr:uri>
            <fr:display-uri>W4WB</fr:display-uri>
            <fr:route>/W4WB/</fr:route>
            <fr:title text="Architecture Variations">Architecture Variations</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Norms">Norms</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:strong>Pre-norm vs. Post-norm</html:strong>: The first architecture variation discussed is when to apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link>. As can be seen in the figure below, rather than apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> in the residual stream. They instead place it before the FFN and MHA layers.</html:p>
<html:figure><html:img width="40%" src="/bafkrmiawz4eenenwuvnx3pzxjniwghlortvgc5ytxzq5z5oyznugozl22y.png" />
<html:figcaption>Post norm (a) vs. pre norm (b).</html:figcaption></html:figure>
<html:p>They actually found that adding <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer normalization</fr:link> before and after the MHA and FFN works the best. This is known as <html:em>double norm</html:em>.</html:p>
<html:p><html:strong><fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> vs. <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link></html:strong>:
Another useful trick is to use the <fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> instead of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link>.
</html:p>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/1XNO/</fr:uri><fr:display-uri>1XNO</fr:display-uri><fr:route>/1XNO/</fr:route><fr:title text="RMS Norm">RMS Norm</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The RMS norm is simply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> without the added bias term and centering i.e. 
<fr:tex display="block"><![CDATA[\mathrm {RMSNorm}: \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \frac {\mathbf {x}}{\sqrt {\mathrm {Var}[\mathbf {x}_i] + \epsilon }}*\gamma .]]></fr:tex>
This is used to spare memory movement of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layernorm</fr:link>, where the bias term <fr:tex display="inline"><![CDATA[\beta  \in  \mathbb {R}^d]]></fr:tex> and has been found to be empirically almost if not as good.
</html:p></fr:mainmatter></fr:tree>
<html:p>One might wonder if matrix multiplies are the only thing that matters what can such a small change really accomplish.</html:p>
<html:ul><html:li>Due to memory movement, despite being .17<![CDATA[%]]> of <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link>, provided 25<![CDATA[%]]> speedup in runtime!!</html:li></html:ul>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Most people apparently just drop the bias term and keep the centering i.e. subtracting the mean. This makes sense because computing the mean does not require loading any additional information back to memory.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Activations">Activations</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Despite a long list of activations, the two focused on are <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> and <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link>.</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/BEA2/</fr:uri><fr:display-uri>BEA2</fr:display-uri><fr:route>/BEA2/</fr:route><fr:title text="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>rectified linear unit</html:em>(ReLU) is defined as
<fr:tex display="block"><![CDATA[\mathrm {ReLU} : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \max (0, \mathbf {x}),]]></fr:tex>
where the <fr:tex display="inline"><![CDATA[\max ]]></fr:tex> is done elementwise i.e. <fr:tex display="inline"><![CDATA[\mathrm {ReLU}(\mathbf {x})_i = \max (0, x_i)]]></fr:tex>
This provides "nice" gradients, making it common when training neural networks.
</html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/5SPM/</fr:uri><fr:display-uri>5SPM</fr:display-uri><fr:route>/5SPM/</fr:route><fr:title text="Gaussian Error Linear Unit (GELU)">Gaussian Error Linear Unit (GELU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Gaussian Error Linear Unit</html:em> (GELU) is a slight modification to the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> to account for the non-differentiability at <fr:tex display="inline"><![CDATA[0]]></fr:tex>. Namely,
<fr:tex display="block"><![CDATA[\mathrm {GELU}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} \cdot  \psi (\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[\psi (\mathbf {x}) = \mathrm {CDF}(\mathcal {N}(\mathbf {x}, \mathbf {I}))]]></fr:tex></html:p><html:figure><html:img width="70%" src="/bafkrmian7h7ke2dp6nqdq4624wx4off5lk346f7d5pp6mv7bbimcy7azni.png" />
<html:figcaption>GELU vs. ReLU activation and derivative. Taken from <fr:link href="https://www.baeldung.com/cs/gelu-activation-function" type="external">here</fr:link>.</html:figcaption></html:figure></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/OZD8/</fr:uri><fr:display-uri>OZD8</fr:display-uri><fr:route>/OZD8/</fr:route><fr:title text="Swish Activation">Swish Activation</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Swish</html:em> activation is given by
<fr:tex display="block"><![CDATA[\mathrm {Swish}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} s(\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[s : \mathbb {R}^d \to  \mathbb {R}^d]]></fr:tex> is the <fr:link href="/NNDS/" title="Sigmoid" uri="https://kellenkanarios.com/NNDS/" display-uri="NNDS" type="local">sigmoid function</fr:link>.
</html:p></fr:mainmatter></fr:tree>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  The extra computation required by <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link> or <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> is a non-factor because <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> are dominated by matrix multiplication and there is no increase in memory pressure.
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/K2N7/</fr:uri><fr:display-uri>K2N7</fr:display-uri><fr:route>/K2N7/</fr:route><fr:title text="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>A <html:em>gated linear unit</html:em> is an activation function combined with an elementwise multiplication i.e. 
<html:ul><html:li><fr:tex display="inline"><![CDATA[\mathrm {ReGLU} = \mathrm {ReLU}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li>
  <html:li><fr:tex display="inline"><![CDATA[\mathrm {SwiGLU} = \mathrm {Swish}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li></html:ul>
where <fr:tex display="inline"><![CDATA[\mathrm {ReLU}]]></fr:tex> is the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> activation and <fr:tex display="inline"><![CDATA[\mathrm {Swish}]]></fr:tex> is the <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> activation respectively.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The matrix <fr:tex display="inline"><![CDATA[\mathbf {V}]]></fr:tex> introduces additional learnable parameters. Therefore, an architecture that uses gating typically reduces their other parameters by a factor of <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>.
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Serial vs. Parallel">Serial vs. Parallel</fr:title></fr:frontmatter><fr:mainmatter>
Another small trick is to parallelize computation. Traditionally, one computes the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> output and then passes this to the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> i.e.
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x} + \mathrm {MHA}(\mathrm {LN}(\mathbf {x})))).]]></fr:tex>
However, this requires waiting for the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> to complete before performing the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> computation. It has been found that performing these in parallel does not cause any severe degradation in performance. Explicitly, instead they do
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x})) + \mathrm {MHA}(\mathrm {LN}(\mathbf {x}))]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>14</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/9VZH/</fr:uri>
            <fr:display-uri>9VZH</fr:display-uri>
            <fr:route>/9VZH/</fr:route>
            <fr:title text="Hyperparameter Rules (CS336)">Hyperparameter Rules (<fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>)</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p><html:strong>Rule 1</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {ff}} = d_{\text {model}}]]></fr:tex>
<html:ul><html:li><fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex> is the input dimension</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d_{\text {ff}}]]></fr:tex> is the hidden dimension</html:li></html:ul></html:p>
            <html:p><html:strong>Exception 1</html:strong>: in the case of <fr:link href="/K2N7/" title="Gated Linear Unit (GLU)" uri="https://kellenkanarios.com/K2N7/" display-uri="K2N7" type="local">GLUs</fr:link>, we recall that to account for extra parameters we scale <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>. Therefore, <fr:tex display="inline"><![CDATA[d_{\text {ff}} = \frac {8}{3}d_{\text {model}}]]></fr:tex>.</html:p>
            <html:p><html:strong>Exception 2</html:strong>: In T5, they use <fr:tex display="inline"><![CDATA[d_{\text {ff}} = 64d_{\text {model}}]]></fr:tex>. The logic for this was that we could maximize the <fr:link href="/99G8/" title="Model FLOP Utilization (MFU)" uri="https://kellenkanarios.com/99G8/" display-uri="99G8" type="local">MFU</fr:link> by increasing matrix size. Ended up going back to small <fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex>.</html:p>
            <html:p><html:strong>Rule 2</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {head}} = d_{\text {model}} / \text {num heads}]]></fr:tex>
<html:ul><html:li>I believe the <fr:tex display="inline"><![CDATA[d_{\text {head}}]]></fr:tex> is the output dim of each head.</html:li>
  <html:li>This just says the total output dim for <fr:tex display="inline"><![CDATA[n]]></fr:tex> heads is the same as if we did one head.</html:li>
  <html:li>Means there is something important about splitting up the heads.</html:li></html:ul></html:p>
            <html:p><html:strong>Rule 3</html:strong>: Aspect ratio = <fr:tex display="inline"><![CDATA[\frac {d_{\text {model}}}{n_{\text {layer}}} \approx  100-200]]></fr:tex>
<html:ul><html:li>Pipeline dependent: if network speed is fast than parallelizing is easier and shallow networks are better.</html:li>
  <html:li>If poor network speed then pipeline parallel might be more viable and deeper networks would be more parallelizable.</html:li></html:ul></html:p>
            <html:p><html:strong>Rule 4:</html:strong> Vocab size
<html:ul><html:li>For single language models, vocab size is typically <fr:tex display="inline"><![CDATA[30-50k]]></fr:tex></html:li>
  <html:li>For multi language models, vocab size is typically <fr:tex display="inline"><![CDATA[100-250k]]></fr:tex></html:li></html:ul></html:p>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>14</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/OZD8/</fr:uri>
            <fr:display-uri>OZD8</fr:display-uri>
            <fr:route>/OZD8/</fr:route>
            <fr:title text="Swish Activation">Swish Activation</fr:title>
            <fr:taxon>Definition</fr:taxon>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>The <html:em>Swish</html:em> activation is given by
<fr:tex display="block"><![CDATA[\mathrm {Swish}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} s(\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[s : \mathbb {R}^d \to  \mathbb {R}^d]]></fr:tex> is the <fr:link href="/NNDS/" title="Sigmoid" uri="https://kellenkanarios.com/NNDS/" display-uri="NNDS" type="local">sigmoid function</fr:link>.
</html:p>
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>14</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/BEA2/</fr:uri>
            <fr:display-uri>BEA2</fr:display-uri>
            <fr:route>/BEA2/</fr:route>
            <fr:title text="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</fr:title>
            <fr:taxon>Definition</fr:taxon>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>The <html:em>rectified linear unit</html:em>(ReLU) is defined as
<fr:tex display="block"><![CDATA[\mathrm {ReLU} : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \max (0, \mathbf {x}),]]></fr:tex>
where the <fr:tex display="inline"><![CDATA[\max ]]></fr:tex> is done elementwise i.e. <fr:tex display="inline"><![CDATA[\mathrm {ReLU}(\mathbf {x})_i = \max (0, x_i)]]></fr:tex>
This provides "nice" gradients, making it common when training neural networks.
</html:p>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
