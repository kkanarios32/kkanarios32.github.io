[{"title":"https://kellenkanarios.com/QIFY/","uri":"QIFY","taxon":"Definition","tags":["public"],"route":"/QIFY/","metas":{}},{"title":"AlphaCraftax","uri":"EE7A","taxon":null,"tags":["project"],"route":"/EE7A/","metas":{"author":"True"}},{"title":"Contrastive RL + Deepseek R1 Talk","uri":"CRLR1","taxon":null,"tags":["group","talk"],"route":"/CRLR1/","metas":{"date":"false","slides":"https://kellenkanarios.com/bafkrmietzsrc4u2kxu6vzdty2i7xvfe734uapzyesmuk62vbvhlbvfzabu.pdf"}},{"title":"RLHF Talk","uri":"RLHF","taxon":null,"tags":["group","talk"],"route":"/RLHF/","metas":{"date":"false","slides":"https://kellenkanarios.com/bafkrmid63jfs3kuhekwcujc7vdgvbbphyyd7kg4dmajlvp2xf7vqecz33i.pdf"}},{"title":"Contrastive Motion Planning","uri":"DERC","taxon":null,"tags":["project"],"route":"/DERC/","metas":{"author":"True"}},{"title":"Optimization on the Riemmanian Manifold","uri":"DERB","taxon":null,"tags":[],"route":"/DERB/","metas":{}},{"title":"Newton and Quasi-Newton Method","uri":"DERA","taxon":null,"tags":[],"route":"/DERA/","metas":{}},{"title":"How function properties affect convergence","uri":"DER9","taxon":null,"tags":[],"route":"/DER9/","metas":{}},{"title":"Line Search","uri":"DER8","taxon":null,"tags":[],"route":"/DER8/","metas":{}},{"title":"Accelerated Gradient Descent","uri":"DER7","taxon":null,"tags":[],"route":"/DER7/","metas":{}},{"title":"Convergence of Gradient Descent","uri":"DER6","taxon":null,"tags":[],"route":"/DER6/","metas":{}},{"title":"Rate of Convergence","uri":"DER5","taxon":null,"tags":[],"route":"/DER5/","metas":{}},{"title":"Constrained Optimality","uri":"DER4","taxon":null,"tags":[],"route":"/DER4/","metas":{}},{"title":"Optimality Conditions","uri":"DER3","taxon":null,"tags":[],"route":"/DER3/","metas":{}},{"title":"Taylor Expansion and Lipschitz Functions","uri":"DER2","taxon":null,"tags":[],"route":"/DER2/","metas":{}},{"title":"Basic Matrix Analysis","uri":"DER1","taxon":null,"tags":[],"route":"/DER1/","metas":{}},{"title":"Operations that preserve convexity","uri":"DER0","taxon":null,"tags":[],"route":"/DER0/","metas":{}},{"title":"https://kellenkanarios.com/DEQZ/","uri":"DEQZ","taxon":null,"tags":[],"route":"/DEQZ/","metas":{}},{"title":"https://kellenkanarios.com/DEQY/","uri":"DEQY","taxon":null,"tags":[],"route":"/DEQY/","metas":{}},{"title":"https://kellenkanarios.com/DEQX/","uri":"DEQX","taxon":null,"tags":[],"route":"/DEQX/","metas":{}},{"title":"https://kellenkanarios.com/DEQW/","uri":"DEQW","taxon":null,"tags":[],"route":"/DEQW/","metas":{}},{"title":"Course outline","uri":"DEQT","taxon":null,"tags":[],"route":"/DEQT/","metas":{}},{"title":"https://kellenkanarios.com/0Q9H/","uri":"0Q9H","taxon":null,"tags":[],"route":"/0Q9H/","metas":{}},{"title":"https://kellenkanarios.com/008J/","uri":"008J","taxon":null,"tags":[],"route":"/008J/","metas":{}},{"title":"Reparametrization Trick","uri":"0089","taxon":null,"tags":[],"route":"/0089/","metas":{}},{"title":"Expectation Maximization","uri":"0088","taxon":null,"tags":[],"route":"/0088/","metas":{}},{"title":"Tokenization","uri":"0087","taxon":null,"tags":[],"route":"/0087/","metas":{}},{"title":"\n  #Deliberate-Practice\n (2/66): CS336 Lecture 1","uri":"0086","taxon":null,"tags":[],"route":"/0086/","metas":{}},{"title":"Notebook: Stanford CS336","uri":"0085","taxon":null,"tags":["note","top"],"route":"/0085/","metas":{}},{"title":"Notebook: Three Easy Pieces","uri":"0084","taxon":null,"tags":["note","top"],"route":"/0084/","metas":{}},{"title":"\n  #Deliberate-Practice\n (3/66): TPE Process Interlude","uri":"007Y","taxon":null,"tags":[],"route":"/007Y/","metas":{}},{"title":"LLMs","uri":"003W","taxon":null,"tags":["project","top"],"route":"/003W/","metas":{}},{"title":"Latent Variable","uri":"0083","taxon":"Definition","tags":[],"route":"/0083/","metas":{}},{"title":"Evidence Lower Bound","uri":"0082","taxon":null,"tags":[],"route":"/0082/","metas":{}},{"title":"\n  #Deliberate-Practice\n (1/66): TPE Processes","uri":"007V","taxon":null,"tags":[],"route":"/007V/","metas":{}},{"title":"Weeknotes","uri":"007S","taxon":null,"tags":["top"],"route":"/007S/","metas":{}},{"title":"Constrained MDPs?","uri":"007P","taxon":null,"tags":[],"route":"/007P/","metas":{}},{"title":"RL as Probablistic Inference","uri":"007O","taxon":null,"tags":[],"route":"/007O/","metas":{}},{"title":"https://kellenkanarios.com/007J/","uri":"007J","taxon":null,"tags":[],"route":"/007J/","metas":{}},{"title":"https://kellenkanarios.com/007K/","uri":"007K","taxon":null,"tags":[],"route":"/007K/","metas":{}},{"title":"Marginalia","uri":"007L","taxon":null,"tags":[],"route":"/007L/","metas":{}},{"title":"Upcoming Blogs","uri":"007M","taxon":null,"tags":["blog","upcoming"],"route":"/007M/","metas":{}},{"title":"Upcoming papers of the week","uri":"007N","taxon":null,"tags":[],"route":"/007N/","metas":{}},{"title":"A Survey of State Representation Learning for Deep Reinforcement Learning","uri":"echchahedSurveyStateRepresentation2025","taxon":"Reference","tags":[],"route":"/echchahedSurveyStateRepresentation2025/","metas":{"doi":"10.48550/arXiv.2506.17518","external":"https://arxiv.org/abs/2506.17518","bibtex":"@misc{echchahedSurveyStateRepresentation2025,\n title = {A {{Survey}} of {{State Representation Learning}} for {{Deep Reinforcement Learning}}},\n author = {Echchahed, Ayoub and Castro, Pablo Samuel},\n year = {2025},\n doi = {10.48550/arXiv.2506.17518},\n urldate = {2025-06-27},\n number = {arXiv:2506.17518},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/8T83X33L/Echchahed and Castro - 2025 - A Survey of State Representation Learning for Deep.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide a broad categorization of these methods within a model-free online setting, exploring how they tackle the learning of state representations differently. We categorize the methods into six main classes, detailing their mechanisms, benefits, and limitations. Through this taxonomy, our aim is to enhance the understanding of this field and provide a guide for new researchers. We also discuss techniques for assessing the quality of representations, and detail relevant future directions.},\n primaryclass = {cs},\n eprint = {2506.17518},\n month = {June}\n}"}},{"title":"Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning","uri":"klissarovDiscoveringTemporalStructure2025","taxon":"Reference","tags":[],"route":"/klissarovDiscoveringTemporalStructure2025/","metas":{"doi":"10.48550/arXiv.2506.14045","external":"https://arxiv.org/abs/2506.14045","bibtex":"@misc{klissarovDiscoveringTemporalStructure2025,\n title = {Discovering {{Temporal Structure}}: {{An Overview}} of {{Hierarchical Reinforcement Learning}}},\n author = {Klissarov, Martin and Bagaria, Akhil and Luo, Ziyan and Konidaris, George and Precup, Doina and Machado, Marlos C.},\n year = {2025},\n doi = {10.48550/arXiv.2506.14045},\n urldate = {2025-06-27},\n number = {arXiv:2506.14045},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/5WDE2SRD/Klissarov et al. - 2025 - Discovering Temporal Structure An Overview of Hie.pdf},\n keywords = {Computer Science - Artificial Intelligence},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.},\n primaryclass = {cs},\n eprint = {2506.14045},\n month = {June},\n shorttitle = {Discovering {{Temporal Structure}}}\n}"}},{"title":"Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning","uri":"yoonFastMonteCarlo2025","taxon":"Reference","tags":[],"route":"/yoonFastMonteCarlo2025/","metas":{"doi":"10.48550/arXiv.2506.09498","external":"https://arxiv.org/abs/2506.09498","bibtex":"@misc{yoonFastMonteCarlo2025,\n title = {Fast {{Monte Carlo Tree Diffusion}}: 100x {{Speedup}} via {{Parallel Sparse Planning}}},\n author = {Yoon, Jaesik and Cho, Hyeonseo and Bengio, Yoshua and Ahn, Sungjin},\n year = {2025},\n doi = {10.48550/arXiv.2506.09498},\n urldate = {2025-06-25},\n number = {arXiv:2506.09498},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/59RZRQQT/Yoon et al. - 2025 - Fast Monte Carlo Tree Diffusion 100x Speedup via .pdf},\n keywords = {Computer Science - Artificial Intelligence},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combining diffusion with tree-based search, achieving state-of-the-art performance on complex planning problems. Despite its strengths, our analysis shows that MCTD incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising. To address this, we propose Fast-MCTD, a more efficient variant that preserves the strengths of MCTD while significantly improving its speed and scalability. Fast-MCTD integrates two techniques: Parallel MCTD, which enables parallel rollouts via delayed tree updates and redundancy-aware selection; and Sparse MCTD, which reduces rollout length through trajectory coarsening. Experiments show that Fast-MCTD achieves up to 100{\\texttimes} speedup over standard MCTD while maintaining or improving planning performance. Remarkably, it even outperforms Diffuser in inference speed on some tasks, despite Diffuser requiring no search and yielding weaker solutions. These results position Fast-MCTD as a practical and scalable solution for diffusion-based inference-time reasoning.},\n primaryclass = {cs},\n eprint = {2506.09498},\n month = {June},\n shorttitle = {Fast {{Monte Carlo Tree Diffusion}}}\n}"}},{"title":"General agents need world models","uri":"richensGeneralAgentsNeed2025","taxon":"Reference","tags":[],"route":"/richensGeneralAgentsNeed2025/","metas":{"doi":"10.48550/arXiv.2506.01622","external":"https://arxiv.org/abs/2506.01622","bibtex":"@misc{richensGeneralAgentsNeed2025,\n title = {General Agents Need World Models},\n author = {Richens, Jonathan and Abel, David and Bellot, Alexis and Everitt, Tom},\n year = {2025},\n doi = {10.48550/arXiv.2506.01622},\n urldate = {2025-06-05},\n number = {arXiv:2506.01622},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/XJACDMMA/Richens et al. - 2025 - General agents need world models.pdf;/home/kellen/Downloads/pdfs/storage/9PQHSPK2/2506.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.},\n primaryclass = {cs},\n eprint = {2506.01622},\n month = {June}\n}"}},{"title":"General agents need world models","uri":"richensGeneralAgentsNeed2025a","taxon":"Reference","tags":[],"route":"/richensGeneralAgentsNeed2025a/","metas":{"doi":"10.48550/arXiv.2506.01622","external":"https://arxiv.org/abs/2506.01622","bibtex":"@misc{richensGeneralAgentsNeed2025a,\n title = {General Agents Need World Models},\n author = {Richens, Jonathan and Abel, David and Bellot, Alexis and Everitt, Tom},\n year = {2025},\n doi = {10.48550/arXiv.2506.01622},\n urldate = {2025-06-11},\n number = {arXiv:2506.01622},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/3NQTKCFM/Richens et al. - 2025 - General agents need world models.pdf;/home/kellen/Downloads/pdfs/storage/QUPA8N7V/2506.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.},\n primaryclass = {cs},\n eprint = {2506.01622},\n month = {June}\n}"}},{"title":"Horizon Reduction Makes RL Scalable","uri":"parkHorizonReductionMakes2025","taxon":"Reference","tags":[],"route":"/parkHorizonReductionMakes2025/","metas":{"doi":"10.48550/arXiv.2506.04168","external":"https://arxiv.org/abs/2506.04168","bibtex":"@misc{parkHorizonReductionMakes2025,\n title = {Horizon {{Reduction Makes RL Scalable}}},\n author = {Park, Seohong and Frans, Kevin and Mann, Deepinder and Eysenbach, Benjamin and Kumar, Aviral and Levine, Sergey},\n year = {2025},\n doi = {10.48550/arXiv.2506.04168},\n urldate = {2025-06-14},\n number = {arXiv:2506.04168},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GZMYP9DB/Park et al. - 2025 - Horizon Reduction Makes RL Scalable.pdf;/home/kellen/Downloads/pdfs/storage/GF47MGGU/2506.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000x larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL. Code: https://github.com/seohongpark/horizon-reduction},\n primaryclass = {cs},\n eprint = {2506.04168},\n month = {June}\n}"}},{"title":"Horizon Reduction Makes RL Scalable","uri":"parkHorizonReductionMakes2025a","taxon":"Reference","tags":[],"route":"/parkHorizonReductionMakes2025a/","metas":{"doi":"10.48550/arXiv.2506.04168","external":"https://arxiv.org/abs/2506.04168","bibtex":"@misc{parkHorizonReductionMakes2025a,\n title = {Horizon {{Reduction Makes RL Scalable}}},\n author = {Park, Seohong and Frans, Kevin and Mann, Deepinder and Eysenbach, Benjamin and Kumar, Aviral and Levine, Sergey},\n year = {2025},\n doi = {10.48550/arXiv.2506.04168},\n urldate = {2025-07-01},\n number = {arXiv:2506.04168},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/ZTBUYVHS/Park et al. - 2025 - Horizon Reduction Makes RL Scalable.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000x larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL. Code: https://github.com/seohongpark/horizon-reduction},\n primaryclass = {cs},\n eprint = {2506.04168},\n month = {June}\n}"}},{"title":"Intention-Conditioned Flow Occupancy Models","uri":"zhengIntentionConditionedFlowOccupancy2025","taxon":"Reference","tags":[],"route":"/zhengIntentionConditionedFlowOccupancy2025/","metas":{"doi":"10.48550/arXiv.2506.08902","external":"https://arxiv.org/abs/2506.08902","bibtex":"@misc{zhengIntentionConditionedFlowOccupancy2025,\n title = {Intention-{{Conditioned Flow Occupancy Models}}},\n author = {Zheng, Chongyi and Park, Seohong and Levine, Sergey and Eysenbach, Benjamin},\n year = {2025},\n doi = {10.48550/arXiv.2506.08902},\n urldate = {2025-06-25},\n number = {arXiv:2506.08902},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/MGADHFSN/Zheng et al. - 2025 - Intention-Conditioned Flow Occupancy Models.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on 36 state-based and 4 image-based benchmark tasks demonstrate that the proposed method achieves 1.8{\\texttimes} median improvement in returns and increases success rates by 36\\%.},\n primaryclass = {cs},\n eprint = {2506.08902},\n month = {June}\n}"}},{"title":"Normalizing Flows are Capable Models for RL","uri":"ghugareNormalizingFlowsAre2025","taxon":"Reference","tags":[],"route":"/ghugareNormalizingFlowsAre2025/","metas":{"doi":"10.48550/arXiv.2505.23527","external":"https://arxiv.org/abs/2505.23527","bibtex":"@misc{ghugareNormalizingFlowsAre2025,\n title = {Normalizing {{Flows}} Are {{Capable Models}} for {{RL}}},\n author = {Ghugare, Raj and Eysenbach, Benjamin},\n year = {2025},\n doi = {10.48550/arXiv.2505.23527},\n urldate = {2025-06-30},\n number = {arXiv:2505.23527},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IW3SLYLD/Ghugare and Eysenbach - 2025 - Normalizing Flows are Capable Models for RL.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. To this end, RL researchers often choose to pay the price of accommodating these models into their algorithms -- diffusion models are expressive, but are computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs), by contrast, seem to provide an appealing alternative, as they enable likelihoods and sampling without solving differential equations or autoregressive architectures. However, their potential in RL has received limited attention, partly due to the prevailing belief that normalizing flows lack sufficient expressivity. We show that this is not the case. Building on recent work in NFs, we propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL.},\n primaryclass = {cs},\n eprint = {2505.23527},\n month = {June}\n}"}},{"title":"Q-learning is not yet scalable","uri":"parkQlearningNotScalable2025","taxon":"Reference","tags":[],"route":"/parkQlearningNotScalable2025/","metas":{"bibtex":"@misc{parkQlearningNotScalable2025,\n title = {Q-Learning Is Not yet Scalable},\n author = {Park, Seohong},\n year = {2025},\n month = {June}\n}"}},{"title":"Flow Q-Learning","uri":"parkFlowQLearning2025","taxon":"Reference","tags":[],"route":"/parkFlowQLearning2025/","metas":{"doi":"10.48550/arXiv.2502.02538","external":"https://arxiv.org/abs/2502.02538","bibtex":"@misc{parkFlowQLearning2025,\n title = {Flow {{Q-Learning}}},\n author = {Park, Seohong and Li, Qiyang and Levine, Sergey},\n year = {2025},\n doi = {10.48550/arXiv.2502.02538},\n urldate = {2025-07-01},\n number = {arXiv:2502.02538},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/83WF47FI/Park et al. - 2025 - Flow Q-Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/},\n primaryclass = {cs},\n eprint = {2502.02538},\n month = {May}\n}"}},{"title":"Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn","uri":"tangMitigatingPlasticityLoss2025","taxon":"Reference","tags":[],"route":"/tangMitigatingPlasticityLoss2025/","metas":{"doi":"10.48550/arXiv.2506.00592","external":"https://arxiv.org/abs/2506.00592","bibtex":"@misc{tangMitigatingPlasticityLoss2025,\n title = {Mitigating {{Plasticity Loss}} in {{Continual Reinforcement Learning}} by {{Reducing Churn}}},\n author = {Tang, Hongyao and {Obando-Ceron}, Johan and Castro, Pablo Samuel and Courville, Aaron and Berseth, Glen},\n year = {2025},\n doi = {10.48550/arXiv.2506.00592},\n urldate = {2025-06-27},\n number = {arXiv:2506.00592},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/PWAQ3ZB5/Tang et al. - 2025 - Mitigating Plasticity Loss in Continual Reinforcem.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.},\n primaryclass = {cs},\n eprint = {2506.00592},\n month = {May}\n}"}},{"title":"Reward-Aware Proto-Representations in Reinforcement Learning","uri":"tseRewardAwareProtoRepresentationsReinforcement2025","taxon":"Reference","tags":[],"route":"/tseRewardAwareProtoRepresentationsReinforcement2025/","metas":{"doi":"10.48550/arXiv.2505.16217","external":"https://arxiv.org/abs/2505.16217","bibtex":"@misc{tseRewardAwareProtoRepresentationsReinforcement2025,\n title = {Reward-{{Aware Proto-Representations}} in {{Reinforcement Learning}}},\n author = {Tse, Hon Tik and Chandrasekar, Siddarth and Machado, Marlos C.},\n year = {2025},\n doi = {10.48550/arXiv.2505.16217},\n urldate = {2025-06-27},\n number = {arXiv:2505.16217},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/V4VASC3J/Tse et al. - 2025 - Reward-Aware Proto-Representations in Reinforcemen.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In recent years, the successor representation (SR) has attracted increasing attention in reinforcement learning (RL), and it has been used to address some of its key challenges, such as exploration, credit assignment, and generalization. The SR can be seen as representing the underlying credit assignment structure of the environment by implicitly encoding its induced transition dynamics. However, the SR is reward-agnostic. In this paper, we discuss a similar representation that also takes into account the reward dynamics of the problem. We study the default representation (DR), a recently proposed representation with limited theoretical (and empirical) analysis. Here, we lay some of the theoretical foundation underlying the DR in the tabular case by (1) deriving dynamic programming and (2) temporaldifference methods to learn the DR, (3) characterizing the basis for the vector space of the DR, and (4) formally extending the DR to the function approximation case through default features. Empirically, we analyze the benefits of the DR in many of the settings in which the SR has been applied, including (1) reward shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our results show that, compared to the SR, the DR gives rise to qualitatively different, reward-aware behaviour and quantitatively better performance in several settings.},\n primaryclass = {cs},\n eprint = {2505.16217},\n month = {May}\n}"}},{"title":"Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics","uri":"bobrinZeroShotAdaptationBehavioral2025","taxon":"Reference","tags":[],"route":"/bobrinZeroShotAdaptationBehavioral2025/","metas":{"doi":"10.48550/arXiv.2505.13150","external":"https://arxiv.org/abs/2505.13150","bibtex":"@misc{bobrinZeroShotAdaptationBehavioral2025,\n title = {Zero-{{Shot Adaptation}} of {{Behavioral Foundation Models}} to {{Unseen Dynamics}}},\n author = {Bobrin, Maksim and Zisman, Ilya and Nikulin, Alexander and Kurenkov, Vladislav and Dylov, Dmitry},\n year = {2025},\n doi = {10.48550/arXiv.2505.13150},\n urldate = {2025-06-05},\n number = {arXiv:2505.13150},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/B7PR392U/Bobrin et al. - 2025 - Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics.pdf;/home/kellen/Downloads/pdfs/storage/A6XW86X2/2505.html},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Behavioral Foundation Models (BFMs) proved successful in producing policies for arbitrary tasks in a zero-shot manner, requiring no test-time training or task-specific fine-tuning. Among the most promising BFMs are the ones that estimate the successor measure learned in an unsupervised way from task-agnostic offline data. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward-Backward (FB) representation, one of the methods from the BFM family, cannot distinguish between distinct dynamics, leading to an interference among the latent directions, which parametrize different policies. To address this, we propose a FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. We also show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. These traits allow our method to respond to the dynamics observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.},\n primaryclass = {cs},\n eprint = {2505.13150},\n month = {May}\n}"}},{"title":"https://kellenkanarios.com/007F/","uri":"007F","taxon":"Theorem","tags":[],"route":"/007F/","metas":{}},{"title":"Differential Entropy","uri":"007C","taxon":"Definition","tags":[],"route":"/007C/","metas":{}},{"title":"Discrete Approximation of Differential Entropy","uri":"007G","taxon":"Definition","tags":[],"route":"/007G/","metas":{}},{"title":"Information dimension","uri":"007H","taxon":"Definition","tags":[],"route":"/007H/","metas":{}},{"title":"Multivariate Normal Distribution","uri":"007I","taxon":"Definition","tags":[],"route":"/007I/","metas":{}},{"title":"Typical Set for Continuous Random Variables","uri":"007E","taxon":"Definition","tags":["8.11"],"route":"/007E/","metas":{}},{"title":"Volume of set","uri":"007D","taxon":"Definition","tags":[],"route":"/007D/","metas":{}},{"title":"Retraction","uri":"0076","taxon":"Definition","tags":[],"route":"/0076/","metas":{}},{"title":"Riemannian Gradient","uri":"0077","taxon":"Definition","tags":[],"route":"/0077/","metas":{}},{"title":"Riemannian Gradient Descent","uri":"007B","taxon":"Definition","tags":[],"route":"/007B/","metas":{}},{"title":"Riemannian Hessian","uri":"007A","taxon":"Definition","tags":[],"route":"/007A/","metas":{}},{"title":"Riemannian Manifold","uri":"0072","taxon":"Definition","tags":[],"route":"/0072/","metas":{}},{"title":"Riemannian Metric","uri":"0079","taxon":"Definition","tags":[],"route":"/0079/","metas":{}},{"title":"Riemmanian Metric","uri":"0078","taxon":"Claim","tags":[],"route":"/0078/","metas":{}},{"title":"Smooth Manifold","uri":"0073","taxon":"Definition","tags":[],"route":"/0073/","metas":{}},{"title":"Tangent Bundle","uri":"0074","taxon":"Definition","tags":[],"route":"/0074/","metas":{}},{"title":"Tangent Space","uri":"0075","taxon":"Definition","tags":[],"route":"/0075/","metas":{}},{"title":"Encoder","uri":"0071","taxon":"Definition","tags":[],"route":"/0071/","metas":{}},{"title":"General definition of channel","uri":"0070","taxon":"Definition","tags":[],"route":"/0070/","metas":{}},{"title":"Coming Soon!!!","uri":"006Z","taxon":null,"tags":[],"route":"/006Z/","metas":{}},{"title":"Achievable rates","uri":"006U","taxon":"Definition","tags":[],"route":"/006U/","metas":{}},{"title":"Channel capacity","uri":"006V","taxon":"Definition","tags":[],"route":"/006V/","metas":{}},{"title":"Channel coding problem","uri":"006S","taxon":"Definition","tags":[],"route":"/006S/","metas":{}},{"title":"Discrete memoryless channel","uri":"006T","taxon":"Definition","tags":[],"route":"/006T/","metas":{}},{"title":"Information channel capacity","uri":"006W","taxon":"Definition","tags":[],"route":"/006W/","metas":{}},{"title":"Representation Learning via Non-Contrastive Mutual Information","uri":"guoRepresentationLearningNonContrastive2025","taxon":"Reference","tags":[],"route":"/guoRepresentationLearningNonContrastive2025/","metas":{"doi":"10.48550/arXiv.2504.16667","external":"https://arxiv.org/abs/2504.16667","bibtex":"@misc{guoRepresentationLearningNonContrastive2025,\n title = {Representation {{Learning}} via {{Non-Contrastive Mutual Information}}},\n author = {Guo, Zhaohan Daniel and Pires, Bernardo Avila and Khetarpal, Khimya and Schuurmans, Dale and Dai, Bo},\n year = {2025},\n doi = {10.48550/arXiv.2504.16667},\n urldate = {2025-05-29},\n number = {arXiv:2504.16667},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/FX33U8RF/Guo et al. - 2025 - Representation Learning via Non-Contrastive Mutual Information.pdf;/home/kellen/Downloads/pdfs/storage/MYI2C56X/2504.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline.},\n primaryclass = {cs},\n eprint = {2504.16667},\n month = {April}\n}"}},{"title":"Shannon's coding theorem","uri":"006X","taxon":"Theorem","tags":[],"route":"/006X/","metas":{}},{"title":"Symmetric Channels","uri":"006Y","taxon":"Definition","tags":[],"route":"/006Y/","metas":{}},{"title":"A Dynamic Duo: Tree Search + RL","uri":"006Q","taxon":null,"tags":[],"route":"/006Q/","metas":{}},{"title":"Alpha-Zero","uri":"006R","taxon":null,"tags":[],"route":"/006R/","metas":{}},{"title":"https://kellenkanarios.com/006O/","uri":"006O","taxon":"Theorem","tags":[],"route":"/006O/","metas":{}},{"title":"https://kellenkanarios.com/006M/","uri":"006M","taxon":"Proposition","tags":[],"route":"/006M/","metas":{}},{"title":"https://kellenkanarios.com/006L/","uri":"006L","taxon":"Theorem","tags":[],"route":"/006L/","metas":{}},{"title":"Fixed rate universal code","uri":"006K","taxon":"Definition","tags":[],"route":"/006K/","metas":{}},{"title":"Lempel-Ziv Algorithm","uri":"006P","taxon":null,"tags":[],"route":"/006P/","metas":{}},{"title":"Minimax Redundancy","uri":"006N","taxon":"Definition","tags":[],"route":"/006N/","metas":{}},{"title":"Optimal Bayesian Error Exponent","uri":"006I","taxon":"Definition","tags":[],"route":"/006I/","metas":{}},{"title":"Optimal Bayesian Error Exponent","uri":"006J","taxon":"Theorem","tags":[],"route":"/006J/","metas":{}},{"title":"Optimal Bayesian Test","uri":"006F","taxon":"Proposition","tags":[],"route":"/006F/","metas":{}},{"title":"Optimal Error Exponent in NP setting","uri":"006G","taxon":"Definition","tags":[],"route":"/006G/","metas":{}},{"title":"Stein's Lemma","uri":"006H","taxon":"Theorem","tags":[],"route":"/006H/","metas":{}},{"title":"https://kellenkanarios.com/0065/","uri":"0065","taxon":"Lemma","tags":[],"route":"/0065/","metas":{}},{"title":"https://kellenkanarios.com/0063/","uri":"0063","taxon":"Proposition","tags":[],"route":"/0063/","metas":{}},{"title":"https://kellenkanarios.com/0062/","uri":"0062","taxon":"Proposition","tags":[],"route":"/0062/","metas":{}},{"title":"https://kellenkanarios.com/005V/","uri":"005V","taxon":null,"tags":[],"route":"/005V/","metas":{}},{"title":"https://kellenkanarios.com/0064/","uri":"0064","taxon":"Theorem","tags":[],"route":"/0064/","metas":{}},{"title":"Bayesian Framework","uri":"006D","taxon":"Definition","tags":[],"route":"/006D/","metas":{}},{"title":"Huffman Code","uri":"005Z","taxon":null,"tags":[],"route":"/005Z/","metas":{}},{"title":"Huffman Code","uri":"005W","taxon":"Definition","tags":[],"route":"/005W/","metas":{}},{"title":"Hypothesis error","uri":"006B","taxon":"Definition","tags":[],"route":"/006B/","metas":{}},{"title":"Hypothesis test","uri":"006A","taxon":"Definition","tags":[],"route":"/006A/","metas":{}},{"title":"Hypothesis testing problem","uri":"0069","taxon":"Definition","tags":[],"route":"/0069/","metas":{}},{"title":"Minimax Optimal","uri":"006E","taxon":"Definition","tags":[],"route":"/006E/","metas":{}},{"title":"Neyman-Pearson Framework","uri":"006C","taxon":"Definition","tags":[],"route":"/006C/","metas":{}},{"title":"Optimal NP Test","uri":"0068","taxon":"Theorem","tags":[],"route":"/0068/","metas":{}},{"title":"Optimality of Huffman Code","uri":"0060","taxon":"Theorem","tags":[],"route":"/0060/","metas":{}},{"title":"Sanov's Theorem","uri":"0067","taxon":"Theorem","tags":[],"route":"/0067/","metas":{}},{"title":"Set of all types","uri":"005Y","taxon":"Definition","tags":[],"route":"/005Y/","metas":{}},{"title":"The probability of a type class","uri":"0066","taxon":"Theorem","tags":[],"route":"/0066/","metas":{}},{"title":"Type classes","uri":"0061","taxon":"Definition","tags":[],"route":"/0061/","metas":{}},{"title":"type or empirical distribution","uri":"005X","taxon":"Definition","tags":[],"route":"/005X/","metas":{}},{"title":"Singularity of a code","uri":"005U","taxon":"Definition","tags":[],"route":"/005U/","metas":{}},{"title":"https://kellenkanarios.com/005K/","uri":"005K","taxon":"Definition","tags":[],"route":"/005K/","metas":{}},{"title":"https://kellenkanarios.com/005J/","uri":"005J","taxon":null,"tags":[],"route":"/005J/","metas":{}},{"title":"https://kellenkanarios.com/005L/","uri":"005L","taxon":"Definition","tags":[],"route":"/005L/","metas":{}},{"title":"https://kellenkanarios.com/005S/","uri":"005S","taxon":"Theorem","tags":[],"route":"/005S/","metas":{}},{"title":"https://kellenkanarios.com/005I/","uri":"005I","taxon":"Theorem","tags":[],"route":"/005I/","metas":{}},{"title":"Extensions and Unique Decodability","uri":"005M","taxon":"Definition","tags":[],"route":"/005M/","metas":{}},{"title":"Kraft's inequality","uri":"005O","taxon":"Theorem","tags":[],"route":"/005O/","metas":{}},{"title":"Shannon Fano Elias Code","uri":"005T","taxon":null,"tags":["2025-03-23"],"route":"/005T/","metas":{}},{"title":"Variable minimum rate","uri":"005R","taxon":"Definition","tags":[],"route":"/005R/","metas":{}},{"title":"Variable rate achievability","uri":"005Q","taxon":"Definition","tags":[],"route":"/005Q/","metas":{}},{"title":"Variable rate code","uri":"005P","taxon":"Definition","tags":[],"route":"/005P/","metas":{}},{"title":"instantaneous / prefix-free code","uri":"005N","taxon":"Definition","tags":[],"route":"/005N/","metas":{}},{"title":"https://kellenkanarios.com/005H/","uri":"005H","taxon":null,"tags":[],"route":"/005H/","metas":{}},{"title":"Papers of a Week","uri":"005G","taxon":null,"tags":[],"route":"/005G/","metas":{}},{"title":"Rebuilding My (Neo)Vim Config From Scratch","uri":"005F","taxon":null,"tags":[],"route":"/005F/","metas":{}},{"title":"1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities","uri":"wang1000LayerNetworks2025","taxon":"Reference","tags":[],"route":"/wang1000LayerNetworks2025/","metas":{"doi":"10.48550/arXiv.2503.14858","external":"https://arxiv.org/abs/2503.14858","bibtex":"@misc{wang1000LayerNetworks2025,\n title = {1000 {{Layer Networks}} for {{Self-Supervised RL}}: {{Scaling Depth Can Enable New Goal-Reaching Capabilities}}},\n author = {Wang, Kevin and Javali, Ishaan and Bortkiewicz, Micha{\\l} and Trzci{\\'n}ski, Tomasz and Eysenbach, Benjamin},\n year = {2025},\n doi = {10.48550/arXiv.2503.14858},\n urldate = {2025-06-15},\n number = {arXiv:2503.14858},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/ZF9NUUCX/Wang et al. - 2025 - 1000 Layer Networks for Self-Supervised RL Scaling Depth Can Enable New Goal-Reaching Capabilities.pdf;/home/kellen/Downloads/pdfs/storage/6J3X7MF5/2503.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 - 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance by \\$2{\\textbackslash}times\\$ - \\$50{\\textbackslash}times\\$. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.},\n primaryclass = {cs},\n eprint = {2503.14858},\n month = {March},\n shorttitle = {1000 {{Layer Networks}} for {{Self-Supervised RL}}}\n}"}},{"title":"Temporal Difference Flows","uri":"farebrotherTemporalDifferenceFlows2025","taxon":"Reference","tags":[],"route":"/farebrotherTemporalDifferenceFlows2025/","metas":{"doi":"10.48550/arXiv.2503.09817","external":"https://arxiv.org/abs/2503.09817","bibtex":"@misc{farebrotherTemporalDifferenceFlows2025,\n title = {Temporal {{Difference Flows}}},\n author = {Farebrother, Jesse and Pirotta, Matteo and Tirinzoni, Andrea and Munos, R{\\'e}mi and Lazaric, Alessandro and Touati, Ahmed},\n year = {2025},\n doi = {10.48550/arXiv.2503.09817},\n urldate = {2025-06-14},\n number = {arXiv:2503.09817},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2V2SR66H/Farebrother et al. - 2025 - Temporal Difference Flows.pdf;/home/kellen/Downloads/pdfs/storage/D27PSL2L/2503.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Predictive models of the future are fundamental for an agent's ability to reason and plan. A common strategy learns a world model and unrolls it step-by-step at inference, where small errors can rapidly compound. Geometric Horizon Models (GHMs) offer a compelling alternative by directly making predictions of future states, avoiding cumulative inference errors. While GHMs can be conveniently learned by a generative analog to temporal difference (TD) learning, existing methods are negatively affected by bootstrapping predictions at train time and struggle to generate high-quality predictions at long horizons. This paper introduces Temporal Difference Flows (TD-Flow), which leverages the structure of a novel Bellman equation on probability paths alongside flow-matching techniques to learn accurate GHMs at over 5x the horizon length of prior methods. Theoretically, we establish a new convergence result and primarily attribute TD-Flow's efficacy to reduced gradient variance during training. We further show that similar arguments can be extended to diffusion-based methods. Empirically, we validate TD-Flow across a diverse set of domains on both generative metrics and downstream tasks including policy evaluation. Moreover, integrating TD-Flow with recent behavior foundation models for planning over pre-trained policies demonstrates substantial performance gains, underscoring its promise for long-horizon decision-making.},\n primaryclass = {cs},\n eprint = {2503.09817},\n month = {March}\n}"}},{"title":"Optimization from a Deep Learning Perspective","uri":"005D","taxon":null,"tags":["blog","upcoming"],"route":"/005D/","metas":{}},{"title":"Achievability","uri":"005A","taxon":"Definition","tags":[],"route":"/005A/","metas":{}},{"title":"Coding system","uri":"0059","taxon":"Definition","tags":[],"route":"/0059/","metas":{}},{"title":"Entropy as MRSC","uri":"005C","taxon":"Theorem","tags":[],"route":"/005C/","metas":{}},{"title":"Minimum rate of source coding","uri":"005B","taxon":"Definition","tags":[],"route":"/005B/","metas":{}},{"title":"https://kellenkanarios.com/0058/","uri":"0058","taxon":"Theorem","tags":[],"route":"/0058/","metas":{}},{"title":"https://kellenkanarios.com/0052/","uri":"0052","taxon":"Corollary","tags":[],"route":"/0052/","metas":{}},{"title":"AEP Theorem","uri":"0054","taxon":"Theorem","tags":[],"route":"/0054/","metas":{}},{"title":"Continuous Mapping Theorem","uri":"0053","taxon":"Theorem","tags":[],"route":"/0053/","metas":{}},{"title":"Decomposition Theorem","uri":"0057","taxon":"Theorem","tags":[],"route":"/0057/","metas":{}},{"title":"High probability sets","uri":"0056","taxon":"Corollary","tags":[],"route":"/0056/","metas":{}},{"title":"Weak Law of Large Numbers","uri":"0051","taxon":"Theorem","tags":[],"route":"/0051/","metas":{}},{"title":"\\epsilon -typical set","uri":"0055","taxon":null,"tags":[],"route":"/0055/","metas":{}},{"title":"Data Processing Inequality","uri":"004Z","taxon":"Theorem","tags":[],"route":"/004Z/","metas":{}},{"title":"Fano's Inequality","uri":"0050","taxon":null,"tags":[],"route":"/0050/","metas":{}},{"title":"Maximum Aposteriori Estimator","uri":"004Y","taxon":"Theorem","tags":[],"route":"/004Y/","metas":{}},{"title":"A Note on Advantage Estimation","uri":"004X","taxon":null,"tags":["blog","upcoming"],"route":"/004X/","metas":{}},{"title":"Armijo Condition","uri":"004R","taxon":"Definition","tags":[],"route":"/004R/","metas":{}},{"title":"Convergence of GD with backtracking","uri":"004T","taxon":"Theorem","tags":[],"route":"/004T/","metas":{}},{"title":"Convergence of accelerated GD","uri":"004P","taxon":"Theorem","tags":[],"route":"/004P/","metas":{}},{"title":"Exact Linesearch","uri":"004Q","taxon":"Definition","tags":[],"route":"/004Q/","metas":{}},{"title":"Linear convergence for strong convexity","uri":"004U","taxon":"Theorem","tags":[],"route":"/004U/","metas":{}},{"title":"Nesterov's Method","uri":"004O","taxon":null,"tags":[],"route":"/004O/","metas":{}},{"title":"Newton's Method","uri":"004V","taxon":null,"tags":[],"route":"/004V/","metas":{}},{"title":"Quasi-Newton's Method","uri":"004W","taxon":null,"tags":[],"route":"/004W/","metas":{}},{"title":"Sublinear convergence of GD","uri":"004M","taxon":"Theorem","tags":[],"route":"/004M/","metas":{}},{"title":"Suboptimality of Gradient Descent","uri":"004N","taxon":"Theorem","tags":[],"route":"/004N/","metas":{}},{"title":"Sufficient Value","uri":"004L","taxon":"Lemma","tags":[],"route":"/004L/","metas":{}},{"title":"Wolfe condition","uri":"004S","taxon":"Definition","tags":[],"route":"/004S/","metas":{}},{"title":"Word Embeddings","uri":"004J","taxon":null,"tags":[],"route":"/004J/","metas":{}},{"title":"forester2html","uri":"004K","taxon":null,"tags":[],"route":"/004K/","metas":{}},{"title":"The Computer in Computer Science","uri":"004H","taxon":null,"tags":["project","top","systems","systems"],"route":"/004H/","metas":{}},{"title":"Virtual Memory","uri":"004I","taxon":null,"tags":[],"route":"/004I/","metas":{}},{"title":"Convex Function","uri":"004D","taxon":"Definition","tags":[],"route":"/004D/","metas":{}},{"title":"Convex set","uri":"004C","taxon":"Definition","tags":[],"route":"/004C/","metas":{}},{"title":"DeepSeek-V3 Technical Report","uri":"deepseek-aiDeepSeekV3TechnicalReport2025","taxon":"Reference","tags":[],"route":"/deepseek-aiDeepSeekV3TechnicalReport2025/","metas":{"doi":"10.48550/arXiv.2412.19437","external":"https://arxiv.org/abs/2412.19437","bibtex":"@misc{deepseek-aiDeepSeekV3TechnicalReport2025,\n title = {{{DeepSeek-V3 Technical Report}}},\n author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},\n year = {2025},\n doi = {10.48550/arXiv.2412.19437},\n urldate = {2025-06-27},\n number = {arXiv:2412.19437},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/6FVL6N8T/DeepSeek-AI et al. - 2025 - DeepSeek-V3 Technical Report.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},\n primaryclass = {cs},\n eprint = {2412.19437},\n month = {February}\n}"}},{"title":"Gibb's inequality","uri":"004E","taxon":"Theorem","tags":[],"route":"/004E/","metas":{}},{"title":"Improving Transformer World Models for Data-Efficient RL","uri":"dedieuImprovingTransformerWorld2025","taxon":"Reference","tags":[],"route":"/dedieuImprovingTransformerWorld2025/","metas":{"doi":"10.48550/arXiv.2502.01591","external":"https://arxiv.org/abs/2502.01591","bibtex":"@misc{dedieuImprovingTransformerWorld2025,\n title = {Improving {{Transformer World Models}} for {{Data-Efficient RL}}},\n author = {Dedieu, Antoine and Ortiz, Joseph and Lou, Xinghua and Wendelken, Carter and Lehrach, Wolfgang and Guntupalli, J. Swaroop and {Lazaro-Gredilla}, Miguel and Murphy, Kevin Patrick},\n year = {2025},\n doi = {10.48550/arXiv.2502.01591},\n urldate = {2025-02-16},\n number = {arXiv:2502.01591},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/ICQPUZXS/Dedieu et al. - 2025 - Improving Transformer World Models for Data-Efficient RL.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4\\% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2\\%, and, for the first time, exceeds human performance of 65.0\\%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) \"Dyna with warmup\", which trains the policy on real and imaginary data, (b) \"nearest neighbor tokenizer\" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) \"block teacher forcing\", which allows the TWM to reason jointly about the future tokens of the next timestep.},\n primaryclass = {cs},\n eprint = {2502.01591},\n month = {February}\n}"}},{"title":"Log-Sum Inequality","uri":"004F","taxon":null,"tags":[],"route":"/004F/","metas":{}},{"title":"Muon is Scalable for LLM Training","uri":"liuMuonScalableLLM2025","taxon":"Reference","tags":[],"route":"/liuMuonScalableLLM2025/","metas":{"doi":"10.48550/arXiv.2502.16982","external":"https://arxiv.org/abs/2502.16982","bibtex":"@misc{liuMuonScalableLLM2025,\n title = {Muon Is {{Scalable}} for {{LLM Training}}},\n author = {Liu, Jingyuan and Su, Jianlin and Yao, Xingcheng and Jiang, Zhejun and Lai, Guokun and Du, Yulun and Qin, Yidao and Xu, Weixin and Lu, Enzhe and Yan, Junjie and Chen, Yanru and Zheng, Huabin and Liu, Yibo and Liu, Shaowei and Yin, Bohong and He, Weiran and Zhu, Han and Wang, Yuzhi and Wang, Jianzhou and Dong, Mengnan and Zhang, Zheng and Kang, Yongsheng and Zhang, Hao and Xu, Xinran and Zhang, Yutao and Wu, Yuxin and Zhou, Xinyu and Yang, Zhilin},\n year = {2025},\n doi = {10.48550/arXiv.2502.16982},\n urldate = {2025-02-26},\n number = {arXiv:2502.16982},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/JSHH2RNX/Liu et al. - 2025 - Muon is Scalable for LLM Training.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Recently, the Muon optimizer (K. Jordan et al. 2024) based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-ofthe-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves {$\\sim$} 2{\\texttimes} computational efficiency compared to AdamW with compute optimal training. Based on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models. We open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.},\n primaryclass = {cs},\n eprint = {2502.16982},\n month = {February}\n}"}},{"title":"Notebook: Information Theory","uri":"004B","taxon":null,"tags":["note","top"],"route":"/004B/","metas":{}},{"title":"Self-Attention","uri":"004G","taxon":null,"tags":["llms"],"route":"/004G/","metas":{}},{"title":"https://kellenkanarios.com/0048/","uri":"0048","taxon":"Definition","tags":[],"route":"/0048/","metas":{}},{"title":"https://kellenkanarios.com/0049/","uri":"0049","taxon":"Lemma","tags":[],"route":"/0049/","metas":{}},{"title":"https://kellenkanarios.com/0041/","uri":"0041","taxon":"Theorem","tags":[],"route":"/0041/","metas":{}},{"title":"Chapman-Kolmogorov Equation","uri":"0042","taxon":"Definition","tags":[],"route":"/0042/","metas":{}},{"title":"Communicates","uri":"0046","taxon":"Definition","tags":[],"route":"/0046/","metas":{}},{"title":"Group Relative Policy Optimization","uri":"003X","taxon":null,"tags":["rl","llms"],"route":"/003X/","metas":{}},{"title":"Markov Chain","uri":"0040","taxon":"Definition","tags":[],"route":"/0040/","metas":{}},{"title":"Notebook: Stochastic Processes","uri":"003Z","taxon":null,"tags":["note","prob","top"],"route":"/003Z/","metas":{}},{"title":"Old Talks","uri":"004A","taxon":null,"tags":[],"route":"/004A/","metas":{}},{"title":"Stopping time","uri":"0043","taxon":"Definition","tags":[],"route":"/0043/","metas":{}},{"title":"Strong Markov Property","uri":"0044","taxon":"Theorem","tags":[],"route":"/0044/","metas":{}},{"title":"The History and Evolution of Policy Gradient Algorithms","uri":"003Y","taxon":null,"tags":["blog","upcoming","rl"],"route":"/003Y/","metas":{}},{"title":"Transient and recurrent states","uri":"0045","taxon":"Definition","tags":[],"route":"/0045/","metas":{}},{"title":"Transitivity of communcation","uri":"0047","taxon":"Lemma","tags":[],"route":"/0047/","metas":{}},{"title":"Chain Rule of Mutual Info","uri":"003V","taxon":"Theorem","tags":["infot"],"route":"/003V/","metas":{}},{"title":"Threads on the hardware level","uri":"003U","taxon":null,"tags":[],"route":"/003U/","metas":{}},{"title":"Deepseek v1 through R1: RL is back!","uri":"003D","taxon":null,"tags":["blog","llms","upcoming"],"route":"/003D/","metas":{}},{"title":"First-order approximation","uri":"003N","taxon":"Theorem","tags":[],"route":"/003N/","metas":{}},{"title":"First-order convergence","uri":"003R","taxon":"Definition","tags":[],"route":"/003R/","metas":{}},{"title":"Lipschitz Continous","uri":"003J","taxon":"Definition","tags":[],"route":"/003J/","metas":{}},{"title":"Matrix Operator Norm","uri":"003G","taxon":"Definition","tags":[],"route":"/003G/","metas":{}},{"title":"Mean Value Theorem I","uri":"003L","taxon":"Theorem","tags":[],"route":"/003L/","metas":{}},{"title":"Mean Value Theorem II","uri":"003M","taxon":"Theorem","tags":[],"route":"/003M/","metas":{}},{"title":"Q-convergence","uri":"003Q","taxon":"Definition","tags":[],"route":"/003Q/","metas":{}},{"title":"R-convergence","uri":"003S","taxon":"Definition","tags":[],"route":"/003S/","metas":{}},{"title":"Schatten p-norm","uri":"003I","taxon":"Definition","tags":[],"route":"/003I/","metas":{}},{"title":"Second-order approximation","uri":"003O","taxon":"Theorem","tags":[],"route":"/003O/","metas":{}},{"title":"Smooth Function","uri":"003K","taxon":"Definition","tags":[],"route":"/003K/","metas":{}},{"title":"Stationary point","uri":"003P","taxon":"Definition","tags":[],"route":"/003P/","metas":{}},{"title":"Unitary Invariant Matrix Norm","uri":"003H","taxon":"Definition","tags":[],"route":"/003H/","metas":{}},{"title":"Best rank-r approximation","uri":"003C","taxon":"Theorem","tags":[],"route":"/003C/","metas":{}},{"title":"First-order condition","uri":"0034","taxon":"Theorem","tags":["optimization"],"route":"/0034/","metas":{}},{"title":"Notebook: Optimization Theory","uri":"0033","taxon":null,"tags":["note","top"],"route":"/0033/","metas":{}},{"title":"Positive (semi)definiteness","uri":"003A","taxon":"Definition","tags":[],"route":"/003A/","metas":{}},{"title":"Second-order condition","uri":"0035","taxon":"Theorem","tags":["optimization"],"route":"/0035/","metas":{}},{"title":"Singular Value Decomposition","uri":"003B","taxon":"Fact","tags":["optimization"],"route":"/003B/","metas":{}},{"title":"Smooth Problem","uri":"0037","taxon":"Definition","tags":["optimization"],"route":"/0037/","metas":{}},{"title":"Strong convexity","uri":"0036","taxon":"Definition","tags":["optimization"],"route":"/0036/","metas":{}},{"title":"Subdifferential","uri":"0039","taxon":"Definition","tags":["optimization"],"route":"/0039/","metas":{}},{"title":"Subgradient","uri":"0038","taxon":"Definition","tags":["optimization"],"route":"/0038/","metas":{}},{"title":"https://kellenkanarios.com/002Z/","uri":"002Z","taxon":null,"tags":[],"route":"/002Z/","metas":{}},{"title":"Soft Actor Critic","uri":"002T","taxon":null,"tags":["rl"],"route":"/002T/","metas":{}},{"title":"Unreasonable Effectiveness of Eligibility Traces","uri":"002U","taxon":null,"tags":["blog","draft"],"route":"/002U/","metas":{}},{"title":"https://kellenkanarios.com/002M/","uri":"002M","taxon":"Theorem","tags":[],"route":"/002M/","metas":{}},{"title":"Banach Space","uri":"002Q","taxon":null,"tags":[],"route":"/002Q/","metas":{}},{"title":"Hoeffding's Inequality","uri":"002N","taxon":"Theorem","tags":[],"route":"/002N/","metas":{}},{"title":"Holder's Inequality","uri":"002R","taxon":null,"tags":[],"route":"/002R/","metas":{}},{"title":"Linear MDP","uri":"002S","taxon":"Definition","tags":[],"route":"/002S/","metas":{}},{"title":"Sub-Gaussian Norm","uri":"002P","taxon":"Definition","tags":[],"route":"/002P/","metas":{}},{"title":"Sub-Gaussian Random Variables","uri":"002L","taxon":"Definition","tags":[],"route":"/002L/","metas":{}},{"title":"khinchin's Inequality","uri":"002O","taxon":"Corollary","tags":[],"route":"/002O/","metas":{}},{"title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning","uri":"deepseek-aiDeepSeekR1IncentivizingReasoning2025","taxon":"Reference","tags":[],"route":"/deepseek-aiDeepSeekR1IncentivizingReasoning2025/","metas":{"doi":"10.48550/arXiv.2501.12948","external":"https://arxiv.org/abs/2501.12948","bibtex":"@misc{deepseek-aiDeepSeekR1IncentivizingReasoning2025,\n title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},\n author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},\n year = {2025},\n doi = {10.48550/arXiv.2501.12948},\n urldate = {2025-06-27},\n number = {arXiv:2501.12948},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/WM8KGXDY/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},\n primaryclass = {cs},\n eprint = {2501.12948},\n month = {January},\n shorttitle = {{{DeepSeek-R1}}}\n}"}},{"title":"Grokking at the Edge of Numerical Stability","uri":"prietoGrokkingEdgeNumerical2025","taxon":"Reference","tags":[],"route":"/prietoGrokkingEdgeNumerical2025/","metas":{"doi":"10.48550/arXiv.2501.04697","external":"https://arxiv.org/abs/2501.04697","bibtex":"@misc{prietoGrokkingEdgeNumerical2025,\n title = {Grokking at the {{Edge}} of {{Numerical Stability}}},\n author = {Prieto, Lucas and Barsbey, Melih and Mediano, Pedro A. M. and Birdal, Tolga},\n year = {2025},\n doi = {10.48550/arXiv.2501.04697},\n urldate = {2025-02-26},\n number = {arXiv:2501.04697},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/V2IGQLJH/Prieto et al. - 2025 - Grokking at the Edge of Numerical Stability.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Grokking, or sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon that has challenged our understanding of deep learning. While a lot of progress has been made in understanding grokking, it is still not clear why generalization is delayed and why grokking often does not happen without regularization. In this work we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax that we refer to as Softmax Collapse (SC). We show that SC prevents grokking and that mitigating SC leads to grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na{\\textasciidieresis}{\\i}ve loss minimization (NLM) direction. This component of the gradient does not change the predictions of the model but decreases the loss by scaling the logits, usually through the scaling of the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking, and eventually leads to SC, stopping learning altogether. To validate these hypotheses, we introduce two key contributions that mitigate the issues faced in grokking tasks: (i) StableMax, a new activation function that prevents SC and enables grokking without regularization, and (ii) {$\\perp$} Grad, a training algorithm that leads to quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, shedding light on its delayed generalization, reliance on regularization, and the effectiveness of known grokking-inducing methods. Code for this paper can be found at: https://github.com/LucasPrietoAl/ grokking-at-the-edge-of-numerical-stability.},\n primaryclass = {cs},\n eprint = {2501.04697},\n month = {January}\n}"}},{"title":"Horizon Generalization in Reinforcement Learning","uri":"myersHorizonGeneralizationReinforcement2025","taxon":"Reference","tags":[],"route":"/myersHorizonGeneralizationReinforcement2025/","metas":{"doi":"10.48550/arXiv.2501.02709","external":"https://arxiv.org/abs/2501.02709","bibtex":"@misc{myersHorizonGeneralizationReinforcement2025,\n title = {Horizon {{Generalization}} in {{Reinforcement Learning}}},\n author = {Myers, Vivek and Ji, Catherine and Eysenbach, Benjamin},\n year = {2025},\n doi = {10.48550/arXiv.2501.02709},\n urldate = {2025-06-14},\n number = {arXiv:2501.02709},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/TAKJZ8JS/Myers et al. - 2025 - Horizon Generalization in Reinforcement Learning.pdf;/home/kellen/Downloads/pdfs/storage/4FIJY9TU/2501.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Thus, such a policy trained to reach nearby goals should succeed at reaching arbitrarily-distant goals. Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results and recall findings from prior work in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.},\n primaryclass = {cs},\n eprint = {2501.02709},\n month = {January}\n}"}},{"title":"Towards General-Purpose Model-Free Reinforcement Learning","uri":"fujimotoGeneralPurposeModelFreeReinforcement2025","taxon":"Reference","tags":[],"route":"/fujimotoGeneralPurposeModelFreeReinforcement2025/","metas":{"doi":"10.48550/arXiv.2501.16142","external":"https://arxiv.org/abs/2501.16142","bibtex":"@misc{fujimotoGeneralPurposeModelFreeReinforcement2025,\n title = {Towards {{General-Purpose Model-Free Reinforcement Learning}}},\n author = {Fujimoto, Scott and D'Oro, Pierluca and Zhang, Amy and Tian, Yuandong and Rabbat, Michael},\n year = {2025},\n doi = {10.48550/arXiv.2501.16142},\n urldate = {2025-07-01},\n number = {arXiv:2501.16142},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/I3ZRN2A7/Fujimoto et al. - 2025 - Towards General-Purpose Model-Free Reinforcement L.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning (RL) promises a framework for near-universal problemsolving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.},\n primaryclass = {cs},\n eprint = {2501.16142},\n month = {January}\n}"}},{"title":"What type of inference is planning?","uri":"lazaro-gredillaWhatTypeInference2025","taxon":"Reference","tags":[],"route":"/lazaro-gredillaWhatTypeInference2025/","metas":{"doi":"10.48550/arXiv.2406.17863","external":"https://arxiv.org/abs/2406.17863","bibtex":"@misc{lazaro-gredillaWhatTypeInference2025,\n title = {What Type of Inference Is Planning?},\n author = {{L{\\'a}zaro-Gredilla}, Miguel and Ku, Li Yang and Murphy, Kevin P. and George, Dileep},\n year = {2025},\n doi = {10.48550/arXiv.2406.17863},\n urldate = {2025-06-09},\n number = {arXiv:2406.17863},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/G3LX7K3T/Lázaro-Gredilla et al. - 2025 - What type of inference is planning.pdf;/home/kellen/Downloads/pdfs/storage/YIEWIKBY/2406.html},\n keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about \"planning as inference\"? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds exactly to a different set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.},\n primaryclass = {cs},\n eprint = {2406.17863},\n month = {January}\n}"}},{"title":"Chebyshev's Inequality","uri":"002F","taxon":"Theorem","tags":[],"route":"/002F/","metas":{}},{"title":"Holder's Inequality","uri":"002H","taxon":"Theorem","tags":[],"route":"/002H/","metas":{}},{"title":"Jensen's Inequality","uri":"002G","taxon":"Theorem","tags":[],"route":"/002G/","metas":{}},{"title":"Markov's Inequality","uri":"002E","taxon":"Theorem","tags":[],"route":"/002E/","metas":{}},{"title":"https://kellenkanarios.com/0024/","uri":"0024","taxon":"Theorem","tags":[],"route":"/0024/","metas":{}},{"title":"https://kellenkanarios.com/002C/","uri":"002C","taxon":"Theorem","tags":[],"route":"/002C/","metas":{}},{"title":"https://kellenkanarios.com/002D/","uri":"002D","taxon":null,"tags":[],"route":"/002D/","metas":{}},{"title":"Almost Sure Convergence","uri":"001Z","taxon":null,"tags":[],"route":"/001Z/","metas":{}},{"title":"Convergence in Probability","uri":"0020","taxon":null,"tags":[],"route":"/0020/","metas":{}},{"title":"Dense Subset","uri":"001Y","taxon":"Definition","tags":["math"],"route":"/001Y/","metas":{}},{"title":"Distribution","uri":"0023","taxon":"Definition","tags":[],"route":"/0023/","metas":{}},{"title":"Expected Value","uri":"0029","taxon":"Definition","tags":[],"route":"/0029/","metas":{}},{"title":"Finite Sequence Measurability","uri":"0022","taxon":"Theorem","tags":[],"route":"/0022/","metas":{}},{"title":"Independence of Random Variable","uri":"0021","taxon":"Definition","tags":[],"route":"/0021/","metas":{}},{"title":"Simple Random Variable","uri":"001W","taxon":"Definition","tags":[],"route":"/001W/","metas":{}},{"title":"Stack vs. Heap","uri":"0025","taxon":null,"tags":[],"route":"/0025/","metas":{}},{"title":"Uniformly Bounded","uri":"002B","taxon":"Definition","tags":[],"route":"/002B/","metas":{}},{"title":"\\sigma -field of RV","uri":"001X","taxon":"Definition","tags":[],"route":"/001X/","metas":{}},{"title":"kth Moment","uri":"002A","taxon":"Definition","tags":[],"route":"/002A/","metas":{}},{"title":"https://kellenkanarios.com/001S/","uri":"001S","taxon":"Definition","tags":[],"route":"/001S/","metas":{}},{"title":"https://kellenkanarios.com/001P/","uri":"001P","taxon":"Theorem","tags":[],"route":"/001P/","metas":{}},{"title":"Array Method","uri":"001R","taxon":"Theorem","tags":[],"route":"/001R/","metas":{}},{"title":"Borel-Cantelli One","uri":"001Q","taxon":"Theorem","tags":[],"route":"/001Q/","metas":{}},{"title":"Borel-Cantelli Two","uri":"001T","taxon":"Theorem","tags":[],"route":"/001T/","metas":{}},{"title":"Kolmogorov's Zero-one Law","uri":"001V","taxon":"Theorem","tags":[],"route":"/001V/","metas":{}},{"title":"Tail \\sigma -field","uri":"001U","taxon":"Definition","tags":[],"route":"/001U/","metas":{}},{"title":"Independence","uri":"001O","taxon":"Definition","tags":[],"route":"/001O/","metas":{}},{"title":"liminf of sets","uri":"001N","taxon":"Definition","tags":[],"route":"/001N/","metas":{}},{"title":"limsup of sets","uri":"001M","taxon":"Definition","tags":[],"route":"/001M/","metas":{}},{"title":"https://kellenkanarios.com/001K/","uri":"001K","taxon":"Theorem","tags":[],"route":"/001K/","metas":{}},{"title":"Chain Rule of Probability","uri":"001I","taxon":"Proposition","tags":[],"route":"/001I/","metas":{}},{"title":"Conditional Probability","uri":"001J","taxon":"Definition","tags":[],"route":"/001J/","metas":{}},{"title":"Diffusion Meets Flow Matching: Two Sides of the Same Coin","uri":"gaoDiffusionMeetsFlow2024","taxon":"Reference","tags":[],"route":"/gaoDiffusionMeetsFlow2024/","metas":{"bibtex":"@misc{gaoDiffusionMeetsFlow2024,\n title = {Diffusion {{Meets Flow Matching}}: {{Two Sides}} of the {{Same Coin}}},\n author = {Gao, Ruiqi and Hoogeboom, Emiel and Heek, Jonathan and De Bortoli, Valentin and Murphy, Kevin P. and Salimans, Tim},\n year = {2024},\n month = {December}\n}"}},{"title":"Flow Matching Guide and Code","uri":"lipmanFlowMatchingGuide2024a","taxon":"Reference","tags":[],"route":"/lipmanFlowMatchingGuide2024a/","metas":{"doi":"10.48550/arXiv.2412.06264","external":"https://arxiv.org/abs/2412.06264","bibtex":"@misc{lipmanFlowMatchingGuide2024a,\n title = {Flow {{Matching Guide}} and {{Code}}},\n author = {Lipman, Yaron and Havasi, Marton and Holderrieth, Peter and Shaul, Neta and Le, Matt and Karrer, Brian and Chen, Ricky T. Q. and {Lopez-Paz}, David and {Ben-Hamu}, Heli and Gat, Itai},\n year = {2024},\n doi = {10.48550/arXiv.2412.06264},\n urldate = {2025-06-24},\n number = {arXiv:2412.06264},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/4INDYLDI/Lipman et al. - 2024 - Flow Matching Guide and Code.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.},\n primaryclass = {cs},\n eprint = {2412.06264},\n month = {December}\n}"}},{"title":"MaestroMotif: Skill Design from Artificial Intelligence Feedback","uri":"klissarovMaestroMotifSkillDesign2024","taxon":"Reference","tags":[],"route":"/klissarovMaestroMotifSkillDesign2024/","metas":{"doi":"10.48550/arXiv.2412.08542","external":"https://arxiv.org/abs/2412.08542","bibtex":"@misc{klissarovMaestroMotifSkillDesign2024,\n title = {{{MaestroMotif}}: {{Skill Design}} from {{Artificial Intelligence Feedback}}},\n author = {Klissarov, Martin and Henaff, Mikael and Raileanu, Roberta and Sodhani, Shagun and Vincent, Pascal and Zhang, Amy and Bacon, Pierre-Luc and Precup, Doina and Machado, Marlos C. and D'Oro, Pierluca},\n year = {2024},\n doi = {10.48550/arXiv.2412.08542},\n urldate = {2025-01-02},\n number = {arXiv:2412.08542},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Hierarchical RL/Klissarov et al_2024_MaestroMotif.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields highperforming and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.},\n primaryclass = {cs},\n eprint = {2412.08542},\n month = {December},\n shorttitle = {{{MaestroMotif}}}\n}"}},{"title":"Old Optimizer, New Norm: An Anthology","uri":"bernsteinOldOptimizerNew2024","taxon":"Reference","tags":[],"route":"/bernsteinOldOptimizerNew2024/","metas":{"doi":"10.48550/arXiv.2409.20325","external":"https://arxiv.org/abs/2409.20325","bibtex":"@misc{bernsteinOldOptimizerNew2024,\n title = {Old {{Optimizer}}, {{New Norm}}: {{An Anthology}}},\n author = {Bernstein, Jeremy and Newhouse, Laker},\n year = {2024},\n doi = {10.48550/arXiv.2409.20325},\n urldate = {2025-02-26},\n number = {arXiv:2409.20325},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/96NMW68I/Bernstein and Newhouse - 2024 - Old Optimizer, New Norm An Anthology.pdf;/home/kellen/Downloads/pdfs/storage/URA6S2M3/2409.html},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},\n archiveprefix = {arXiv},\n abstract = {Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of \\${\\textbackslash}mathbb\\{R\\}{\\textasciicircum}\\{m{\\textbackslash}times n\\}\\$, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.},\n primaryclass = {cs},\n eprint = {2409.20325},\n month = {December},\n shorttitle = {Old {{Optimizer}}, {{New Norm}}}\n}"}},{"title":"Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback","uri":"zhengOnlineIntrinsicRewards2024a","taxon":"Reference","tags":[],"route":"/zhengOnlineIntrinsicRewards2024a/","metas":{"doi":"10.48550/arXiv.2410.23022","external":"https://arxiv.org/abs/2410.23022","bibtex":"@misc{zhengOnlineIntrinsicRewards2024a,\n title = {Online {{Intrinsic Rewards}} for {{Decision Making Agents}} from {{Large Language Model Feedback}}},\n author = {Zheng, Qinqing and Henaff, Mikael and Zhang, Amy and Grover, Aditya and Amos, Brandon},\n year = {2024},\n doi = {10.48550/arXiv.2410.23022},\n urldate = {2025-06-27},\n number = {arXiv:2410.23022},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/R34D6Q59/Zheng et al. - 2024 - Online Intrinsic Rewards for Decision Making Agent.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets. We make our code available at https://github.com/facebookresearch/oni.},\n primaryclass = {cs},\n eprint = {2410.23022},\n month = {December}\n}"}},{"title":"Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback","uri":"zhengOnlineIntrinsicRewards2024","taxon":"Reference","tags":[],"route":"/zhengOnlineIntrinsicRewards2024/","metas":{"doi":"10.48550/arXiv.2410.23022","external":"https://arxiv.org/abs/2410.23022","bibtex":"@misc{zhengOnlineIntrinsicRewards2024,\n title = {Online {{Intrinsic Rewards}} for {{Decision Making Agents}} from {{Large Language Model Feedback}}},\n author = {Zheng, Qinqing and Henaff, Mikael and Zhang, Amy and Grover, Aditya and Amos, Brandon},\n year = {2024},\n doi = {10.48550/arXiv.2410.23022},\n urldate = {2025-01-02},\n number = {arXiv:2410.23022},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Hierarchical RL/Zheng et al_2024_Online Intrinsic Rewards for Decision Making Agents from Large Language Model.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets. We make our code available at https://github.com/facebookresearch/oni.},\n primaryclass = {cs},\n eprint = {2410.23022},\n month = {December}\n}"}},{"title":"Parseval Regularization for Continual Reinforcement Learning","uri":"chungParsevalRegularizationContinual2024","taxon":"Reference","tags":[],"route":"/chungParsevalRegularizationContinual2024/","metas":{"doi":"10.48550/arXiv.2412.07224","external":"https://arxiv.org/abs/2412.07224","bibtex":"@misc{chungParsevalRegularizationContinual2024,\n title = {Parseval {{Regularization}} for {{Continual Reinforcement Learning}}},\n author = {Chung, Wesley and Cherif, Lynn and Meger, David and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2412.07224},\n urldate = {2024-12-18},\n number = {arXiv:2412.07224},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Chung et al_2024_Parseval Regularization for Continual Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Loss of plasticity, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---all referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.},\n primaryclass = {cs},\n eprint = {2412.07224},\n month = {December}\n}"}},{"title":"https://kellenkanarios.com/001A/","uri":"001A","taxon":"Lemma","tags":[],"route":"/001A/","metas":{}},{"title":"Axiom of Choice","uri":"001H","taxon":"Axiom","tags":[],"route":"/001H/","metas":{}},{"title":"Completeness of a measure","uri":"001F","taxon":"Definition","tags":["prog"],"route":"/001F/","metas":{}},{"title":"Dynkin's \\pi \\text {-}\\lambda ","uri":"001B","taxon":"Theorem","tags":[],"route":"/001B/","metas":{}},{"title":"Halmo's Monotone Class Theorem","uri":"001E","taxon":"Theorem","tags":[],"route":"/001E/","metas":{}},{"title":"Monotone Class","uri":"001D","taxon":"Definition","tags":[],"route":"/001D/","metas":{}},{"title":"Uniqueness of Extension","uri":"001C","taxon":"Theorem","tags":[],"route":"/001C/","metas":{}},{"title":"Vitali Sets","uri":"001G","taxon":"Example","tags":[],"route":"/001G/","metas":{}},{"title":"\\lambda -system","uri":"0019","taxon":"Definition","tags":[],"route":"/0019/","metas":{}},{"title":"\\pi -system","uri":"0018","taxon":"Definition","tags":[],"route":"/0018/","metas":{}},{"title":"https://kellenkanarios.com/0016/","uri":"0016","taxon":null,"tags":[],"route":"/0016/","metas":{}},{"title":"https://kellenkanarios.com/0017/","uri":"0017","taxon":null,"tags":[],"route":"/0017/","metas":{}},{"title":"GPU Training Stuff","uri":"0014","taxon":null,"tags":[],"route":"/0014/","metas":{}},{"title":"Notebooks","uri":"0015","taxon":null,"tags":[],"route":"/0015/","metas":{}},{"title":"Mean Square Value Error","uri":"0013","taxon":"Definition","tags":[],"route":"/0013/","metas":{}},{"title":"Notebook: Reinforcement learning: An introduction","uri":"0012","taxon":null,"tags":["rl","note","top"],"route":"/0012/","metas":{}},{"title":"Action Value Function","uri":"0011","taxon":"Definition","tags":[],"route":"/0011/","metas":{}},{"title":"Markov Decision Process","uri":"000Z","taxon":"Definition","tags":[],"route":"/000Z/","metas":{}},{"title":"Markov Decision Processes","uri":"000Y","taxon":null,"tags":[],"route":"/000Y/","metas":{}},{"title":"Value Function","uri":"0010","taxon":"Definition","tags":[],"route":"/0010/","metas":{}},{"title":"Kullback-Liebler Divergence","uri":"000W","taxon":"Definition","tags":[],"route":"/000W/","metas":{}},{"title":"Mutual Information","uri":"000V","taxon":"Definition","tags":[],"route":"/000V/","metas":{}},{"title":"Outer Measure","uri":"000T","taxon":"Definition","tags":[],"route":"/000T/","metas":{}},{"title":"P^*-measurable","uri":"000U","taxon":"Definition","tags":[],"route":"/000U/","metas":{}},{"title":"https://kellenkanarios.com/000R/","uri":"000R","taxon":"Theorem","tags":[],"route":"/000R/","metas":{}},{"title":"Additivity of Intervals","uri":"000S","taxon":"Theorem","tags":[],"route":"/000S/","metas":{}},{"title":"Probability Measure","uri":"000P","taxon":"Definition","tags":[],"route":"/000P/","metas":{}},{"title":"Probability Space","uri":"000Q","taxon":"Definition","tags":[],"route":"/000Q/","metas":{}},{"title":"\\sigma -field","uri":"000O","taxon":"Definition","tags":[],"route":"/000O/","metas":{}},{"title":"Problem 1.1 Billingsley","uri":"000N","taxon":"Solution","tags":[],"route":"/000N/","metas":{}},{"title":"Weak Law of Large Numbers (Dyadic)","uri":"000M","taxon":"Theorem","tags":[],"route":"/000M/","metas":{}},{"title":"Dyadic Intervals","uri":"000L","taxon":"Example","tags":[],"route":"/000L/","metas":{}},{"title":"On-policy Prediction with Approximation","uri":"000G","taxon":null,"tags":[],"route":"/000G/","metas":{}},{"title":"https://kellenkanarios.com/000D/","uri":"000D","taxon":null,"tags":[],"route":"/000D/","metas":{}},{"title":"Consistent estimator","uri":"000F","taxon":"Definition","tags":[],"route":"/000F/","metas":{}},{"title":"InfoNCE","uri":"000B","taxon":"Definition","tags":["loss"],"route":"/000B/","metas":{}},{"title":"Maximize Mutual info","uri":"000C","taxon":"Theorem","tags":[],"route":"/000C/","metas":{}},{"title":"NCE loss","uri":"000E","taxon":"Definition","tags":["loss"],"route":"/000E/","metas":{}},{"title":"Proto Successor Measure: Representing the Space of All Possible Solutions of Reinforcement Learning","uri":"agarwalProtoSuccessorMeasure2024","taxon":"Reference","tags":[],"route":"/agarwalProtoSuccessorMeasure2024/","metas":{"doi":"10.48550/arXiv.2411.19418","external":"https://arxiv.org/abs/2411.19418","bibtex":"@misc{agarwalProtoSuccessorMeasure2024,\n title = {Proto {{Successor Measure}}: {{Representing}} the {{Space}} of {{All Possible Solutions}} of {{Reinforcement Learning}}},\n author = {Agarwal, Siddhant and Sikchi, Harshit and Stone, Peter and Zhang, Amy},\n year = {2024},\n doi = {10.48550/arXiv.2411.19418},\n urldate = {2024-12-04},\n number = {arXiv:2411.19418},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IZN7I7NS/Agarwal et al. - 2024 - Proto Successor Measure Representing the Space of All Possible Solutions of Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. Referred to as \"zero-shot learning,\" this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present {\\textbackslash}emph\\{Proto Successor Measure\\}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.},\n primaryclass = {cs},\n eprint = {2411.19418},\n month = {November},\n shorttitle = {Proto {{Successor Measure}}}\n}"}},{"title":"TVM: What makes a compiler an ML compiler?","uri":"000A","taxon":null,"tags":["blog","draft"],"route":"/000A/","metas":{}},{"title":"Why Do We Need Weight Decay in Modern Deep Learning?","uri":"dangeloWhyWeNeed2024","taxon":"Reference","tags":[],"route":"/dangeloWhyWeNeed2024/","metas":{"doi":"10.48550/arXiv.2310.04415","external":"https://arxiv.org/abs/2310.04415","bibtex":"@misc{dangeloWhyWeNeed2024,\n title = {Why {{Do We Need Weight Decay}} in {{Modern Deep Learning}}?},\n author = {D'Angelo, Francesco and Andriushchenko, Maksym and Varre, Aditya and Flammarion, Nicolas},\n year = {2024},\n doi = {10.48550/arXiv.2310.04415},\n urldate = {2025-02-26},\n number = {arXiv:2310.04415},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/5GJGM755/D'Angelo et al. - 2024 - Why Do We Need Weight Decay in Modern Deep Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. The code is available at https://github.com/tml-epfl/why-weight-decay},\n primaryclass = {cs},\n eprint = {2310.04415},\n month = {November}\n}"}},{"title":"Contrastive Learning","uri":"0009","taxon":null,"tags":[],"route":"/0009/","metas":{}},{"title":"Notes on Probability and measure","uri":"0007","taxon":null,"tags":["prob","note"],"route":"/0007/","metas":{}},{"title":"/dev/null","uri":"0004","taxon":null,"tags":[],"route":"/0004/","metas":{}},{"title":"Blog","uri":"0002","taxon":null,"tags":["top"],"route":"/0002/","metas":{}},{"title":"Contrastive Reinforcement Learning","uri":"0005","taxon":null,"tags":[],"route":"/0005/","metas":{}},{"title":"What's up with all these fancy optimizers?","uri":"0006","taxon":null,"tags":["blog","draft"],"route":"/0006/","metas":{}},{"title":"Adaptive Exploration for Data-Efficient General Value Function Evaluations","uri":"jainAdaptiveExplorationDataEfficient2024","taxon":"Reference","tags":[],"route":"/jainAdaptiveExplorationDataEfficient2024/","metas":{"doi":"10.48550/arXiv.2405.07838","external":"https://arxiv.org/abs/2405.07838","bibtex":"@misc{jainAdaptiveExplorationDataEfficient2024,\n title = {Adaptive {{Exploration}} for {{Data-Efficient General Value Function Evaluations}}},\n author = {Jain, Arushi and Hanna, Josiah P. and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2405.07838},\n urldate = {2024-12-18},\n number = {arXiv:2405.07838},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Jain et al_2024_Adaptive Exploration for Data-Efficient General Value Function Evaluations.pdf;/home/kellen/Downloads/pdfs/storage/SAPYRAL8/2405.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {General Value Functions (GVFs) (Sutton et al., 2011) represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique reward. Existing methods relying on fixed behavior policies or pre-collected data often face data efficiency issues when learning multiple GVFs in parallel using off-policy methods. To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel. Our method optimizes the behavior policy by minimizing the total variance in return across GVFs, thereby reducing the required environmental interactions. We use an existing temporal-difference-style variance estimator to approximate the return variance. We prove that each behavior policy update decreases the overall mean squared error in GVF predictions. We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.},\n primaryclass = {cs},\n eprint = {2405.07838},\n month = {October}\n}"}},{"title":"Learning Successor Features the Simple Way","uri":"chuaLearningSuccessorFeatures2024","taxon":"Reference","tags":[],"route":"/chuaLearningSuccessorFeatures2024/","metas":{"doi":"10.48550/arXiv.2410.22133","external":"https://arxiv.org/abs/2410.22133","bibtex":"@misc{chuaLearningSuccessorFeatures2024,\n title = {Learning {{Successor Features}} the {{Simple Way}}},\n author = {Chua, Raymond and Ghosh, Arna and Kaplanis, Christos and Richards, Blake A. and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2410.22133},\n urldate = {2024-12-04},\n number = {arXiv:2410.22133},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/D4MGLIU6/Chua et al. - 2024 - Learning Successor Features the Simple Way.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporaldifference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required1.},\n primaryclass = {cs},\n eprint = {2410.22133},\n month = {October}\n}"}},{"title":"MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters","uri":"sharifnassabMetaOptimizeFrameworkOptimizing2024","taxon":"Reference","tags":[],"route":"/sharifnassabMetaOptimizeFrameworkOptimizing2024/","metas":{"doi":"10.48550/arXiv.2402.02342","external":"https://arxiv.org/abs/2402.02342","bibtex":"@misc{sharifnassabMetaOptimizeFrameworkOptimizing2024,\n title = {{{MetaOptimize}}: {{A Framework}} for {{Optimizing Step Sizes}} and {{Other Meta-parameters}}},\n author = {Sharifnassab, Arsalan and Salehkaleybar, Saber and Sutton, Richard},\n year = {2024},\n doi = {10.48550/arXiv.2402.02342},\n urldate = {2025-03-04},\n number = {arXiv:2402.02342},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/MWJ3CB3E/2402.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We address the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.},\n primaryclass = {cs},\n eprint = {2402.02342},\n month = {October},\n shorttitle = {{{MetaOptimize}}}\n}"}},{"title":"Quantifying the Gain in Weak-to-Strong Generalization","uri":"charikarQuantifyingGainWeaktoStrong2024","taxon":"Reference","tags":[],"route":"/charikarQuantifyingGainWeaktoStrong2024/","metas":{"external":"https://arxiv.org/abs/2405.15116","bibtex":"@misc{charikarQuantifyingGainWeaktoStrong2024,\n title = {Quantifying the {{Gain}} in {{Weak-to-Strong Generalization}}},\n author = {Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},\n year = {2024},\n urldate = {2024-10-24},\n number = {arXiv:2405.15116},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/Y3LYJ8Q8/Charikar et al. - 2024 - Quantifying the Gain in Weak-to-Strong Generalization.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. [BIK+23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts---a phenomenon they term weak-to-strong generalization.},\n primaryclass = {cs},\n eprint = {2405.15116},\n month = {October}\n}"}},{"title":"Streaming Deep Reinforcement Learning Finally Works","uri":"elsayedStreamingDeepReinforcement2024","taxon":"Reference","tags":[],"route":"/elsayedStreamingDeepReinforcement2024/","metas":{"doi":"10.48550/arXiv.2410.14606","external":"https://arxiv.org/abs/2410.14606","bibtex":"@misc{elsayedStreamingDeepReinforcement2024,\n title = {Streaming {{Deep Reinforcement Learning Finally Works}}},\n author = {Elsayed, Mohamed and Vasan, Gautham and Mahmood, A. Rupam},\n year = {2024},\n doi = {10.48550/arXiv.2410.14606},\n urldate = {2024-12-03},\n number = {arXiv:2410.14606},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2HFK85PR/Elsayed et al. - 2024 - Streaming Deep Reinforcement Learning Finally Works.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resourceconstrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.},\n primaryclass = {cs},\n eprint = {2410.14606},\n month = {October}\n}"}},{"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback","uri":"leeRLAIFVsRLHF2024","taxon":"Reference","tags":[],"route":"/leeRLAIFVsRLHF2024/","metas":{"external":"https://arxiv.org/abs/2309.00267","bibtex":"@misc{leeRLAIFVsRLHF2024,\n title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},\n author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2309.00267},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IWPVFZCM/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards ``self-improvement'' by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},\n primaryclass = {cs},\n eprint = {2309.00267},\n month = {September},\n shorttitle = {{{RLAIF}} vs. {{RLHF}}}\n}"}},{"title":"Meta Learning for Continual Reinforcement Learning: An Investigation","uri":"kanariosThesis2024","taxon":"Reference","tags":["research"],"route":"/kanariosThesis2024/","metas":{"slides":"https://kellenkanarios.com/bafkrmiatsfeu2hkk6jy2ixdsm5vufq4w7jybkq56oq4mrmphmx4fiy7cde.pdf","paper":"https://kellenkanarios.com/bafkrmiadhsesgrg4ie67bgm7bwzhrku3fucpwivzpaunkdg5jr7xulk4vu.pdf","image":"https://kellenkanarios.com/bafkrmiaalur6esfpgl25lftysiofd2dqqshovigrlisa7vzyhrzwgqikpq.png"}},{"title":"A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals","uri":"liuSingleGoalAll2024","taxon":"Reference","tags":[],"route":"/liuSingleGoalAll2024/","metas":{"doi":"10.48550/arXiv.2408.05804","external":"https://arxiv.org/abs/2408.05804","bibtex":"@misc{liuSingleGoalAll2024,\n title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations}}, or {{Subgoals}}},\n author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2408.05804},\n urldate = {2024-09-06},\n number = {arXiv:2408.05804},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/U8KGMTC6/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.},\n primaryclass = {cs},\n eprint = {2408.05804},\n month = {August},\n shorttitle = {A {{Single Goal}} Is {{All You Need}}}\n}"}},{"title":"Safe Policy Exploration Improvement via Subgoals","uri":"anguloSafePolicyExploration2024","taxon":"Reference","tags":[],"route":"/anguloSafePolicyExploration2024/","metas":{"doi":"10.48550/arXiv.2408.13881","external":"https://arxiv.org/abs/2408.13881","bibtex":"@misc{anguloSafePolicyExploration2024,\n title = {Safe {{Policy Exploration Improvement}} via {{Subgoals}}},\n author = {Angulo, Brian and Gorbov, Gregory and Panov, Aleksandr and Yakovlev, Konstantin},\n year = {2024},\n doi = {10.48550/arXiv.2408.13881},\n urldate = {2025-03-17},\n number = {arXiv:2408.13881},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/PBVLANC9/Angulo et al. - 2024 - Safe Policy Exploration Improvement via Subgoals.pdf},\n keywords = {Computer Science - Machine Learning,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning is a widely used approach to autonomous navigation, showing potential in various tasks and robotic setups. Still, it often struggles to reach distant goals when safety constraints are imposed (e.g., the wheeled robot is prohibited from moving close to the obstacles). One of the main reasons for poor performance in such setups, which is common in practice, is that the need to respect the safety constraints degrades the exploration capabilities of an RL agent. To this end, we introduce a novel learnable algorithm that is based on decomposing the initial problem into smaller sub-problems via intermediate goals, on the one hand, and respects the limit of the cumulative safety constraints, on the other hand -- SPEIS(Safe Policy Exploration Improvement via Subgoals). It comprises the two coupled policies trained end-to-end: subgoal and safe. The subgoal policy is trained to generate the subgoal based on the transitions from the buffer of the safe (main) policy that helps the safe policy to reach distant goals. Simultaneously, the safe policy maximizes its rewards while attempting not to violate the limit of the cumulative safety constraints, thus providing a certain level of safety. We evaluate SPEIS in a wide range of challenging (simulated) environments that involve different types of robots in two different environments: autonomous vehicles from the POLAMP environment and car, point, doggo, and sweep from the safety-gym environment. We demonstrate that our method consistently outperforms state-of-the-art competitors and can significantly reduce the collision rate while maintaining high success rates (higher by 80\\% compared to the best-performing methods).},\n primaryclass = {cs},\n eprint = {2408.13881},\n month = {August}\n}"}},{"title":"The Need for a Big World Simulator: A Scientific Challenge for Continual Learning","uri":"kumarNeedBigWorld2024","taxon":"Reference","tags":[],"route":"/kumarNeedBigWorld2024/","metas":{"doi":"10.48550/arXiv.2408.02930","external":"https://arxiv.org/abs/2408.02930","bibtex":"@misc{kumarNeedBigWorld2024,\n title = {The {{Need}} for a {{Big World Simulator}}: {{A Scientific Challenge}} for {{Continual Learning}}},\n author = {Kumar, Saurabh and Jeon, Hong Jun and Lewandowski, Alex and Roy, Benjamin Van},\n year = {2024},\n doi = {10.48550/arXiv.2408.02930},\n urldate = {2025-06-27},\n number = {arXiv:2408.02930},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2G4W3ULT/Kumar et al. - 2024 - The Need for a Big World Simulator A Scientific C.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The ``small agent, big world'' frame offers a conceptual view that motivates the need for continual learning. The idea is that a small agent operating in a much bigger world cannot store all information that the world has to offer. To perform well, the agent must be carefully designed to ingest, retain, and eject the right information. To enable the development of performant continual learning agents, a number of synthetic environments have been proposed. However, these benchmarks suffer from limitations, including unnatural distribution shifts and a lack of fidelity to the ``small agent, big world'' framing. This paper aims to formalize two desiderata for the design of future simulated environments. These two criteria aim to reflect the objectives and complexity of continual learning in practical settings while enabling rapid prototyping of algorithms on a smaller scale.},\n primaryclass = {cs},\n eprint = {2408.02930},\n month = {August},\n shorttitle = {The {{Need}} for a {{Big World Simulator}}}\n}"}},{"title":"Unsupervised-to-Online Reinforcement Learning","uri":"kimUnsupervisedtoOnlineReinforcementLearning2024","taxon":"Reference","tags":[],"route":"/kimUnsupervisedtoOnlineReinforcementLearning2024/","metas":{"external":"https://arxiv.org/abs/2408.14785v1","bibtex":"@misc{kimUnsupervisedtoOnlineReinforcementLearning2024,\n title = {Unsupervised-to-{{Online Reinforcement Learning}}},\n author = {Kim, Junsu and Park, Seohong and Levine, Sergey},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2408.14785v1},\n journal = {arXiv.org},\n file = {/home/kellen/Downloads/pdfs/storage/FRV2CM6E/Kim et al. - 2024 - Unsupervised-to-Online Reinforcement Learning.pdf},\n langid = {english},\n abstract = {Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL, has been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL), which replaces domain-specific supervised offline RL with unsupervised offline RL, as a better alternative to offline-to-online RL. U2O RL not only enables reusing a single pre-trained model for multiple downstream tasks, but also learns better representations, which often result in even better performance and stability than supervised offline-to-online RL. To instantiate U2O RL in practice, we propose a general recipe for U2O RL to bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning. Throughout our experiments in nine state-based and pixel-based environments, we empirically demonstrate that U2O RL achieves strong performance that matches or even outperforms previous offline-to-online RL approaches, while being able to reuse a single pre-trained model for a number of different downstream tasks.},\n month = {August}\n}"}},{"title":"Cost Aware Best Arm Identification","uri":"kanariosCostAwareBest2024","taxon":"Reference","tags":["research"],"route":"/kanariosCostAwareBest2024/","metas":{"doi":"10.48550/arXiv.2402.16710","slides":"https://kellenkanarios.com/bafkrmigprjuluerfvbmngg4hy6ncbqejf2v5vxkq4praqkav736rsf3yse.pdf","poster":"https://kellenkanarios.com/bafkrmiejtaxo762x4ipx5fgdgzhef5jbv6zwnr2shrxtts57fggd7e3ad4.pdf","image":"https://kellenkanarios.com/bafkrmia2u3dd5angpqkfj5n75kr2r2mcbn2cuvoeihacpr32hyyoi6d5vq.png","bibtex":"@misc{kanariosCostAwareBest2024,\n title = {Cost {{Aware Best Arm Identification}}},\n author = {Kanarios, Kellen and Zhang, Qining and Ying, Lei},\n year = {2024},\n doi = {10.48550/arXiv.2402.16710},\n urldate = {2025-06-27},\n number = {arXiv:2402.16710},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/KCBBU7NQ/Kanarios et al. - 2024 - Cost Aware Best Arm Identification.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this paper, we study a best arm identification problem with dual objects. In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. We call it Cost Aware Best Arm Identification (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. We first derive a theoretical lower bound for CABAI and propose an algorithm called CTAS to match it asymptotically. To reduce the computation of CTAS, we further propose a simple algorithm called Chernoff Overlap (CO), based on a square-root rule, which we prove is optimal in simplified two-armed models and generalizes well in numerical experiments. Our results show that (i) ignoring the heterogeneous action cost results in sub-optimality in practice, and (ii) simple algorithms can deliver near-optimal performance over a wide range of problems.},\n primaryclass = {cs},\n eprint = {2402.16710},\n month = {July}\n}"}},{"title":"Predictive representations: Building blocks of intelligence","uri":"carvalhoPredictiveRepresentationsBuilding2024","taxon":"Reference","tags":[],"route":"/carvalhoPredictiveRepresentationsBuilding2024/","metas":{"external":"https://arxiv.org/abs/2402.06590","bibtex":"@misc{carvalhoPredictiveRepresentationsBuilding2024,\n title = {Predictive Representations: Building Blocks of Intelligence},\n author = {Carvalho, Wilka and Tomov, Momchil S. and {de Cothi}, William and Barry, Caswell and Gershman, Samuel J.},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2402.06590},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/AWBJ53L5/Carvalho et al. - 2024 - Predictive representations building blocks of intelligence.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.},\n primaryclass = {cs},\n eprint = {2402.06590},\n month = {July},\n shorttitle = {Predictive Representations}\n}"}},{"title":"Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning","uri":"matthewsCraftaxLightningFastBenchmark2024","taxon":"Reference","tags":[],"route":"/matthewsCraftaxLightningFastBenchmark2024/","metas":{"doi":"10.48550/arXiv.2402.16801","external":"https://arxiv.org/abs/2402.16801","bibtex":"@misc{matthewsCraftaxLightningFastBenchmark2024,\n title = {Craftax: {{A Lightning-Fast Benchmark}} for {{Open-Ended Reinforcement Learning}}},\n author = {Matthews, Michael and Beukman, Michael and Ellis, Benjamin and Samvelyan, Mikayel and Jackson, Matthew and Coward, Samuel and Foerster, Jakob},\n year = {2024},\n doi = {10.48550/arXiv.2402.16801},\n urldate = {2025-06-26},\n number = {arXiv:2402.16801},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/X9IELMIS/Matthews et al. - 2024 - Craftax A Lightning-Fast Benchmark for Open-Ended.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms. We identify that existing benchmarks used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90\\% of the optimal reward. To provide a more compelling challenge we present the main Craftax benchmark, a significant extension of the Crafter mechanics with elements inspired from NetHack1. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as unsupervised environment design fail to make material progress on the benchmark. We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.},\n primaryclass = {cs},\n eprint = {2402.16801},\n month = {June},\n shorttitle = {Craftax}\n}"}},{"title":"Information Theoretic Guarantees For Policy Alignment In Large Language Models","uri":"mrouehInformationTheoreticGuarantees2024","taxon":"Reference","tags":[],"route":"/mrouehInformationTheoreticGuarantees2024/","metas":{"external":"https://arxiv.org/abs/2406.05883","bibtex":"@misc{mrouehInformationTheoreticGuarantees2024,\n title = {Information {{Theoretic Guarantees For Policy Alignment In Large Language Models}}},\n author = {Mroueh, Youssef},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2406.05883},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/5NKQB3ZU/Mroueh - 2024 - Information Theoretic Guarantees For Policy Alignment In Large Language Models.pdf},\n keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an \\$f\\$-divergence such as the \\${\\textbackslash}mathsf\\{KL\\}\\$ divergence. The best of \\$n\\$ alignment policy selects a sample from the reference policy that has the maximum reward among \\$n\\$ independent samples. For both cases (policy alignment and best of \\$n\\$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$, with an explicit bound in \\$n\\$ on the \\${\\textbackslash}mathsf\\{KL\\}\\$ for the best of \\$n\\$ policy. We show in this paper that the \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of \\$n\\$ policy, that the \\${\\textbackslash}mathsf\\{KL\\}\\$ upper bound can be obtained for any \\$f\\$-divergence via a reduction to exponential order statistics owing to the R{\\textbackslash}'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R{\\textbackslash}'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.},\n primaryclass = {cs, math, stat},\n eprint = {2406.05883},\n month = {June}\n}"}},{"title":"Learning Continually by Spectral Regularization","uri":"lewandowskiLearningContinuallySpectral2024","taxon":"Reference","tags":[],"route":"/lewandowskiLearningContinuallySpectral2024/","metas":{"external":"https://arxiv.org/abs/2406.06811","bibtex":"@misc{lewandowskiLearningContinuallySpectral2024,\n title = {Learning {{Continually}} by {{Spectral Regularization}}},\n author = {Lewandowski, Alex and Kumar, Saurabh and Schuurmans, Dale and Gy{\\\"o}rgy, Andr{\\'a}s and Machado, Marlos C.},\n year = {2024},\n urldate = {2024-10-18},\n number = {arXiv:2406.06811},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/6HX2DA95/Lewandowski et al. - 2024 - Learning Continually by Spectral Regularization.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Loss of plasticity is a phenomenon where neural networks become more difficult to train during the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good predictive performance while maintaining network trainability. We develop new techniques for improving continual learning by first reconsidering how initialization can ensure trainability during early phases of learning. From this perspective, we derive new regularization strategies for continual learning that ensure beneficial initialization properties are better maintained throughout training. In particular, we investigate two new regularization techniques for continual learning: (i) Wasserstein regularization toward the initial weight distribution, which is less restrictive than regularizing toward initial weights; and (ii) regularizing weight matrix singular values, which directly ensures gradient diversity is maintained throughout training. We present an experimental analysis that shows these alternative regularizers can improve continual learning performance across a range of supervised learning tasks and model architectures. The alternative regularizers prove to be less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance.},\n primaryclass = {cs},\n eprint = {2406.06811},\n month = {June}\n}"}},{"title":"Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making","uri":"myersLearningTemporalDistances2024","taxon":"Reference","tags":[],"route":"/myersLearningTemporalDistances2024/","metas":{"doi":"10.48550/arXiv.2406.17098","external":"https://arxiv.org/abs/2406.17098","bibtex":"@misc{myersLearningTemporalDistances2024,\n title = {Learning {{Temporal Distances}}: {{Contrastive Successor Features Can Provide}} a {{Metric Structure}} for {{Decision-Making}}},\n author = {Myers, Vivek and Zheng, Chongyi and Dragan, Anca and Levine, Sergey and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2406.17098},\n urldate = {2024-09-06},\n number = {arXiv:2406.17098},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/X3LR92NR/Myers et al. - 2024 - Learning Temporal Distances Contrastive Successor Features Can Provide a Metric Structure for Decis.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., \"stitching\") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.},\n primaryclass = {cs},\n eprint = {2406.17098},\n month = {June},\n shorttitle = {Learning {{Temporal Distances}}}\n}"}},{"title":"Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing","uri":"qiOnlineDPOOnline2024","taxon":"Reference","tags":[],"route":"/qiOnlineDPOOnline2024/","metas":{"doi":"10.48550/arXiv.2406.05534","external":"https://arxiv.org/abs/2406.05534","bibtex":"@misc{qiOnlineDPOOnline2024,\n title = {Online {{DPO}}: {{Online Direct Preference Optimization}} with {{Fast-Slow Chasing}}},\n author = {Qi, Biqing and Li, Pengfei and Li, Fangyuan and Gao, Junqi and Zhang, Kaiyan and Zhou, Bowen},\n year = {2024},\n doi = {10.48550/arXiv.2406.05534},\n urldate = {2024-09-08},\n number = {arXiv:2406.05534},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/74CS8AIC/Qi et al. - 2024 - Online DPO Online Direct Preference Optimization with Fast-Slow Chasing.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.},\n primaryclass = {cs},\n eprint = {2406.05534},\n month = {June},\n shorttitle = {Online {{DPO}}}\n}"}},{"title":"A First Course in Monte Carlo Methods","uri":"sanz-alonsoFirstCourseMonte2024","taxon":"Reference","tags":[],"route":"/sanz-alonsoFirstCourseMonte2024/","metas":{"external":"https://arxiv.org/abs/2405.16359","bibtex":"@misc{sanz-alonsoFirstCourseMonte2024,\n title = {A {{First Course}} in {{Monte Carlo Methods}}},\n author = {{Sanz-Alonso}, Daniel and {Al-Ghattas}, Omar},\n year = {2024},\n urldate = {2024-10-28},\n number = {arXiv:2405.16359},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/YQNKW8C5/Sanz-Alonso and Al-Ghattas - 2024 - A First Course in Monte Carlo Methods.pdf},\n keywords = {Computer Science - Numerical Analysis,Mathematics - History and Overview,Mathematics - Numerical Analysis,Statistics - Computation},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {This is a concise mathematical introduction to Monte Carlo methods, a rich family of algorithms with far-reaching applications in science and engineering. Monte Carlo methods are an exciting subject for mathematical statisticians and computational and applied mathematicians: the design and analysis of modern algorithms are rooted in a broad mathematical toolbox that includes ergodic theory of Markov chains, Hamiltonian dynamical systems, transport maps, stochastic differential equations, information theory, optimization, Riemannian geometry, and gradient flows, among many others. These lecture notes celebrate the breadth of mathematical ideas that have led to tangible advancements in Monte Carlo methods and their applications. To accommodate a diverse audience, the level of mathematical rigor varies from chapter to chapter, giving only an intuitive treatment to the most technically demanding subjects. The aim is not to be comprehensive or encyclopedic, but rather to illustrate some key principles in the design and analysis of Monte Carlo methods through a carefully-crafted choice of topics that emphasizes timeless over timely ideas. Algorithms are presented in a way that is conducive to conceptual understanding and mathematical analysis -- clarity and intuition are favored over state-of-the-art implementations that are harder to comprehend or rely on ad-hoc heuristics. To help readers navigate the expansive landscape of Monte Carlo methods, each algorithm is accompanied by a summary of its pros and cons, and by a discussion of the type of problems for which they are most useful. The presentation is self-contained, and therefore adequate for self-guided learning or as a teaching resource. Each chapter contains a section with bibliographic remarks that will be useful for those interested in conducting research on Monte Carlo methods and their applications.},\n primaryclass = {stat},\n eprint = {2405.16359},\n month = {May}\n}"}},{"title":"A statistical framework for weak-to-strong generalization","uri":"somerstepStatisticalFrameworkWeaktostrong2024","taxon":"Reference","tags":[],"route":"/somerstepStatisticalFrameworkWeaktostrong2024/","metas":{"external":"https://arxiv.org/abs/2405.16236","bibtex":"@misc{somerstepStatisticalFrameworkWeaktostrong2024,\n title = {A Statistical Framework for Weak-to-Strong Generalization},\n author = {Somerstep, Seamus and Polo, Felipe Maia and Banerjee, Moulinath and Ritov, Ya'acov and Yurochkin, Mikhail and Sun, Yuekai},\n year = {2024},\n urldate = {2024-09-25},\n number = {arXiv:2405.16236},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/UIG2LRFF/Somerstep et al. - 2024 - A statistical framework for weak-to-strong general.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in three LLM alignment tasks.},\n primaryclass = {cs, stat},\n eprint = {2405.16236},\n month = {May}\n}"}},{"title":"Foundation Policies with Hilbert Representations","uri":"parkFoundationPoliciesHilbert2024","taxon":"Reference","tags":[],"route":"/parkFoundationPoliciesHilbert2024/","metas":{"doi":"10.48550/arXiv.2402.15567","external":"https://arxiv.org/abs/2402.15567","bibtex":"@misc{parkFoundationPoliciesHilbert2024,\n title = {Foundation {{Policies}} with {{Hilbert Representations}}},\n author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.15567},\n urldate = {2024-09-18},\n number = {arXiv:2402.15567},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/WEHZ63LJ/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Downloads/pdfs/storage/YNKYPBF3/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Downloads/pdfs/storage/5H2B6U6Y/2402.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy \"prompting\" schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.},\n primaryclass = {cs},\n eprint = {2402.15567},\n month = {May}\n}"}},{"title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark","uri":"zhangRevisitingZerothOrderOptimization2024","taxon":"Reference","tags":[],"route":"/zhangRevisitingZerothOrderOptimization2024/","metas":{"doi":"10.48550/arXiv.2402.11592","external":"https://arxiv.org/abs/2402.11592","bibtex":"@misc{zhangRevisitingZerothOrderOptimization2024,\n title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}: {{A Benchmark}}},\n author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},\n year = {2024},\n doi = {10.48550/arXiv.2402.11592},\n urldate = {2024-09-08},\n number = {arXiv:2402.11592},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A Benchmark.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow \\{in size\\}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .},\n primaryclass = {cs},\n eprint = {2402.11592},\n month = {May},\n shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}}\n}"}},{"title":"Theoretical Analysis of Weak-to-Strong Generalization","uri":"langTheoreticalAnalysisWeaktoStrong2024","taxon":"Reference","tags":[],"route":"/langTheoreticalAnalysisWeaktoStrong2024/","metas":{"external":"https://arxiv.org/abs/2405.16043","bibtex":"@misc{langTheoreticalAnalysisWeaktoStrong2024,\n title = {Theoretical {{Analysis}} of {{Weak-to-Strong Generalization}}},\n author = {Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2405.16043},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/LJQ96YTP/Lang et al. - 2024 - Theoretical Analysis of Weak-to-Strong Generalization.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.},\n primaryclass = {cs, stat},\n eprint = {2405.16043},\n month = {May}\n}"}},{"title":"Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning","uri":"elsayedAddressingLossPlasticity2024","taxon":"Reference","tags":[],"route":"/elsayedAddressingLossPlasticity2024/","metas":{"doi":"10.48550/arXiv.2404.00781","external":"https://arxiv.org/abs/2404.00781","bibtex":"@misc{elsayedAddressingLossPlasticity2024,\n title = {Addressing {{Loss}} of {{Plasticity}} and {{Catastrophic Forgetting}} in {{Continual Learning}}},\n author = {Elsayed, Mohamed and Mahmood, A. Rupam},\n year = {2024},\n doi = {10.48550/arXiv.2404.00781},\n urldate = {2025-06-19},\n number = {arXiv:2404.00781},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/EZ2X8XML/Elsayed and Mahmood - 2024 - Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning.pdf;/home/kellen/Downloads/pdfs/storage/6IRTU8RR/2404.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.},\n primaryclass = {cs},\n eprint = {2404.00781},\n month = {April}\n}"}},{"title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models","uri":"shaoDeepSeekMathPushingLimits2024","taxon":"Reference","tags":[],"route":"/shaoDeepSeekMathPushingLimits2024/","metas":{"doi":"10.48550/arXiv.2402.03300","external":"https://arxiv.org/abs/2402.03300","bibtex":"@misc{shaoDeepSeekMathPushingLimits2024,\n title = {{{DeepSeekMath}}: {{Pushing}} the {{Limits}} of {{Mathematical Reasoning}} in {{Open Language Models}}},\n author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},\n year = {2024},\n doi = {10.48550/arXiv.2402.03300},\n urldate = {2025-06-27},\n number = {arXiv:2402.03300},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/A4KAUF66/Shao et al. - 2024 - DeepSeekMath Pushing the Limits of Mathematical R.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pretraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},\n primaryclass = {cs},\n eprint = {2402.03300},\n month = {April},\n shorttitle = {{{DeepSeekMath}}}\n}"}},{"title":"Proper Laplacian Representation Learning","uri":"gomezProperLaplacianRepresentation2024","taxon":"Reference","tags":[],"route":"/gomezProperLaplacianRepresentation2024/","metas":{"external":"https://arxiv.org/abs/2310.10833","bibtex":"@misc{gomezProperLaplacianRepresentation2024,\n title = {Proper {{Laplacian Representation Learning}}},\n author = {Gomez, Diego and Bowling, Michael and Machado, Marlos C.},\n year = {2024},\n urldate = {2024-10-18},\n number = {arXiv:2310.10833},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/AGETEYXM/Gomez et al. - 2024 - Proper Laplacian Representation Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.},\n primaryclass = {cs},\n eprint = {2310.10833},\n month = {April}\n}"}},{"title":"Constrained Reinforcement Learning with Smoothed Log Barrier Function","uri":"zhangConstrainedReinforcementLearning2024","taxon":"Reference","tags":[],"route":"/zhangConstrainedReinforcementLearning2024/","metas":{"doi":"10.48550/arXiv.2403.14508","external":"https://arxiv.org/abs/2403.14508","bibtex":"@misc{zhangConstrainedReinforcementLearning2024,\n title = {Constrained {{Reinforcement Learning}} with {{Smoothed Log Barrier Function}}},\n author = {Zhang, Baohe and Zhang, Yuan and Frison, Lilli and Brox, Thomas and B{\\\"o}decker, Joschka},\n year = {2024},\n doi = {10.48550/arXiv.2403.14508},\n urldate = {2025-02-03},\n number = {arXiv:2403.14508},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GLQ2NBB3/Zhang et al. - 2024 - Constrained Reinforcement Learning with Smoothed Log Barrier Function.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.},\n primaryclass = {cs},\n eprint = {2403.14508},\n month = {March}\n}"}},{"title":"DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training","uri":"chenDeepZeroScalingZerothOrder2024","taxon":"Reference","tags":[],"route":"/chenDeepZeroScalingZerothOrder2024/","metas":{"doi":"10.48550/arXiv.2310.02025","external":"https://arxiv.org/abs/2310.02025","bibtex":"@misc{chenDeepZeroScalingZerothOrder2024,\n title = {{{DeepZero}}: {{Scaling}} up {{Zeroth-Order Optimization}} for {{Deep Model Training}}},\n author = {Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},\n year = {2024},\n doi = {10.48550/arXiv.2310.02025},\n urldate = {2024-09-08},\n number = {arXiv:2310.02025},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/7ALRQ8GJ/Chen et al. - 2024 - DeepZero Scaling up Zeroth-Order Optimization for Deep Model Training.pdf},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinatewise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsityinduced ZO training protocol that extends the model pruning methodology using only finite differences to explore and exploit the sparse DL prior in CGE. Third, we develop the methods of feature reuse and forward parallelization to advance the practical implementations of ZO training. Our extensive experiments show that DeepZero achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10, approaching FO training performance for the first time. Furthermore, we show the practical utility of DeepZero in applications of certified adversarial defense and DL-based partial differential equation error correction, achieving 10-20\\% improvement over SOTA. We believe our results will inspire future research on scalable ZO optimization and contribute to advancing DL with black box. Codes are available at https://github.com/OPTML-Group/DeepZero.},\n primaryclass = {cs},\n eprint = {2310.02025},\n month = {March},\n shorttitle = {{{DeepZero}}}\n}"}},{"title":"Mm2-gb: GPU Accelerated Minimap2 for Long Read DNA Mapping","uri":"dongMm2gbGPUAccelerated2024","taxon":"Reference","tags":[],"route":"/dongMm2gbGPUAccelerated2024/","metas":{"doi":"10.1101/2024.03.23.586366","bibtex":"@misc{dongMm2gbGPUAccelerated2024,\n title = {Mm2-Gb: {{GPU Accelerated Minimap2}} for {{Long Read DNA Mapping}}},\n author = {Dong, Juechu and Liu, Xueshen and Sadasivan, Harisankar and Sitaraman, Sriranjani and Narayanasamy, Satish},\n year = {2024},\n doi = {10.1101/2024.03.23.586366},\n urldate = {2024-12-11},\n file = {/home/kellen/Downloads/pdfs/storage/3PUF5B6Y/Dong et al. - 2024 - mm2-gb GPU Accelerated Minimap2 for Long Read DNA.pdf},\n langid = {english},\n copyright = {http://creativecommons.org/licenses/by-nc/4.0/},\n abstract = {Long-read DNA sequencing is becoming increasingly popular for genetic diagnostics. Minimap2 is the state-of-the-art long-read aligner. However, Minimap2's chaining step is slow on the CPU and takes 40-68\\% of the time especially for long DNA reads. Prior works in accelerating Minimap2 either lose mapping accuracy, are closed source (and not updated) or deliver inconsistent speedups for longer reads. We introduce mm2-gb which accelerates the chaining step of Minimap2 on GPU without compromising mapping accuracy. In addition to intra- and inter-read parallelism exploited by prior works, mm2-gb exploits finer levels of parallelism by breaking down high latency large workloads into smaller independent segments that can be run in parallel and leverages several strategies for better workload balancing including split-kernels and prioritized scheduling of segments based on sorted size. We show that mm2-gb on an AMD Instinct™ MI210 GPU achieves 2.57-5.33x performance improvement on long nanopore reads (10kb-100kb), and 1.87x performance gain on super long reads (100kb-300kb) compared to SIMD accelerated mm2-fast. mm2-gb is open-sourced and available at https://github.com/Minimap2onGPU/minimap2.},\n month = {March},\n shorttitle = {Mm2-Gb}\n}"}},{"title":"Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy","uri":"caoOfflineGoalConditionedReinforcement2024","taxon":"Reference","tags":[],"route":"/caoOfflineGoalConditionedReinforcement2024/","metas":{"doi":"10.48550/arXiv.2403.01734","external":"https://arxiv.org/abs/2403.01734","bibtex":"@misc{caoOfflineGoalConditionedReinforcement2024,\n title = {Offline {{Goal-Conditioned Reinforcement Learning}} for {{Safety-Critical Tasks}} with {{Recovery Policy}}},\n author = {Cao, Chenyang and Yan, Zichen and Lu, Renhao and Tan, Junbo and Wang, Xueqian},\n year = {2024},\n doi = {10.48550/arXiv.2403.01734},\n urldate = {2025-01-03},\n number = {arXiv:2403.01734},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Safe-RL/Cao et al_2024_Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with.pdf;/home/kellen/Downloads/pdfs/storage/MIF9JL6P/2403.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.},\n primaryclass = {cs},\n eprint = {2403.01734},\n month = {March}\n}"}},{"title":"Contrastive Difference Predictive Coding","uri":"zhengContrastiveDifferencePredictive2024","taxon":"Reference","tags":[],"route":"/zhengContrastiveDifferencePredictive2024/","metas":{"doi":"10.48550/arXiv.2310.20141","external":"https://arxiv.org/abs/2310.20141","bibtex":"@misc{zhengContrastiveDifferencePredictive2024,\n title = {Contrastive {{Difference Predictive Coding}}},\n author = {Zheng, Chongyi and Salakhutdinov, Ruslan and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2310.20141},\n urldate = {2024-09-06},\n number = {arXiv:2310.20141},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/LXYBRXB5/Zheng et al. - 2024 - Contrastive Difference Predictive Coding.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves \\$2 {\\textbackslash}times\\$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about \\$20 {\\textbackslash}times\\$ more sample efficient than the successor representation and \\$1500 {\\textbackslash}times\\$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.},\n primaryclass = {cs},\n eprint = {2310.20141},\n month = {February}\n}"}},{"title":"Direct Language Model Alignment from Online AI Feedback","uri":"guoDirectLanguageModel2024","taxon":"Reference","tags":[],"route":"/guoDirectLanguageModel2024/","metas":{"external":"https://arxiv.org/abs/2402.04792","bibtex":"@misc{guoDirectLanguageModel2024,\n title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},\n author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel, Mathieu},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2402.04792},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GTI664DM/Guo et al. - 2024 - Direct Language Model Alignment from Online AI Feedback.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.},\n primaryclass = {cs},\n eprint = {2402.04792},\n month = {February}\n}"}},{"title":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings","uri":"fransUnsupervisedZeroShotReinforcement2024","taxon":"Reference","tags":[],"route":"/fransUnsupervisedZeroShotReinforcement2024/","metas":{"doi":"10.48550/arXiv.2402.17135","external":"https://arxiv.org/abs/2402.17135","bibtex":"@misc{fransUnsupervisedZeroShotReinforcement2024,\n title = {Unsupervised {{Zero-Shot Reinforcement Learning}} via {{Functional Reward Encodings}}},\n author = {Frans, Kevin and Park, Seohong and Abbeel, Pieter and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.17135},\n urldate = {2024-12-04},\n number = {arXiv:2402.17135},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/FUDVV3U4/Frans et al. - 2024 - Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformerbased variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zeroshot manner, given a small number of rewardannotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: github.com/kvfrans/fre.},\n primaryclass = {cs},\n eprint = {2402.17135},\n month = {February}\n}"}},{"title":"Fine-Tuning Language Models with Just Forward Passes","uri":"malladiFineTuningLanguageModels2024","taxon":"Reference","tags":[],"route":"/malladiFineTuningLanguageModels2024/","metas":{"doi":"10.48550/arXiv.2305.17333","external":"https://arxiv.org/abs/2305.17333","bibtex":"@misc{malladiFineTuningLanguageModels2024,\n title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},\n author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},\n year = {2024},\n doi = {10.48550/arXiv.2305.17333},\n urldate = {2024-09-08},\n number = {arXiv:2305.17333},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2J3FEQX7/Malladi et al. - 2024 - Fine-Tuning Language Models with Just Forward Passes.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},\n primaryclass = {cs},\n eprint = {2305.17333},\n month = {January}\n}"}},{"title":"Model-based Safe Reinforcement Learning using Variable Horizon Rollouts","uri":"guptaModelbasedSafeReinforcement2024","taxon":"Reference","tags":[],"route":"/guptaModelbasedSafeReinforcement2024/","metas":{"doi":"10.1145/3632410.3632446","bibtex":"@inproceedings{guptaModelbasedSafeReinforcement2024,\n title = {Model-Based {{Safe Reinforcement Learning}} Using {{Variable Horizon Rollouts}}},\n author = {Gupta, Shourya and Suryaman, Utkarsh and Narava, Rahul and Jha, Shashi Shekhar},\n year = {2024},\n isbn = {979-8-4007-1634-8},\n doi = {10.1145/3632410.3632446},\n urldate = {2025-03-17},\n booktitle = {Proceedings of the 7th {{Joint International Conference}} on {{Data Science}} \\& {{Management}} of {{Data}} (11th {{ACM IKDD CODS}} and 29th {{COMAD}})},\n pages = {100--108},\n publisher = {ACM},\n address = {Bangalore India},\n file = {/home/kellen/Downloads/pdfs/storage/FBVZY26U/Gupta et al. - 2024 - Model-based Safe Reinforcement Learning using Variable Horizon Rollouts.pdf},\n langid = {english},\n abstract = {Safe reinforcement learning aims to ensure the safety of agents and their interactions with the environment. Traditional reinforcement learning algorithms often neglect safety considerations, resulting in undesirable consequences when deployed in real-world scenarios. To address this issue, safe reinforcement learning algorithms incorporate safety constraints or modify the reward distribution to prioritize the safety of the agent. Recent literature on modelbased safe reinforcement learning uses a fixed look-ahead horizon to avoid unsafe states, limiting the agent's adaptability to changing environments. In this paper, we propose a variable horizon lookahead based on the agent's current state, resulting in improved performance and adaptability in uncertain environments. Our approach leverages the Gaussian process regression model to estimate the value of the horizon dynamically based on the agent's current state. To enhance sample efficiency, we introduce a selective sampling strategy that reduces the number of rollouts by eliminating samples that may lead to unsafe states and also optimizes the use of computational resources while ensuring safety. We evaluate the effectiveness of our approach in multiple mujoco environments. Our results demonstrate a significant reduction in terms of safety violations during validating compared to existing approaches from the literature.},\n month = {January}\n}"}},{"title":"Normalization and effective learning rates in reinforcement learning","uri":"NEURIPS2024_c04d37be","taxon":"Reference","tags":[],"route":"/NEURIPS2024_c04d37be/","metas":{"bibtex":"@inproceedings{NEURIPS2024_c04d37be,\n title = {Normalization and Effective Learning Rates in Reinforcement Learning},\n author = {Lyle, Clare and Zheng, Zeyu and Khetarpal, Khimya and Martens, James and {van Hasselt}, Hado and Pascanu, Razvan and Dabney, Will},\n year = {2024},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},\n volume = {37},\n pages = {106440--106473},\n publisher = {Curran Associates, Inc.}\n}"}},{"title":"Step-size Optimization for Continual Learning","uri":"degrisStepsizeOptimizationContinual2024","taxon":"Reference","tags":[],"route":"/degrisStepsizeOptimizationContinual2024/","metas":{"external":"https://arxiv.org/abs/2401.17401v1","bibtex":"@misc{degrisStepsizeOptimizationContinual2024,\n title = {Step-Size {{Optimization}} for {{Continual Learning}}},\n author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu, Yuxin and Sutton, Richard},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2401.17401v1},\n journal = {arXiv.org},\n file = {/home/kellen/Downloads/pdfs/storage/YSGENB8H/Degris et al. - 2024 - Step-size Optimization for Continual Learning.pdf},\n langid = {english},\n abstract = {In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.},\n month = {January}\n}"}},{"title":"Foundations of Reinforcement Learning and Interactive Decision Making","uri":"fosterFoundationsReinforcementLearning2023","taxon":"Reference","tags":[],"route":"/fosterFoundationsReinforcementLearning2023/","metas":{"doi":"10.48550/arXiv.2312.16730","external":"https://arxiv.org/abs/2312.16730","bibtex":"@misc{fosterFoundationsReinforcementLearning2023,\n title = {Foundations of {{Reinforcement Learning}} and {{Interactive Decision Making}}},\n author = {Foster, Dylan J. and Rakhlin, Alexander},\n year = {2023},\n doi = {10.48550/arXiv.2312.16730},\n urldate = {2025-01-23},\n number = {arXiv:2312.16730},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/EFVPRXIJ/Foster and Rakhlin - 2023 - Foundations of Reinforcement Learning and Interactive Decision Making.pdf;/home/kellen/Downloads/pdfs/storage/GJCWUBT3/2312.html},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},\n archiveprefix = {arXiv},\n abstract = {These lecture notes give a statistical perspective on the foundations of reinforcement learning and interactive decision making. We present a unifying framework for addressing the exploration-exploitation dilemma using frequentist and Bayesian approaches, with connections and parallels between supervised learning/estimation and decision making as an overarching theme. Special attention is paid to function approximation and flexible model classes such as neural networks. Topics covered include multi-armed and contextual bandits, structured bandits, and reinforcement learning with high-dimensional feedback.},\n primaryclass = {cs},\n eprint = {2312.16730},\n month = {December}\n}"}},{"title":"Foundations of Reinforcement Learning and Interactive Decision Making","uri":"fosterFoundationsReinforcementLearning2023a","taxon":"Reference","tags":[],"route":"/fosterFoundationsReinforcementLearning2023a/","metas":{"doi":"10.48550/arXiv.2312.16730","external":"https://arxiv.org/abs/2312.16730","bibtex":"@misc{fosterFoundationsReinforcementLearning2023a,\n title = {Foundations of {{Reinforcement Learning}} and {{Interactive Decision Making}}},\n author = {Foster, Dylan J. and Rakhlin, Alexander},\n year = {2023},\n doi = {10.48550/arXiv.2312.16730},\n urldate = {2025-01-23},\n number = {arXiv:2312.16730},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/YCJRGLTL/Foster and Rakhlin - 2023 - Foundations of Reinforcement Learning and Interactive Decision Making.pdf},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {These lecture notes give a statistical perspective on the foundations of reinforcement learning and interactive decision making. We present a unifying framework for addressing the exploration-exploitation dilemma using frequentist and Bayesian approaches, with connections and parallels between supervised learning/estimation and decision making as an overarching theme. Special attention is paid to function approximation and flexible model classes such as neural networks. Topics covered include multi-armed and contextual bandits, structured bandits, and reinforcement learning with high-dimensional feedback.},\n primaryclass = {cs},\n eprint = {2312.16730},\n month = {December}\n}"}},{"title":"Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning","uri":"moonDiscoveringHierarchicalAchievements2023","taxon":"Reference","tags":[],"route":"/moonDiscoveringHierarchicalAchievements2023/","metas":{"doi":"10.48550/arXiv.2307.03486","external":"https://arxiv.org/abs/2307.03486","bibtex":"@misc{moonDiscoveringHierarchicalAchievements2023,\n title = {Discovering {{Hierarchical Achievements}} in {{Reinforcement Learning}} via {{Contrastive Learning}}},\n author = {Moon, Seungyong and Yeom, Junyoung and Park, Bumsoo and Song, Hyun Oh},\n year = {2023},\n doi = {10.48550/arXiv.2307.03486},\n urldate = {2025-06-26},\n number = {arXiv:2307.03486},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/6Q3NDH73/Moon et al. - 2023 - Discovering Hierarchical Achievements in Reinforce.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Discovering achievements with a hierarchical structure in procedurally generated environments presents a significant challenge. This requires an agent to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods have been built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be advantageous for learning hierarchical dependencies. However, these methods demand an excessive number of environment interactions or large model sizes, limiting their practicality. In this work, we demonstrate that proximal policy optimization (PPO), a simple yet versatile model-free algorithm, outperforms previous methods when optimized with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, albeit with limited confidence. Based on this observation, we introduce a novel contrastive learning method, called achievement distillation, which strengthens the agent's ability to predict the next achievement. Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment in a sample-efficient manner while utilizing fewer model parameters.},\n primaryclass = {cs},\n eprint = {2307.03486},\n month = {November}\n}"}},{"title":"Probabilistic Inference in Reinforcement Learning Done Right","uri":"tarbouriechProbabilisticInferenceReinforcement2023","taxon":"Reference","tags":[],"route":"/tarbouriechProbabilisticInferenceReinforcement2023/","metas":{"doi":"10.48550/arXiv.2311.13294","external":"https://arxiv.org/abs/2311.13294","bibtex":"@misc{tarbouriechProbabilisticInferenceReinforcement2023,\n title = {Probabilistic {{Inference}} in {{Reinforcement Learning Done Right}}},\n author = {Tarbouriech, Jean and Lattimore, Tor and O'Donoghue, Brendan},\n year = {2023},\n doi = {10.48550/arXiv.2311.13294},\n urldate = {2025-06-09},\n number = {arXiv:2311.13294},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/6QTWS9LM/Tarbouriech et al. - 2023 - Probabilistic Inference in Reinforcement Learning Done Right.pdf;/home/kellen/Downloads/pdfs/storage/YTUL9362/2311.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.},\n primaryclass = {cs},\n eprint = {2311.13294},\n month = {November}\n}"}},{"title":"Reward-respecting subtasks for model-based reinforcement learning","uri":"suttonRewardrespectingSubtasksModelbased2023","taxon":"Reference","tags":[],"route":"/suttonRewardrespectingSubtasksModelbased2023/","metas":{"doi":"10.1016/j.artint.2023.104001","bibtex":"@article{suttonRewardrespectingSubtasksModelbased2023,\n title = {Reward-Respecting Subtasks for Model-Based Reinforcement Learning},\n author = {Sutton, Richard S. and Machado, Marlos C. and Holland, G. Zacharias and Szepesvari, David and Timbers, Finbarr and Tanner, Brian and White, Adam},\n year = {2023},\n doi = {10.1016/j.artint.2023.104001},\n urldate = {2025-06-27},\n journal = {Artificial Intelligence},\n volume = {324},\n pages = {104001},\n file = {/home/kellen/Downloads/pdfs/storage/NVDLPRUE/Sutton et al. - 2023 - Reward-respecting subtasks for model-based reinfor.pdf},\n langid = {english},\n issn = {00043702},\n month = {November}\n}"}},{"title":"Stochastic Interpolants: A Unifying Framework for Flows and Diffusions","uri":"albergoStochasticInterpolantsUnifying2023","taxon":"Reference","tags":[],"route":"/albergoStochasticInterpolantsUnifying2023/","metas":{"doi":"10.48550/arXiv.2303.08797","external":"https://arxiv.org/abs/2303.08797","bibtex":"@misc{albergoStochasticInterpolantsUnifying2023,\n title = {Stochastic {{Interpolants}}: {{A Unifying Framework}} for {{Flows}} and {{Diffusions}}},\n author = {Albergo, Michael S. and Boffi, Nicholas M. and {Vanden-Eijnden}, Eric},\n year = {2023},\n doi = {10.48550/arXiv.2303.08797},\n urldate = {2025-07-01},\n number = {arXiv:2303.08797},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2KCKGFYP/Albergo et al. - 2023 - Stochastic Interpolants A Unifying Framework for .pdf},\n keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {A class of generative models that unifies flow-based and diffusion-based methods is introduced. These models extend the framework proposed in [2], enabling the use of a broad class of continuoustime stochastic processes called `stochastic interpolants' to bridge any two arbitrary probability density functions exactly in finite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a flexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion coefficient. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability flow equations or stochastic differential equations with an adjustable level of noise. The drift coefficients entering these models are time-dependent velocity fields characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. We show that minimization of these quadratic objectives leads to control of the likelihood for generative models built upon stochastic dynamics, while likelihood control for deterministic dynamics is more stringent. We also construct estimators for the likelihood and the cross-entropy of interpolant-based generative models, and we discuss connections with other methods such as score-based diffusion models, stochastic localization processes, probabilistic denoising techniques, and rectifying flows. In addition, we demonstrate that stochastic interpolants recover the Schro{\\textasciidieresis}dinger bridge between the two target densities when explicitly optimizing over the interpolant. Finally, algorithmic aspects are discussed and the approach is illustrated on numerical examples.},\n primaryclass = {cs},\n eprint = {2303.08797},\n month = {November},\n shorttitle = {Stochastic {{Interpolants}}}\n}"}},{"title":"C++ Design Patterns for Low-latency Applications Including High-frequency Trading","uri":"bilokonDesignPatternsLowlatency2023","taxon":"Reference","tags":[],"route":"/bilokonDesignPatternsLowlatency2023/","metas":{"doi":"10.48550/arXiv.2309.04259","external":"https://arxiv.org/abs/2309.04259","bibtex":"@misc{bilokonDesignPatternsLowlatency2023,\n title = {C++ {{Design Patterns}} for {{Low-latency Applications Including High-frequency Trading}}},\n author = {Bilokon, Paul and Gunduz, Burak},\n year = {2023},\n doi = {10.48550/arXiv.2309.04259},\n urldate = {2025-03-22},\n number = {arXiv:2309.04259},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/S3IB9ZSU/Bilokon and Gunduz - 2023 - C++ Design Patterns for Low-latency Applications Including High-frequency Trading.pdf},\n keywords = {Computer Science - Performance,Quantitative Finance - Trading and Market Microstructure},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {This work aims to bridge the existing knowledge gap in the optimisation of latency-critical code, specifically focusing on high-frequency trading (HFT) systems. The research culminates in three main contributions: the creation of a Low-Latency Programming Repository, the optimisation of a market-neutral statistical arbitrage pairs trading strategy, and the implementation of the Disruptor pattern in C++. The repository serves as a practical guide and is enriched with rigorous statistical benchmarking, while the trading strategy optimisation led to substantial improvements in speed and profitability. The Disruptor pattern showcased significant performance enhancement over traditional queuing methods. Evaluation metrics include speed, cache utilisation, and statistical significance, among others. Techniques like Cache Warming and Constexpr showed the most significant gains in latency reduction. Future directions involve expanding the repository, testing the optimised trading algorithm in a live trading environment, and integrating the Disruptor pattern with the trading algorithm for comprehensive system benchmarking. The work is oriented towards academics and industry practitioners seeking to improve performance in latencysensitive applications. The repository, trading strategy, and the Disruptor library can be found at https://github.com/0burak/imperial hft.},\n primaryclass = {cs},\n eprint = {2309.04259},\n month = {September}\n}"}},{"title":"Parallel Algebraic Multigrid for Higher Order PDEs","uri":"kanariosParallelAMG2023","taxon":"Reference","tags":["research"],"route":"/kanariosParallelAMG2023/","metas":{"slides":"https://kellenkanarios.com/bafkrmignmsxm4a4sps6664cfy2fipolzrsomzmdxnhhhdtvjzfprlppuee.pdf","poster":"https://kellenkanarios.com/bafkrmibialj6fqnbce2i5qc6jhx2c6qdcecudy5e4u5nrnghmxmvdjvnre.pptx","doi":"https://www.osti.gov/servlets/purl/2205732/","image":"https://kellenkanarios.com/bafkrmibm4snnzyzdaqzlgk52d7fm2kno2dehv7nbd4jjmw3qxzsj3m2tkq.png"}},{"title":"Introduction to Online Convex Optimization","uri":"hazanIntroductionOnlineConvex2023","taxon":"Reference","tags":[],"route":"/hazanIntroductionOnlineConvex2023/","metas":{"doi":"10.48550/arXiv.1909.05207","external":"https://arxiv.org/abs/1909.05207","bibtex":"@misc{hazanIntroductionOnlineConvex2023,\n title = {Introduction to {{Online Convex Optimization}}},\n author = {Hazan, Elad},\n year = {2023},\n doi = {10.48550/arXiv.1909.05207},\n urldate = {2025-02-16},\n number = {arXiv:1909.05207},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/AB366M9X/Hazan - 2023 - Introduction to Online Convex Optimization.pdf},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.},\n primaryclass = {cs},\n eprint = {1909.05207},\n month = {August}\n}"}},{"title":"Mini-Batch Optimization of Contrastive Loss","uri":"choMiniBatchOptimizationContrastive2023","taxon":"Reference","tags":[],"route":"/choMiniBatchOptimizationContrastive2023/","metas":{"external":"https://arxiv.org/abs/2307.05906v1","bibtex":"@misc{choMiniBatchOptimizationContrastive2023,\n title = {Mini-{{Batch Optimization}} of {{Contrastive Loss}}},\n author = {Cho, Jaewoong and Sreenivasan, Kartik and Lee, Keon and Mun, Kyunghoo and Yi, Soheun and Lee, Jeong-Gwan and Lee, Anna and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},\n year = {2023},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2307.05906v1},\n journal = {arXiv.org},\n file = {/home/kellen/Downloads/pdfs/storage/G87QGI8T/Cho et al. - 2023 - Mini-Batch Optimization of Contrastive Loss.pdf},\n langid = {english},\n abstract = {Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all \\${\\textbackslash}binom\\{N\\}\\{B\\}\\$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experimental results validate our theoretical findings and demonstrate that our proposed algorithm outperforms vanilla SGD in practically relevant settings, providing a better understanding of mini-batch optimization in contrastive learning.},\n month = {July}\n}"}},{"title":"Deep Laplacian-based Options for Temporally-Extended Exploration","uri":"klissarovDeepLaplacianbasedOptions2023","taxon":"Reference","tags":[],"route":"/klissarovDeepLaplacianbasedOptions2023/","metas":{"external":"https://arxiv.org/abs/2301.11181","bibtex":"@misc{klissarovDeepLaplacianbasedOptions2023,\n title = {Deep {{Laplacian-based Options}} for {{Temporally-Extended Exploration}}},\n author = {Klissarov, Martin and Machado, Marlos C.},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2301.11181},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/RJ3AYCQA/Klissarov and Machado - 2023 - Deep Laplacian-based Options for Temporally-Extended Exploration.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up options-based exploration. To do so, we introduce a fully online deep RL algorithm for discovering Laplacianbased options and evaluate our approach on a variety of pixel-based tasks. We compare to several state-of-the-art exploration methods and show that our approach is effective, general, and especially promising in non-stationary settings.},\n primaryclass = {cs},\n eprint = {2301.11181},\n month = {June}\n}"}},{"title":"Investigating the Properties of Neural Network Representations in Reinforcement Learning","uri":"wangInvestigatingPropertiesNeural2023","taxon":"Reference","tags":[],"route":"/wangInvestigatingPropertiesNeural2023/","metas":{"external":"https://arxiv.org/abs/2203.15955","bibtex":"@misc{wangInvestigatingPropertiesNeural2023,\n title = {Investigating the {{Properties}} of {{Neural Network Representations}} in {{Reinforcement Learning}}},\n author = {Wang, Han and Miahi, Erfan and White, Martha and Machado, Marlos C. and Abbas, Zaheer and Kumaraswamy, Raksha and Liu, Vincent and White, Adam},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2203.15955},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/3QBBT8HR/Wang et al. - 2023 - Investigating the Properties of Neural Network Representations in Reinforcement Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation---good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25 thousand agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and transfer tasks corresponding to different goal locations. We develop a method to better understand why some representations work better for transfer, through a systematic approach varying task similarity and measuring and correlating representation properties with transfer performance. We demonstrate the generality of the methodology by investigating representations learned by a Rainbow agent that successfully transfer across games modes in Atari 2600.},\n primaryclass = {cs},\n eprint = {2203.15955},\n month = {May}\n}"}},{"title":"Safe Value Functions","uri":"massianiSafeValueFunctions2023a","taxon":"Reference","tags":[],"route":"/massianiSafeValueFunctions2023a/","metas":{"doi":"10.1109/TAC.2022.3200948","bibtex":"@article{massianiSafeValueFunctions2023a,\n title = {Safe {{Value Functions}}},\n author = {Massiani, Pierre-Fran{\\c c}ois and Heim, Steve and Solowjow, Friedrich and Trimpe, Sebastian},\n year = {2023},\n doi = {10.1109/TAC.2022.3200948},\n urldate = {2025-06-14},\n journal = {IEEE Transactions on Automatic Control},\n volume = {68},\n number = {5},\n pages = {2743--2757},\n langid = {english},\n copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n abstract = {Safety constraints and optimality are important but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces a safe value function. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.},\n issn = {0018-9286, 1558-2523, 2334-3303},\n month = {May}\n}"}},{"title":"Safe Value Functions","uri":"massianiSafeValueFunctions2023","taxon":"Reference","tags":[],"route":"/massianiSafeValueFunctions2023/","metas":{"doi":"10.1109/TAC.2022.3200948","external":"https://arxiv.org/abs/2105.12204","bibtex":"@article{massianiSafeValueFunctions2023,\n title = {Safe {{Value Functions}}},\n author = {Massiani, Pierre-Fran{\\c c}ois and Heim, Steve and Solowjow, Friedrich and Trimpe, Sebastian},\n year = {2023},\n doi = {10.1109/TAC.2022.3200948},\n urldate = {2025-06-14},\n journal = {IEEE Transactions on Automatic Control},\n volume = {68},\n number = {5},\n pages = {2743--2757},\n file = {/home/kellen/Downloads/pdfs/storage/PZ39TCLF/Massiani et al. - 2023 - Safe Value Functions.pdf;/home/kellen/Downloads/pdfs/storage/6P779RDU/2105.html},\n keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},\n archiveprefix = {arXiv},\n abstract = {Safety constraints and optimality are important, but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces a safe value function. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.},\n issn = {0018-9286, 1558-2523, 2334-3303},\n primaryclass = {eess},\n eprint = {2105.12204},\n month = {May}\n}"}},{"title":"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel","uri":"zhaoPyTorchFSDPExperiences2023","taxon":"Reference","tags":[],"route":"/zhaoPyTorchFSDPExperiences2023/","metas":{"external":"https://arxiv.org/abs/2304.11277v2","bibtex":"@misc{zhaoPyTorchFSDPExperiences2023,\n title = {{{PyTorch FSDP}}: {{Experiences}} on {{Scaling Fully Sharded Data Parallel}}},\n author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},\n year = {2023},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2304.11277v2},\n journal = {arXiv.org},\n file = {/home/kellen/Downloads/pdfs/storage/C7SNWUS4/Zhao et al. - 2023 - PyTorch FSDP Experiences on Scaling Fully Sharded Data Parallel.pdf},\n langid = {english},\n abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},\n month = {April},\n shorttitle = {{{PyTorch FSDP}}}\n}"}},{"title":"Temporal Abstraction in Reinforcement Learning with the Successor Representation","uri":"machadoTemporalAbstractionReinforcement2023","taxon":"Reference","tags":[],"route":"/machadoTemporalAbstractionReinforcement2023/","metas":{"external":"https://arxiv.org/abs/2110.05740","bibtex":"@misc{machadoTemporalAbstractionReinforcement2023,\n title = {Temporal {{Abstraction}} in {{Reinforcement Learning}} with the {{Successor Representation}}},\n author = {Machado, Marlos C. and Barreto, Andre and Precup, Doina and Bowling, Michael},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2110.05740},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/PEJ6S5ZX/Machado et al. - 2023 - Temporal Abstraction in Reinforcement Learning with the Successor Representation.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent's representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.},\n primaryclass = {cs},\n eprint = {2110.05740},\n month = {April}\n}"}},{"title":"Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning","uri":"elsayedUtilitybasedPerturbedGradient2023","taxon":"Reference","tags":[],"route":"/elsayedUtilitybasedPerturbedGradient2023/","metas":{"doi":"10.48550/arXiv.2302.03281","external":"https://arxiv.org/abs/2302.03281","bibtex":"@misc{elsayedUtilitybasedPerturbedGradient2023,\n title = {Utility-Based {{Perturbed Gradient Descent}}: {{An Optimizer}} for {{Continual Learning}}},\n author = {Elsayed, Mohamed and Mahmood, A. Rupam},\n year = {2023},\n doi = {10.48550/arXiv.2302.03281},\n urldate = {2025-06-27},\n number = {arXiv:2302.03281},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/2RCAQS2F/Elsayed and Mahmood - 2023 - Utility-based Perturbed Gradient Descent An Optim.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.},\n primaryclass = {cs},\n eprint = {2302.03281},\n month = {April},\n shorttitle = {Utility-Based {{Perturbed Gradient Descent}}}\n}"}},{"title":"Does Zero-Shot Reinforcement Learning Exist?","uri":"touatiDoesZeroShotReinforcement2023","taxon":"Reference","tags":[],"route":"/touatiDoesZeroShotReinforcement2023/","metas":{"doi":"10.48550/arXiv.2209.14935","external":"https://arxiv.org/abs/2209.14935","bibtex":"@misc{touatiDoesZeroShotReinforcement2023,\n title = {Does {{Zero-Shot Reinforcement Learning Exist}}?},\n author = {Touati, Ahmed and Rapin, J{\\'e}r{\\'e}my and Ollivier, Yann},\n year = {2023},\n doi = {10.48550/arXiv.2209.14935},\n urldate = {2024-09-18},\n number = {arXiv:2209.14935},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GFV8I99T/Touati et al. - 2023 - Does Zero-Shot Reinforcement Learning Exist.pdf;/home/kellen/Downloads/pdfs/storage/9RA8LX73/2209.html},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards \"controllable\" agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL ave been suggested using successor features (SFs) [BBQ+ 18] or forward-backward (FB) representations [TO21], but testing has been limited. After clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark [LYL+21]. To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers. SFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS), perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching 85\\% of supervised RL performance with a good replay buffer, in a zero-shot manner.},\n primaryclass = {cs},\n eprint = {2209.14935},\n month = {March}\n}"}},{"title":"Contrastive Learning as Goal-Conditioned Reinforcement Learning","uri":"eysenbachContrastiveLearningGoalConditioned2023","taxon":"Reference","tags":[],"route":"/eysenbachContrastiveLearningGoalConditioned2023/","metas":{"doi":"10.48550/arXiv.2206.07568","external":"https://arxiv.org/abs/2206.07568","bibtex":"@misc{eysenbachContrastiveLearningGoalConditioned2023,\n title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning}}},\n author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},\n year = {2023},\n doi = {10.48550/arXiv.2206.07568},\n urldate = {2024-09-06},\n number = {arXiv:2206.07568},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/WVQKIMN4/Eysenbach et al. - 2023 - Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},\n primaryclass = {cs},\n eprint = {2206.07568},\n month = {February}\n}"}},{"title":"Flow Matching for Generative Modeling","uri":"lipmanFlowMatchingGenerative2023","taxon":"Reference","tags":[],"route":"/lipmanFlowMatchingGenerative2023/","metas":{"doi":"10.48550/arXiv.2210.02747","external":"https://arxiv.org/abs/2210.02747","bibtex":"@misc{lipmanFlowMatchingGenerative2023,\n title = {Flow {{Matching}} for {{Generative Modeling}}},\n author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},\n year = {2023},\n doi = {10.48550/arXiv.2210.02747},\n urldate = {2025-06-20},\n number = {arXiv:2210.02747},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/WMMQDW4B/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/home/kellen/Downloads/pdfs/storage/R693NNL4/2210.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},\n primaryclass = {cs},\n eprint = {2210.02747},\n month = {February}\n}"}},{"title":"Hierarchical Reinforcement Learning in Complex 3D Environments","uri":"piresHierarchicalReinforcementLearning2023","taxon":"Reference","tags":[],"route":"/piresHierarchicalReinforcementLearning2023/","metas":{"doi":"10.48550/arXiv.2302.14451","external":"https://arxiv.org/abs/2302.14451","bibtex":"@misc{piresHierarchicalReinforcementLearning2023,\n title = {Hierarchical {{Reinforcement Learning}} in {{Complex 3D Environments}}},\n author = {Pires, Bernardo Avila and Behbahani, Feryal and Soyer, Hubert and Nikiforou, Kyriacos and Keck, Thomas and Singh, Satinder},\n year = {2023},\n doi = {10.48550/arXiv.2302.14451},\n urldate = {2025-02-03},\n number = {arXiv:2302.14451},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/E6YDS983/Pires et al. - 2023 - Hierarchical Reinforcement Learning in Complex 3D Environments.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Hierarchical Reinforcement Learning (HRL) agents have the potential to demonstrate appealing capabilities such as planning and exploration with abstraction, transfer, and skill reuse. Recent successes with HRL across different domains provide evidence that practical, effective HRL agents are possible, even if existing agents do not yet fully realize the potential of HRL. Despite these successes, visually complex partially observable 3D environments remained a challenge for HRL agents. We address this issue with Hierarchical Hybrid Offline-Online (H2O2), a hierarchical deep reinforcement learning agent that discovers and learns to use options from scratch using its own experience. We show that H2O2 is competitive with a strong non-hierarchical Muesli baseline in the DeepMind Hard Eight tasks and we shed new light on the problem of learning hierarchical agents in complex environments. Our empirical study of H2O2 reveals previously unnoticed practical challenges and brings new perspective to the current understanding of hierarchical agents in complex domains.},\n primaryclass = {cs},\n eprint = {2302.14451},\n month = {February}\n}"}},{"title":"Reinforcement Learning in Low-Rank MDPs with Density Features","uri":"huangReinforcementLearningLowRank2023","taxon":"Reference","tags":[],"route":"/huangReinforcementLearningLowRank2023/","metas":{"external":"https://arxiv.org/abs/2302.02252","bibtex":"@misc{huangReinforcementLearningLowRank2023,\n title = {Reinforcement {{Learning}} in {{Low-Rank MDPs}} with {{Density Features}}},\n author = {Huang, Audrey and Chen, Jinglin and Jiang, Nan},\n year = {2023},\n urldate = {2024-09-16},\n number = {arXiv:2302.02252},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/JV6TZ7S9/Huang et al. - 2023 - Reinforcement Learning in Low-Rank MDPs with Density Features.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {MDPs with low-rank transitions---that is, the transition matrix can be factored into the product of two matrices, left and right---is a highly representative structure that enables tractable learning. The left matrix enables expressive function approximation for value-based learning and has been studied extensively. In this work, we instead investigate sample-efficient learning with density features, i.e., the right matrix, which induce powerful models for state-occupancy distributions. This setting not only sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex RL. In the offline setting, we propose an algorithm for off-policy estimation of occupancies that can handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that constructs exploratory data distributions in a level-by-level manner. As a central technical challenge, the additive error of occupancy estimation is incompatible with the multiplicative definition of data coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend to the representation learning setting, when the density features are unknown and must be learned from an exponentially large candidate set.},\n primaryclass = {cs, stat},\n eprint = {2302.02252},\n month = {February}\n}"}},{"title":"Fundamentals of Functional Analysis","uri":"khanferFundamentalsFunctionalAnalysis2023","taxon":"Reference","tags":[],"route":"/khanferFundamentalsFunctionalAnalysis2023/","metas":{"doi":"10.1007/978-981-99-3029-6","bibtex":"@book{khanferFundamentalsFunctionalAnalysis2023,\n title = {Fundamentals of {{Functional Analysis}}},\n author = {Khanfer, Ammar},\n year = {2023},\n isbn = {978-981-9930-28-9 978-981-9930-29-6},\n doi = {10.1007/978-981-99-3029-6},\n urldate = {2024-10-18},\n publisher = {Springer Nature Singapore},\n address = {Singapore},\n file = {/home/kellen/Downloads/pdfs/storage/Z2NQSGWL/Khanfer - 2023 - Fundamentals of Functional Analysis.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs","uri":"liuLearningPoliciesZero2023","taxon":"Reference","tags":[],"route":"/liuLearningPoliciesZero2023/","metas":{"doi":"10.48550/arXiv.2106.02684","external":"https://arxiv.org/abs/2106.02684","bibtex":"@misc{liuLearningPoliciesZero2023,\n title = {Learning {{Policies}} with {{Zero}} or {{Bounded Constraint Violation}} for {{Constrained MDPs}}},\n author = {Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, P. R. and Tian, Chao},\n year = {2023},\n doi = {10.48550/arXiv.2106.02684},\n urldate = {2025-06-15},\n number = {arXiv:2106.02684},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/ZYYI666X/Liu et al. - 2023 - Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs.pdf;/home/kellen/Downloads/pdfs/storage/QS3LNMDW/2106.html},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We address the issue of safety in reinforcement learning. We pose the problem in an episodic framework of a constrained Markov decision process. Existing results have shown that it is possible to achieve a reward regret of \\${\\textbackslash}tilde\\{{\\textbackslash}mathcal\\{O\\}\\}({\\textbackslash}sqrt\\{K\\})\\$ while allowing an \\${\\textbackslash}tilde\\{{\\textbackslash}mathcal\\{O\\}\\}({\\textbackslash}sqrt\\{K\\})\\$ constraint violation in \\$K\\$ episodes. A critical question that arises is whether it is possible to keep the constraint violation even smaller. We show that when a strictly safe policy is known, then one can confine the system to zero constraint violation with arbitrarily high probability while keeping the reward regret of order \\${\\textbackslash}tilde\\{{\\textbackslash}mathcal\\{O\\}\\}({\\textbackslash}sqrt\\{K\\})\\$. The algorithm which does so employs the principle of optimistic pessimism in the face of uncertainty to achieve safe exploration. When no strictly safe policy is known, though one is known to exist, then it is possible to restrict the system to bounded constraint violation with arbitrarily high probability. This is shown to be realized by a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.},\n primaryclass = {cs},\n eprint = {2106.02684},\n month = {January}\n}"}},{"title":"Measure Theory and Integration","uri":"khanferMeasureTheoryIntegration2023","taxon":"Reference","tags":[],"route":"/khanferMeasureTheoryIntegration2023/","metas":{"doi":"10.1007/978-981-99-2882-8","bibtex":"@book{khanferMeasureTheoryIntegration2023,\n title = {Measure {{Theory}} and {{Integration}}},\n author = {Khanfer, Ammar},\n year = {2023},\n isbn = {978-981-9928-81-1 978-981-9928-82-8},\n doi = {10.1007/978-981-99-2882-8},\n urldate = {2024-10-18},\n publisher = {Springer Nature Singapore},\n address = {Singapore},\n file = {/home/kellen/Downloads/pdfs/storage/9CDPYJ3A/Khanfer - 2023 - Measure Theory and Integration.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Operator Fusion in XLA: Analysis and Evaluation","uri":"sniderOperatorFusionXLA2023","taxon":"Reference","tags":[],"route":"/sniderOperatorFusionXLA2023/","metas":{"external":"https://arxiv.org/abs/2301.13062","bibtex":"@misc{sniderOperatorFusionXLA2023,\n title = {Operator {{Fusion}} in {{XLA}}: {{Analysis}} and {{Evaluation}}},\n author = {Snider, Daniel and Liang, Ruofan},\n year = {2023},\n urldate = {2024-10-20},\n number = {arXiv:2301.13062},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/9M5P2JQF/Snider and Liang - 2023 - Operator Fusion in XLA Analysis and Evaluation.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Machine learning (ML) compilers are an active area of research because they offer the potential to automatically speedup tensor programs. Kernel fusion is often cited as an important optimization performed by ML compilers. However, there exists a knowledge gap about how XLA, the most common ML compiler, applies this nuanced optimization, what kind of speedup it can afford, and what low-level effects it has on hardware. Our paper aims to bridge this knowledge gap by studying key compiler passes of XLA's source code. Our evaluation on a reinforcement learning environment Cartpole shows how different fusion decisions in XLA are made in practice. Furthermore, we implement several XLA kernel fusion strategies that can achieve up to 10.56x speedup compared to our baseline implementation.},\n primaryclass = {cs},\n eprint = {2301.13062},\n month = {January},\n shorttitle = {Operator {{Fusion}} in {{XLA}}}\n}"}},{"title":"REVISITING INTRINSIC REWARD FOR EXPLORATION IN PROCEDURALLY GENERATED ENVIRONMENTS","uri":"wangREVISITINGINTRINSICREWARD2023","taxon":"Reference","tags":[],"route":"/wangREVISITINGINTRINSICREWARD2023/","metas":{"bibtex":"@article{wangREVISITINGINTRINSICREWARD2023,\n title = {{{REVISITING INTRINSIC REWARD FOR EXPLORATION IN PROCEDURALLY GENERATED ENVIRONMENTS}}},\n author = {Wang, Kaixin and Zhou, Kuangqi and Kang, Bingyi and Feng, Jiashi and Yan, Shuicheng},\n year = {2023},\n file = {/home/kellen/Downloads/pdfs/storage/MC8I3P2R/Wang et al. - 2023 - REVISITING INTRINSIC REWARD FOR EXPLORATION IN PRO.pdf},\n langid = {english},\n abstract = {Exploration under sparse rewards remains a key challenge in deep reinforcement learning. Recently, studying exploration in procedurally-generated environments has drawn increasing attention. Existing works generally combine lifelong intrinsic rewards and episodic intrinsic rewards to encourage exploration. Though various lifelong and episodic intrinsic rewards have been proposed, the individual contributions of the two kinds of intrinsic rewards to improving exploration are barely investigated. To bridge this gap, we disentangle these two parts and conduct ablative experiments. We consider lifelong and episodic intrinsic rewards used in prior works, and compare the performance of all lifelong-episodic combinations on the commonly used MiniGrid benchmark. Experimental results show that only using episodic intrinsic rewards can match or surpass prior state-of-the-art methods. On the other hand, only using lifelong intrinsic rewards hardly makes progress in exploration. This demonstrates that episodic intrinsic reward is more crucial than lifelong one in boosting exploration. Moreover, we find through experimental analysis that the lifelong intrinsic reward does not accurately reflect the novelty of states, which explains why it does not help much in improving exploration.}\n}"}},{"title":"Auto-Encoding Variational Bayes","uri":"kingmaAutoEncodingVariationalBayes2022","taxon":"Reference","tags":[],"route":"/kingmaAutoEncodingVariationalBayes2022/","metas":{"doi":"10.48550/arXiv.1312.6114","external":"https://arxiv.org/abs/1312.6114","bibtex":"@misc{kingmaAutoEncodingVariationalBayes2022,\n title = {Auto-{{Encoding Variational Bayes}}},\n author = {Kingma, Diederik P. and Welling, Max},\n year = {2022},\n doi = {10.48550/arXiv.1312.6114},\n urldate = {2025-01-07},\n number = {arXiv:1312.6114},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/4CIQB3XP/1312.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},\n primaryclass = {stat},\n eprint = {1312.6114},\n month = {December}\n}"}},{"title":"Contrastive Value Learning: Implicit Models for Simple Offline RL","uri":"mazoureContrastiveValueLearning2022","taxon":"Reference","tags":[],"route":"/mazoureContrastiveValueLearning2022/","metas":{"doi":"10.48550/arXiv.2211.02100","external":"https://arxiv.org/abs/2211.02100","bibtex":"@misc{mazoureContrastiveValueLearning2022,\n title = {Contrastive {{Value Learning}}: {{Implicit Models}} for {{Simple Offline RL}}},\n author = {Mazoure, Bogdan and Eysenbach, Benjamin and Nachum, Ofir and Tompson, Jonathan},\n year = {2022},\n doi = {10.48550/arXiv.2211.02100},\n urldate = {2025-01-09},\n number = {arXiv:2211.02100},\n publisher = {arXiv},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Model-based reinforcement learning (RL) methods are appealing in the offline setting because they allow an agent to reason about the consequences of actions without interacting with the environment. Prior methods learn a 1-step dynamics model, which predicts the next state given the current state and action. These models do not immediately tell the agent which actions to take, but must be integrated into a larger RL framework. Can we model the environment dynamics in a different way, such that the learned model does directly indicate the value of each action? In this paper, we propose Contrastive Value Learning (CVL), which learns an implicit, multi-step model of the environment dynamics. This model can be learned without access to reward functions, but nonetheless can be used to directly estimate the value of each action, without requiring any TD learning. Because this model represents the multi-step transitions implicitly, it avoids having to predict high-dimensional observations and thus scales to high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior offline RL methods on complex continuous control benchmarks.},\n primaryclass = {cs},\n eprint = {2211.02100},\n month = {November},\n shorttitle = {Contrastive {{Value Learning}}}\n}"}},{"title":"EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine","uri":"wengEnvPoolHighlyParallel2022","taxon":"Reference","tags":[],"route":"/wengEnvPoolHighlyParallel2022/","metas":{"doi":"10.48550/arXiv.2206.10558","external":"https://arxiv.org/abs/2206.10558","bibtex":"@misc{wengEnvPoolHighlyParallel2022,\n title = {{{EnvPool}}: {{A Highly Parallel Reinforcement Learning Environment Execution Engine}}},\n author = {Weng, Jiayi and Lin, Min and Huang, Shengyi and Liu, Bo and Makoviichuk, Denys and Makoviychuk, Viktor and Liu, Zichen and Song, Yufan and Luo, Ting and Jiang, Yukun and Xu, Zhongwen and Yan, Shuicheng},\n year = {2022},\n doi = {10.48550/arXiv.2206.10558},\n urldate = {2025-02-17},\n number = {arXiv:2206.10558},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/UEN5NUYA/Weng et al. - 2022 - EnvPool A Highly Parallel Reinforcement Learning Environment Execution Engine.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8{\\texttimes} that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl\\_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop. EnvPool is open-sourced at https://github.com/sail-sg/envpool.},\n primaryclass = {cs},\n eprint = {2206.10558},\n month = {October},\n shorttitle = {{{EnvPool}}}\n}"}},{"title":"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow","uri":"liuFlowStraightFast2022","taxon":"Reference","tags":[],"route":"/liuFlowStraightFast2022/","metas":{"doi":"10.48550/arXiv.2209.03003","external":"https://arxiv.org/abs/2209.03003","bibtex":"@misc{liuFlowStraightFast2022,\n title = {Flow {{Straight}} and {{Fast}}: {{Learning}} to {{Generate}} and {{Transfer Data}} with {{Rectified Flow}}},\n author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},\n year = {2022},\n doi = {10.48550/arXiv.2209.03003},\n urldate = {2025-07-01},\n number = {arXiv:2209.03003},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/49M9XQS5/Liu et al. - 2022 - Flow Straight and Fast Learning to Generate and T.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {$\\pi$}0 and {$\\pi$}1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {$\\pi$}0 and {$\\pi$}1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of {$\\pi$}0 and {$\\pi$}1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},\n primaryclass = {cs},\n eprint = {2209.03003},\n month = {September},\n shorttitle = {Flow {{Straight}} and {{Fast}}}\n}"}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference","uri":"xieExplanationIncontextLearning2022","taxon":"Reference","tags":[],"route":"/xieExplanationIncontextLearning2022/","metas":{"external":"https://arxiv.org/abs/2111.02080","bibtex":"@misc{xieExplanationIncontextLearning2022,\n title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},\n author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},\n year = {2022},\n urldate = {2024-10-15},\n number = {arXiv:2111.02080},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/RKQR98C3/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit Bayesian Inference.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},\n primaryclass = {cs},\n eprint = {2111.02080},\n month = {July}\n}"}},{"title":"Generalised Policy Improvement with Geometric Policy Composition","uri":"thakoorGeneralisedPolicyImprovement2022","taxon":"Reference","tags":[],"route":"/thakoorGeneralisedPolicyImprovement2022/","metas":{"doi":"10.48550/arXiv.2206.08736","external":"https://arxiv.org/abs/2206.08736","bibtex":"@misc{thakoorGeneralisedPolicyImprovement2022,\n title = {Generalised {{Policy Improvement}} with {{Geometric Policy Composition}}},\n author = {Thakoor, Shantanu and Rowland, Mark and Borsa, Diana and Dabney, Will and Munos, R{\\'e}mi and Barreto, Andr{\\'e}},\n year = {2022},\n doi = {10.48550/arXiv.2206.08736},\n urldate = {2025-02-16},\n number = {arXiv:2206.08736},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/7R2UW6U8/Thakoor et al. - 2022 - Generalised Policy Improvement with Geometric Policy Composition.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We introduce a method for policy improvement that interpolates between the greedy approach of value-based reinforcement learning (RL) and the full planning approach typical of model-based RL. The new method builds on the concept of a geometric horizon model (GHM, also known as a {$\\gamma$}-model), which models the discounted statevisitation distribution of a given policy. We show that we can evaluate any non-Markov policy that switches between a set of base Markov policies with fixed probability by a careful composition of the base policy GHMs, without any additional learning. We can then apply generalised policy improvement (GPI) to collections of such nonMarkov policies to obtain a new Markov policy that will in general outperform its precursors. We provide a thorough theoretical analysis of this approach, develop applications to transfer and standard RL, and empirically demonstrate its effectiveness over standard GPI on a challenging deep RL continuous control task. We also provide an analysis of GHM training methods, proving a novel convergence result regarding previously proposed methods and showing how to train these models stably in deep RL settings.},\n primaryclass = {stat},\n eprint = {2206.08736},\n month = {June}\n}"}},{"title":"A History of Meta-gradient: Gradient Methods for Meta-learning","uri":"suttonHistoryMetagradientGradient2022","taxon":"Reference","tags":[],"route":"/suttonHistoryMetagradientGradient2022/","metas":{"external":"https://arxiv.org/abs/2202.09701v1","bibtex":"@misc{suttonHistoryMetagradientGradient2022,\n title = {A {{History}} of {{Meta-gradient}}: {{Gradient Methods}} for {{Meta-learning}}},\n author = {Sutton, Richard S.},\n year = {2022},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2202.09701v1},\n journal = {arXiv.org},\n file = {/home/kellen/Downloads/pdfs/storage/93SUJHK4/Sutton - 2022 - A History of Meta-gradient Gradient Methods for Meta-learning.pdf},\n langid = {english},\n abstract = {The history of meta-learning methods based on gradient descent is reviewed, focusing primarily on methods that adapt step-size (learning rate) meta-parameters.},\n month = {February},\n shorttitle = {A {{History}} of {{Meta-gradient}}}\n}"}},{"title":"Continual Auxiliary Task Learning","uri":"mcleodContinualAuxiliaryTask2022","taxon":"Reference","tags":[],"route":"/mcleodContinualAuxiliaryTask2022/","metas":{"external":"https://arxiv.org/abs/2202.11133","bibtex":"@misc{mcleodContinualAuxiliaryTask2022,\n title = {Continual {{Auxiliary Task Learning}}},\n author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen, Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},\n year = {2022},\n urldate = {2024-10-16},\n number = {arXiv:2202.11133},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GUALHQEZ/McLeod et al. - 2022 - Continual Auxiliary Task Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.},\n primaryclass = {cs},\n eprint = {2202.11133},\n month = {February}\n}"}},{"title":"Goal-oriented exploration for reinforcement learning","uri":"tarbouriechGoalorientedExplorationReinforcement2022","taxon":"Reference","tags":[],"route":"/tarbouriechGoalorientedExplorationReinforcement2022/","metas":{"bibtex":"@phdthesis{tarbouriechGoalorientedExplorationReinforcement2022,\n title = {Goal-Oriented Exploration for Reinforcement Learning},\n author = {Tarbouriech, Jean},\n year = {2022},\n urldate = {2025-06-09},\n file = {/home/kellen/Downloads/pdfs/storage/2QGU8XLP/Tarbouriech - 2022 - Goal-oriented exploration for reinforcement learning.pdf},\n school = {Universit{\\'e} de Lille}\n}"}},{"title":"High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications","uri":"wrightHighDimensionalDataAnalysis2022","taxon":"Reference","tags":[],"route":"/wrightHighDimensionalDataAnalysis2022/","metas":{"doi":"10.1017/9781108779302","bibtex":"@book{wrightHighDimensionalDataAnalysis2022,\n title = {High-{{Dimensional Data Analysis}} with {{Low-Dimensional Models}}: {{Principles}}, {{Computation}}, and {{Applications}}},\n author = {Wright, John and Ma, Yi},\n year = {2022},\n isbn = {978-1-108-77930-2 978-1-108-48973-7},\n doi = {10.1017/9781108779302},\n urldate = {2024-11-11},\n edition = {1},\n publisher = {Cambridge University Press},\n file = {/home/kellen/Downloads/pdfs/storage/QH288KYJ/Wright and Ma - 2022 - High-Dimensional Data Analysis with Low-Dimensional Models Principles, Computation, and Application.pdf},\n langid = {english},\n copyright = {https://www.cambridge.org/core/terms},\n abstract = {Connecting theory with practice, this systematic and rigorous introduction covers the fundamental principles, algorithms and applications of key mathematical models for high-dimensional data analysis. Comprehensive in its approach, it provides unified coverage of many different low-dimensional models and analytical techniques, including sparse and low-rank models, and both convex and non-convex formulations. Readers will learn how to develop efficient and scalable algorithms for solving real-world problems, supported by numerous examples and exercises throughout, and how to use the computational tools learnt in several application contexts. Applications presented include scientific imaging, communication, face recognition, 3D vision, and deep networks for classification. With code available online, this is an ideal textbook for senior and graduate students in computer science, data science, and electrical engineering, as well as for those taking courses on sparsity, low-dimensional structures, and high-dimensional data. Foreword by Emmanuel Cand{\\`e}s.},\n month = {January},\n shorttitle = {High-{{Dimensional Data Analysis}} with {{Low-Dimensional Models}}}\n}"}},{"title":"LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY","uri":"parkLIPSCHITZCONSTRAINEDUNSUPERVISEDSKILL2022","taxon":"Reference","tags":[],"route":"/parkLIPSCHITZCONSTRAINEDUNSUPERVISEDSKILL2022/","metas":{"bibtex":"@article{parkLIPSCHITZCONSTRAINEDUNSUPERVISEDSKILL2022,\n title = {{{LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY}}},\n author = {Park, Seohong and Choi, Jongwook and Kim, Jaekyeom and Lee, Honglak and Kim, Gunhee},\n year = {2022},\n file = {/home/kellen/Downloads/pdfs/storage/EZ9YLY62/Park et al. - 2022 - LIPSCHITZ-CONSTRAINED UNSUPERVISED SKILL DISCOVERY.pdf},\n langid = {english},\n abstract = {We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner --- i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/.}\n}"}},{"title":"PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL","uri":"antonoglouPLANNINGSTOCHASTICENVIRONMENTS2022","taxon":"Reference","tags":[],"route":"/antonoglouPLANNINGSTOCHASTICENVIRONMENTS2022/","metas":{"bibtex":"@article{antonoglouPLANNINGSTOCHASTICENVIRONMENTS2022,\n title = {{{PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL}}},\n author = {Antonoglou, Ioannis and Schrittwieser, Julian and Ozair, Sherjil and Hubert, Thomas},\n year = {2022},\n file = {/home/kellen/Downloads/pdfs/storage/TGICFELW/Antonoglou et al. - 2022 - PLANNING IN STOCHASTIC ENVIRONMENTS WITH A LEARNED MODEL.pdf},\n langid = {english},\n abstract = {Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined valueequivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the superhuman performance of standard MuZero in the game of Go.}\n}"}},{"title":"POLICY IMPROVEMENT BY PLANNING WITH GUMBEL","uri":"danihelkaPOLICYIMPROVEMENTPLANNING2022","taxon":"Reference","tags":[],"route":"/danihelkaPOLICYIMPROVEMENTPLANNING2022/","metas":{"bibtex":"@article{danihelkaPOLICYIMPROVEMENTPLANNING2022,\n title = {{{POLICY IMPROVEMENT BY PLANNING WITH GUMBEL}}},\n author = {Danihelka, Ivo and Guez, Arthur and Schrittwieser, Julian and Silver, David},\n year = {2022},\n file = {/home/kellen/Downloads/pdfs/storage/D7IB2GI6/Danihelka et al. - 2022 - POLICY IMPROVEMENT BY PLANNING WITH GUMBEL.pdf},\n langid = {english},\n abstract = {AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of policy improvement to replace the more heuristic mechanisms by which AlphaZero selects and uses actions, both at root nodes and at non-root nodes. Our new algorithms, Gumbel AlphaZero and Gumbel MuZero, respectively without and with model-learning, match the state of the art on Go, chess, and Atari, and significantly improve prior performance when planning with few simulations.}\n}"}},{"title":"Safe Policies for Reinforcement Learning via Primal-Dual Methods","uri":"paternainSafePoliciesReinforcement2022","taxon":"Reference","tags":[],"route":"/paternainSafePoliciesReinforcement2022/","metas":{"doi":"10.48550/arXiv.1911.09101","external":"https://arxiv.org/abs/1911.09101","bibtex":"@misc{paternainSafePoliciesReinforcement2022,\n title = {Safe {{Policies}} for {{Reinforcement Learning}} via {{Primal-Dual Methods}}},\n author = {Paternain, Santiago and {Calvo-Fullana}, Miguel and Chamon, Luiz F. O. and Ribeiro, Alejandro},\n year = {2022},\n doi = {10.48550/arXiv.1911.09101},\n urldate = {2025-01-27},\n number = {arXiv:1911.09101},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/F3LLEA4F/Paternain et al. - 2022 - Safe Policies for Reinforcement Learning via Primal-Dual Methods.pdf},\n keywords = {Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this paper, we study the design of controllers in the context of stochastic optimal control under the assumption that the model of the system is not available. This is, we aim to control a Markov Decision Process (MDP) of which we do not know the transition probabilities, but we have access to sample trajectories through experience. We define safety as the agent remaining in a desired safe set with high probability during the operation time. The drawbacks of this formulation are twofold. The problem is non-convex and computing the gradients of the constraints with respect to the policies is prohibitive. Hence, we propose an ergodic relaxation of the constraints with the following advantages. (i) The safety guarantees are maintained in the case of episodic tasks and they hold until a given time horizon for continuing tasks. (ii) The constrained optimization problem despite its non-convexity has arbitrarily small duality gap if the parametrization of the controller is rich enough. (iii) The gradients of the Lagrangian associated to the safe learning problem can be computed using standard Reinforcement Learning (RL) results and stochastic approximation tools. Leveraging these advantages, we exploit primal-dual algorithms to find policies that are safe and optimal. We test the proposed approach in a navigation task in a continuous domain. The numerical results show that our algorithm is capable of dynamically adapting the policy to the environment and the required safety levels.},\n primaryclass = {eess},\n eprint = {1911.09101},\n month = {January}\n}"}},{"title":"Constrained Markov Decision Processes: Stochastic Modeling","uri":"altmanConstrainedMarkovDecision2021","taxon":"Reference","tags":[],"route":"/altmanConstrainedMarkovDecision2021/","metas":{"doi":"10.1201/9781315140223","bibtex":"@book{altmanConstrainedMarkovDecision2021,\n title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},\n author = {Altman, Eitan},\n year = {2021},\n isbn = {978-1-315-14022-3},\n doi = {10.1201/9781315140223},\n urldate = {2024-11-27},\n edition = {1},\n publisher = {Routledge},\n address = {Boca Raton},\n file = {/home/kellen/Downloads/pdfs/storage/GBXKPARY/Altman - 2021 - Constrained Markov Decision Processes Stochastic Modeling.pdf},\n langid = {english},\n month = {December},\n shorttitle = {Constrained {{Markov Decision Processes}}}\n}"}},{"title":"DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization","uri":"kumarDR3ValueBasedDeep2021","taxon":"Reference","tags":[],"route":"/kumarDR3ValueBasedDeep2021/","metas":{"external":"https://arxiv.org/abs/2112.04716","bibtex":"@misc{kumarDR3ValueBasedDeep2021,\n title = {{{DR3}}: {{Value-Based Deep Reinforcement Learning Requires Explicit Regularization}}},\n author = {Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},\n year = {2021},\n urldate = {2024-09-11},\n number = {arXiv:2112.04716},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/S9MJBMRM/Kumar et al. - 2021 - DR3 Value-Based Deep Reinforcement Learning Requires Explicit Regularization.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive ``aliasing'', in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.},\n primaryclass = {cs},\n eprint = {2112.04716},\n month = {December},\n shorttitle = {{{DR3}}}\n}"}},{"title":"Representing Knowledge as Predictions (and State as Knowledge)","uri":"ringRepresentingKnowledgePredictions2021","taxon":"Reference","tags":[],"route":"/ringRepresentingKnowledgePredictions2021/","metas":{"doi":"10.48550/arXiv.2112.06336","external":"https://arxiv.org/abs/2112.06336","bibtex":"@misc{ringRepresentingKnowledgePredictions2021,\n title = {Representing {{Knowledge}} as {{Predictions}} (and {{State}} as {{Knowledge}})},\n author = {Ring, Mark},\n year = {2021},\n doi = {10.48550/arXiv.2112.06336},\n urldate = {2025-06-19},\n number = {arXiv:2112.06336},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IYUWPP87/Ring - 2021 - Representing Knowledge as Predictions (and State as Knowledge).pdf;/home/kellen/Downloads/pdfs/storage/6JWTCLLF/2112.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {This paper shows how a single mechanism allows knowledge to be constructed layer by layer directly from an agent's raw sensorimotor stream. This mechanism, the General Value Function (GVF) or \"forecast,\" captures high-level, abstract knowledge as a set of predictions about existing features and knowledge, based exclusively on the agent's low-level senses and actions. Thus, forecasts provide a representation for organizing raw sensorimotor data into useful abstractions over an unlimited number of layers--a long-sought goal of AI and cognitive science. The heart of this paper is a detailed thought experiment providing a concrete, step-by-step formal illustration of how an artificial agent can build true, useful, abstract knowledge from its raw sensorimotor experience alone. The knowledge is represented as a set of layered predictions (forecasts) about the agent's observed consequences of its actions. This illustration shows twelve separate layers: the lowest consisting of raw pixels, touch and force sensors, and a small number of actions; the higher layers increasing in abstraction, eventually resulting in rich knowledge about the agent's world, corresponding roughly to doorways, walls, rooms, and floor plans. I then argue that this general mechanism may allow the representation of a broad spectrum of everyday human knowledge.},\n primaryclass = {cs},\n eprint = {2112.06336},\n month = {December}\n}"}},{"title":"Generative Temporal Difference Learning for Infinite-Horizon Prediction","uri":"jannerGenerativeTemporalDifference2021","taxon":"Reference","tags":[],"route":"/jannerGenerativeTemporalDifference2021/","metas":{"doi":"10.48550/arXiv.2010.14496","external":"https://arxiv.org/abs/2010.14496","bibtex":"@misc{jannerGenerativeTemporalDifference2021,\n title = {Generative {{Temporal Difference Learning}} for {{Infinite-Horizon Prediction}}},\n author = {Janner, Michael and Mordatch, Igor and Levine, Sergey},\n year = {2021},\n doi = {10.48550/arXiv.2010.14496},\n urldate = {2025-05-13},\n number = {arXiv:2010.14496},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/UIRGT6ZE/Janner et al. - 2021 - Generative Temporal Difference Learning for Infinite-Horizon Prediction.pdf;/home/kellen/Downloads/pdfs/storage/53S6YNYL/2010.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We introduce the \\${\\textbackslash}gamma\\$-model, a predictive model of environment dynamics with an infinite probabilistic horizon. Replacing standard single-step models with \\${\\textbackslash}gamma\\$-models leads to generalizations of the procedures central to model-based control, including the model rollout and model-based value estimation. The \\${\\textbackslash}gamma\\$-model, trained with a generative reinterpretation of temporal difference learning, is a natural continuous analogue of the successor representation and a hybrid between model-free and model-based mechanisms. Like a value function, it contains information about the long-term future; like a standard predictive model, it is independent of task reward. We instantiate the \\${\\textbackslash}gamma\\$-model as both a generative adversarial network and normalizing flow, discuss how its training reflects an inescapable tradeoff between training-time and testing-time compounding errors, and empirically investigate its utility for prediction and control.},\n primaryclass = {cs},\n eprint = {2010.14496},\n month = {November}\n}"}},{"title":"SNAP:Successor Entropy based Incremental Subgoal Discovery for Adaptive Navigation","uri":"dubeySNAPSuccessorEntropy2021","taxon":"Reference","tags":[],"route":"/dubeySNAPSuccessorEntropy2021/","metas":{"doi":"10.1145/3487983.3488292","bibtex":"@inproceedings{dubeySNAPSuccessorEntropy2021,\n title = {{{SNAP}}:{{Successor Entropy}} Based {{Incremental Subgoal Discovery}} for {{Adaptive Navigation}}},\n author = {Dubey, Rohit K. and Sohn, Samuel S. and Abualdenien, Jimmy and Thrash, Tyler and Hoelscher, Christoph and Borrmann, Andr{\\'e} and Kapadia, Mubbasir},\n year = {2021},\n isbn = {978-1-4503-9131-3},\n doi = {10.1145/3487983.3488292},\n urldate = {2025-06-14},\n booktitle = {Proceedings of the 14th {{ACM SIGGRAPH Conference}} on {{Motion}}, {{Interaction}} and {{Games}}},\n series = {{{MIG}} '21},\n pages = {1--11},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n file = {/home/kellen/Downloads/pdfs/storage/N28BM5HI/Dubey et al. - 2021 - SNAPSuccessor Entropy based Incremental Subgoal Discovery for Adaptive Navigation.pdf},\n abstract = {Reinforcement learning (RL) has demonstrated great success in solving navigation tasks but often fails when learning complex environmental structures. One open challenge is to incorporate low-level generalizable skills with human-like adaptive path-planning in an RL framework. Motivated by neural findings in animal navigation, we propose a Successor eNtropy-based Adaptive Path-planning (SNAP) that combines a low-level goal-conditioned policy with the flexibility of a classical high-level planner. SNAP decomposes distant goal-reaching tasks into multiple nearby goal-reaching sub-tasks using a topological graph. To construct this graph, we propose an incremental subgoal discovery method that leverages the highest-entropy states in the learned Successor Representation. The Successor Representation encodes the likelihood of being in a future state given the current state and capture the relational structure of states based on a policy. Our main contributions lie in discovering subgoal states that efficiently abstract the state-space and proposing a low-level goal-conditioned controller for local navigation. Since the basic low-level skill is learned independent of state representation, our model easily generalizes to novel environments without intensive relearning. We provide empirical evidence that the proposed method enables agents to perform long-horizon sparse reward tasks quickly, take detours during barrier tasks, and exploit shortcuts that did not exist during training. Our experiments further show that the proposed method outperforms the existing goal-conditioned RL algorithms in successfully reaching distant-goal tasks and policy learning. To evaluate human-like adaptive path-planning, we also compare our optimal agent with human data and found that, on average, the agent was able to find a shorter path than the human participants.},\n month = {November},\n shorttitle = {{{SNAP}}}\n}"}},{"title":"Learning One Representation to Optimize All Rewards","uri":"touatiLearningOneRepresentation2021","taxon":"Reference","tags":[],"route":"/touatiLearningOneRepresentation2021/","metas":{"doi":"10.48550/arXiv.2103.07945","external":"https://arxiv.org/abs/2103.07945","bibtex":"@misc{touatiLearningOneRepresentation2021,\n title = {Learning {{One Representation}} to {{Optimize All Rewards}}},\n author = {Touati, Ahmed and Ollivier, Yann},\n year = {2021},\n doi = {10.48550/arXiv.2103.07945},\n urldate = {2024-09-18},\n number = {arXiv:2103.07945},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/FN8MPGES/Touati and Ollivier - 2021 - Learning One Representation to Optimize All Rewards.pdf;/home/kellen/Downloads/pdfs/storage/SXVNRKWC/2103.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},\n archiveprefix = {arXiv},\n abstract = {We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.},\n primaryclass = {cs, math},\n eprint = {2103.07945},\n month = {October}\n}"}},{"title":"Provable Representation Learning for Imitation with Contrastive Fourier Features","uri":"nachumProvableRepresentationLearning2021","taxon":"Reference","tags":[],"route":"/nachumProvableRepresentationLearning2021/","metas":{"doi":"10.48550/arXiv.2105.12272","external":"https://arxiv.org/abs/2105.12272","bibtex":"@misc{nachumProvableRepresentationLearning2021,\n title = {Provable {{Representation Learning}} for {{Imitation}} with {{Contrastive Fourier Features}}},\n author = {Nachum, Ofir and Yang, Mengjiao},\n year = {2021},\n doi = {10.48550/arXiv.2105.12272},\n urldate = {2025-05-13},\n number = {arXiv:2105.12272},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/GSS265YF/Nachum and Yang - 2021 - Provable Representation Learning for Imitation with Contrastive Fourier Features.pdf;/home/kellen/Downloads/pdfs/storage/DPCP6C3X/2105.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In imitation learning, it is common to learn a behavior policy to match an unknown target policy via max-likelihood training on a collected set of target demonstrations. In this work, we consider using offline experience datasets - potentially far from the target distribution - to learn low-dimensional state representations that provably accelerate the sample-efficiency of downstream imitation learning. A central challenge in this setting is that the unknown target policy itself may not exhibit low-dimensional behavior, and so there is a potential for the representation learning objective to alias states in which the target policy acts differently. Circumventing this challenge, we derive a representation learning objective that provides an upper bound on the performance difference between the target policy and a lowdimensional policy trained with max-likelihood, and this bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. Moving to the practicality of our method, we show that our objective can be implemented as contrastive learning, in which the transition dynamics are approximated by either an implicit energy-based model or, in some special cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical benefits of our proposed objective.},\n primaryclass = {cs},\n eprint = {2105.12272},\n month = {October}\n}"}},{"title":"The Information Geometry of Unsupervised Reinforcement Learning","uri":"eysenbachInformationGeometryUnsupervised2021","taxon":"Reference","tags":[],"route":"/eysenbachInformationGeometryUnsupervised2021/","metas":{"bibtex":"@inproceedings{eysenbachInformationGeometryUnsupervised2021,\n title = {The {{Information Geometry}} of {{Unsupervised Reinforcement Learning}}},\n author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},\n year = {2021},\n urldate = {2025-02-16},\n booktitle = {International {{Conference}} on {{Learning Representations}}},\n file = {/home/kellen/Downloads/pdfs/storage/VE3YH9VL/Eysenbach et al. - 2021 - The Information Geometry of Unsupervised Reinforcement Learning.pdf},\n langid = {english},\n abstract = {How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.},\n month = {October}\n}"}},{"title":"An Elementary Proof that Q-learning Converges Almost Surely","uri":"regehrElementaryProofThat2021","taxon":"Reference","tags":[],"route":"/regehrElementaryProofThat2021/","metas":{"doi":"10.48550/arXiv.2108.02827","external":"https://arxiv.org/abs/2108.02827","bibtex":"@misc{regehrElementaryProofThat2021,\n title = {An {{Elementary Proof}} That {{Q-learning Converges Almost Surely}}},\n author = {Regehr, Matthew T. and Ayoub, Alex},\n year = {2021},\n doi = {10.48550/arXiv.2108.02827},\n urldate = {2025-01-23},\n number = {arXiv:2108.02827},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/FLMMCTDT/Regehr and Ayoub - 2021 - An Elementary Proof that Q-learning Converges Almost Surely.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Watkins' and Dayan's Q-learning is a model-free reinforcement learning algorithm that iteratively refines an estimate for the optimal action-value function of an MDP by stochastically \"visiting\" many state-ation pairs [Watkins and Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent state-of-the-art achievements in reinforcement learning, including the superhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this paper is to reproduce a precise and (nearly) self-contained proof that Q-learning converges. Much of the available literature leverages powerful theory to obtain highly generalizable results in this vein. However, this approach requires the reader to be familiar with and make many deep connections to different research areas. A student seeking to deepen their understand of Q-learning risks becoming caught in a vicious cycle of \"RL-learning Hell\". For this reason, we give a complete proof from start to finish using only one external result from the field of stochastic approximation, despite the fact that this minimal dependence on other results comes at the expense of some \"shininess\".},\n primaryclass = {cs},\n eprint = {2108.02827},\n month = {August}\n}"}},{"title":"HAC Explore: Accelerating Exploration with Hierarchical Reinforcement Learning","uri":"mcclintonHACExploreAccelerating2021","taxon":"Reference","tags":[],"route":"/mcclintonHACExploreAccelerating2021/","metas":{"doi":"10.48550/arXiv.2108.05872","external":"https://arxiv.org/abs/2108.05872","bibtex":"@misc{mcclintonHACExploreAccelerating2021,\n title = {{{HAC Explore}}: {{Accelerating Exploration}} with {{Hierarchical Reinforcement Learning}}},\n author = {McClinton, Willie and Levy, Andrew and Konidaris, George},\n year = {2021},\n doi = {10.48550/arXiv.2108.05872},\n urldate = {2025-06-27},\n number = {arXiv:2108.05872},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/EQGR2TYU/McClinton et al. - 2021 - HAC Explore Accelerating Exploration with Hierarc.pdf},\n keywords = {Computer Science - Artificial Intelligence},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Sparse rewards and long time horizons remain challenging for reinforcement learning algorithms. Exploration bonuses can help in sparse reward settings by encouraging agents to explore the state space, while hierarchical approaches can assist with long-horizon tasks by decomposing lengthy tasks into shorter subtasks. We propose HAC Explore (HACx), a new method that combines these approaches by integrating the exploration bonus method Random Network Distillation (RND) into the hierarchical approach Hierarchical Actor-Critic (HAC). HACx outperforms either component method on its own, as well as an existing approach to combining hierarchy and exploration, in a set of difficult simulated robotics tasks. HACx is the first RL method to solve a sparse reward, continuous-control task that requires over 1,000 actions.},\n primaryclass = {cs},\n eprint = {2108.05872},\n month = {August},\n shorttitle = {{{HAC Explore}}}\n}"}},{"title":"Accelerating long-read analysis on modern CPUs","uri":"kalikarAcceleratingLongreadAnalysis2021","taxon":"Reference","tags":[],"route":"/kalikarAcceleratingLongreadAnalysis2021/","metas":{"doi":"10.1101/2021.07.21.453294","bibtex":"@misc{kalikarAcceleratingLongreadAnalysis2021,\n title = {Accelerating Long-Read Analysis on Modern {{CPUs}}},\n author = {Kalikar, Saurabh and Jain, Chirag and Md, Vasimuddin and Misra, Sanchit},\n year = {2021},\n doi = {10.1101/2021.07.21.453294},\n urldate = {2024-12-11},\n file = {/home/kellen/Downloads/pdfs/storage/FIX8U6F5/Kalikar et al. - 2021 - Accelerating long-read analysis on modern CPUs.pdf},\n langid = {english},\n abstract = {Long read sequencing is now routinely used at scale for genomics and transcriptomics applications. Mapping of long reads or a draft genome assembly to a reference sequence is often one of the most time consuming steps in these applications. Here, we present techniques to accelerate minimap2, a widely used software for mapping. We present multiple optimizations using SIMD parallelization, e cient cache utilization and a learned index data structure to accelerate its three main computational modules, i.e., seeding, chaining and pairwise sequence alignment. These result in reduction of end-to-end mapping time of minimap2 by up to 1.8{$\\RightArrowBar$} while maintaining identical output.},\n month = {July}\n}"}},{"title":"Goal-Conditioned Reinforcement Learning with Imagined Subgoals","uri":"chane-saneGoalConditionedReinforcementLearning2021","taxon":"Reference","tags":[],"route":"/chane-saneGoalConditionedReinforcementLearning2021/","metas":{"doi":"10.48550/arXiv.2107.00541","external":"https://arxiv.org/abs/2107.00541","bibtex":"@misc{chane-saneGoalConditionedReinforcementLearning2021,\n title = {Goal-{{Conditioned Reinforcement Learning}} with {{Imagined Subgoals}}},\n author = {{Chane-Sane}, Elliot and Schmid, Cordelia and Laptev, Ivan},\n year = {2021},\n doi = {10.48550/arXiv.2107.00541},\n urldate = {2025-06-14},\n number = {arXiv:2107.00541},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/T6TN9EDG/Chane-Sane et al. - 2021 - Goal-Conditioned Reinforcement Learning with Imagined Subgoals.pdf},\n keywords = {Computer Science - Machine Learning,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don't require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.},\n primaryclass = {cs},\n eprint = {2107.00541},\n month = {July}\n}"}},{"title":"Goal-Conditioned Reinforcement Learning with Imagined Subgoals","uri":"chane-saneGoalConditionedReinforcementLearning2021a","taxon":"Reference","tags":[],"route":"/chane-saneGoalConditionedReinforcementLearning2021a/","metas":{"doi":"10.48550/arXiv.2107.00541","external":"https://arxiv.org/abs/2107.00541","bibtex":"@misc{chane-saneGoalConditionedReinforcementLearning2021a,\n title = {Goal-{{Conditioned Reinforcement Learning}} with {{Imagined Subgoals}}},\n author = {{Chane-Sane}, Elliot and Schmid, Cordelia and Laptev, Ivan},\n year = {2021},\n doi = {10.48550/arXiv.2107.00541},\n urldate = {2025-06-14},\n number = {arXiv:2107.00541},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/52GUMEKN/Chane-Sane et al. - 2021 - Goal-Conditioned Reinforcement Learning with Imagined Subgoals.pdf;/home/kellen/Downloads/pdfs/storage/6UY9VF4F/2107.html},\n keywords = {Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don't require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.},\n primaryclass = {cs},\n eprint = {2107.00541},\n month = {July}\n}"}},{"title":"Representation Matters: Offline Pretraining for Sequential Decision Making","uri":"yangRepresentationMattersOffline2021","taxon":"Reference","tags":[],"route":"/yangRepresentationMattersOffline2021/","metas":{"bibtex":"@inproceedings{yangRepresentationMattersOffline2021,\n title = {Representation {{Matters}}: {{Offline Pretraining}} for {{Sequential Decision Making}}},\n author = {Yang, Mengjiao and Nachum, Ofir},\n year = {2021},\n urldate = {2024-09-08},\n booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},\n pages = {11784--11794},\n publisher = {PMLR},\n file = {/home/kellen/Downloads/pdfs/storage/B54IUCU7/Yang and Nachum - 2021 - Representation Matters Offline Pretraining for Sequential Decision Making.pdf;/home/kellen/Downloads/pdfs/storage/BANYUMGT/Yang and Nachum - 2021 - Representation Matters Offline Pretraining for Sequential Decision Making.pdf},\n langid = {english},\n abstract = {The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives \\{--\\} e.g., reward prediction, continuous or discrete representations, pretraining or finetuning \\{--\\} are most important and in which settings.},\n issn = {2640-3498},\n month = {July},\n shorttitle = {Representation {{Matters}}}\n}"}},{"title":"Safe Reinforcement Learning Using Advantage-Based Intervention","uri":"wagenerSafeReinforcementLearning2021","taxon":"Reference","tags":[],"route":"/wagenerSafeReinforcementLearning2021/","metas":{"bibtex":"@inproceedings{wagenerSafeReinforcementLearning2021,\n title = {Safe {{Reinforcement Learning Using Advantage-Based Intervention}}},\n author = {Wagener, Nolan C. and Boots, Byron and Cheng, Ching-An},\n year = {2021},\n urldate = {2025-06-15},\n booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},\n pages = {10630--10640},\n publisher = {PMLR},\n file = {/home/kellen/Downloads/pdfs/storage/ALKUBTMZ/Wagener et al. - 2021 - Safe Reinforcement Learning Using Advantage-Based Intervention.pdf;/home/kellen/Downloads/pdfs/storage/YR8E34C3/Wagener et al. - 2021 - Safe Reinforcement Learning Using Advantage-Based Intervention.pdf},\n langid = {english},\n abstract = {Many sequential decision problems involve finding a policy that maximizes total reward while obeying safety constraints. Although much recent research has focused on the development of safe reinforcement learning (RL) algorithms that produce a safe policy after training, ensuring safety during training as well remains an open problem. A fundamental challenge is performing exploration while still satisfying constraints in an unknown Markov decision process (MDP). In this work, we address this problem for the chance-constrained setting.We propose a new algorithm, SAILR, that uses an intervention mechanism based on advantage functions to keep the agent safe throughout training and optimizes the agent's policy using off-the-shelf RL algorithms designed for unconstrained MDPs. Our method comes with strong guarantees on safety during \"both\" training and deployment (i.e., after training and without the intervention mechanism) and policy performance compared to the optimal safety-constrained policy. In our experiments, we show that SAILR violates constraints far less during training than standard safe RL and constrained MDP approaches and converges to a well-performing policy that can be deployed safely without intervention. Our code is available at https://github.com/nolanwagener/safe\\_rl.},\n issn = {2640-3498},\n month = {July}\n}"}},{"title":"Learning and Planning in Complex Action Spaces","uri":"hubertLearningPlanningComplex2021","taxon":"Reference","tags":[],"route":"/hubertLearningPlanningComplex2021/","metas":{"doi":"10.48550/arXiv.2104.06303","external":"https://arxiv.org/abs/2104.06303","bibtex":"@misc{hubertLearningPlanningComplex2021,\n title = {Learning and {{Planning}} in {{Complex Action Spaces}}},\n author = {Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Barekatain, Mohammadamin and Schmitt, Simon and Silver, David},\n year = {2021},\n doi = {10.48550/arXiv.2104.06303},\n urldate = {2025-03-31},\n number = {arXiv:2104.06303},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/AWL8Y6DZ/Hubert et al. - 2021 - Learning and Planning in Complex Action Spaces.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.},\n primaryclass = {cs},\n eprint = {2104.06303},\n month = {April}\n}"}},{"title":"Podracer architectures for scalable Reinforcement Learning","uri":"hesselPodracerArchitecturesScalable2021","taxon":"Reference","tags":[],"route":"/hesselPodracerArchitecturesScalable2021/","metas":{"doi":"10.48550/arXiv.2104.06272","external":"https://arxiv.org/abs/2104.06272","bibtex":"@misc{hesselPodracerArchitecturesScalable2021,\n title = {Podracer Architectures for Scalable {{Reinforcement Learning}}},\n author = {Hessel, Matteo and Kroiss, Manuel and Clark, Aidan and Kemaev, Iurii and Quan, John and Keck, Thomas and Viola, Fabio and van Hasselt, Hado},\n year = {2021},\n doi = {10.48550/arXiv.2104.06272},\n urldate = {2025-02-11},\n number = {arXiv:2104.06272},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/YM29JDFM/Hessel et al. - 2021 - Podracer architectures for scalable Reinforcement Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Supporting state-of-the-art AI research requires balancing rapid prototyping, ease of use, and quick iteration, with the ability to deploy experiments at a scale traditionally associated with production systems.Deep learning frameworks such as TensorFlow, PyTorch and JAX allow users to transparently make use of accelerators, such as TPUs and GPUs, to offload the more computationally intensive parts of training and inference in modern deep learning systems. Popular training pipelines that use these frameworks for deep learning typically focus on (un-)supervised learning. How to best train reinforcement learning (RL) agents at scale is still an active research area. In this report we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way. Specifically we describe two architectures designed to make the best use of the resources available on a TPU Pod (a special configuration in a Google data center that features multiple TPU devices connected to each other by extremely low latency communication channels).},\n primaryclass = {cs},\n eprint = {2104.06272},\n month = {April}\n}"}},{"title":"The Deep Learning Compiler: A Comprehensive Survey","uri":"liDeepLearningCompiler2021","taxon":"Reference","tags":[],"route":"/liDeepLearningCompiler2021/","metas":{"doi":"10.1109/TPDS.2020.3030548","external":"https://arxiv.org/abs/2002.03794","bibtex":"@article{liDeepLearningCompiler2021,\n title = {The {{Deep Learning Compiler}}: {{A Comprehensive Survey}}},\n author = {Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},\n year = {2021},\n doi = {10.1109/TPDS.2020.3030548},\n urldate = {2024-10-20},\n journal = {IEEE Transactions on Parallel and Distributed Systems},\n volume = {32},\n number = {3},\n pages = {708--727},\n file = {/home/kellen/Downloads/pdfs/storage/TLNQRMTF/Li et al. - 2021 - The Deep Learning Compiler A Comprehensive Survey.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this paper, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey paper focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},\n issn = {1045-9219, 1558-2183, 2161-9883},\n primaryclass = {cs},\n eprint = {2002.03794},\n month = {March},\n shorttitle = {The {{Deep Learning Compiler}}}\n}"}},{"title":"Integrated Task and Motion Planning","uri":"garrettIntegratedTaskMotion2021","taxon":"Reference","tags":[],"route":"/garrettIntegratedTaskMotion2021/","metas":{"doi":"10.1146/annurev-control-091420-084139","bibtex":"@article{garrettIntegratedTaskMotion2021,\n title = {Integrated {{Task}} and {{Motion Planning}}},\n author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and {Lozano-P{\\'e}rez}, Tom{\\'a}s},\n year = {2021},\n doi = {10.1146/annurev-control-091420-084139},\n journal = {Annual Review of Control, Robotics, and Autonomous Systems},\n volume = {4},\n number = {Volume 4, 2021},\n pages = {265--293},\n publisher = {Annual Reviews},\n keywords = {robotics},\n abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete--continuous mathematical programming, and continuous motion planning and thus cannot be effectively addressed by any of these fields directly. In this article, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},\n issn = {2573-5144},\n type = {Journal {{Article}}}\n}"}},{"title":"Learning Successor States and Goal-Dependent Values: A Mathematical Viewpoint","uri":"blierLearningSuccessorStates2021","taxon":"Reference","tags":[],"route":"/blierLearningSuccessorStates2021/","metas":{"doi":"10.48550/arXiv.2101.07123","external":"https://arxiv.org/abs/2101.07123","bibtex":"@misc{blierLearningSuccessorStates2021,\n title = {Learning {{Successor States}} and {{Goal-Dependent Values}}: {{A Mathematical Viewpoint}}},\n author = {Blier, L{\\'e}onard and Tallec, Corentin and Ollivier, Yann},\n year = {2021},\n doi = {10.48550/arXiv.2101.07123},\n urldate = {2025-02-27},\n number = {arXiv:2101.07123},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/LUN44LMX/Blier et al. - 2021 - Learning Successor States and Goal-Dependent Values A Mathematical Viewpoint.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In reinforcement learning, temporal difference-based algorithms can be sample-inefficient: for instance, with sparse rewards, no learning occurs until a reward is observed. This can be remedied by learning richer objects, such as a model of the environment, or successor states. Successor states model the expected future state occupancy from any given state [Dayan, 1993, Kulkarni et al., 2016], and summarize all paths in the environment for a given policy. They are related to goal-dependent value functions, which learn how to reach arbitrary states.},\n primaryclass = {cs},\n eprint = {2101.07123},\n month = {January},\n shorttitle = {Learning {{Successor States}} and {{Goal-Dependent Values}}}\n}"}},{"title":"ZeRO-Offload: Democratizing Billion-Scale Model Training","uri":"renZeROOffloadDemocratizingBillionScale2021","taxon":"Reference","tags":[],"route":"/renZeROOffloadDemocratizingBillionScale2021/","metas":{"doi":"10.48550/arXiv.2101.06840","external":"https://arxiv.org/abs/2101.06840","bibtex":"@misc{renZeROOffloadDemocratizingBillionScale2021,\n title = {{{ZeRO-Offload}}: {{Democratizing Billion-Scale Model Training}}},\n author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},\n year = {2021},\n doi = {10.48550/arXiv.2101.06840},\n urldate = {2025-01-30},\n number = {arXiv:2101.06840},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/EQNAIMY3/Ren et al. - 2021 - ZeRO-Offload Democratizing Billion-Scale Model Training.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency.},\n primaryclass = {cs},\n eprint = {2101.06840},\n month = {January},\n shorttitle = {{{ZeRO-Offload}}}\n}"}},{"title":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","uri":"schrittwieserMasteringAtariGo2020","taxon":"Reference","tags":[],"route":"/schrittwieserMasteringAtariGo2020/","metas":{"doi":"10.1038/s41586-020-03051-4","external":"https://arxiv.org/abs/1911.08265","bibtex":"@article{schrittwieserMasteringAtariGo2020,\n title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},\n author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},\n year = {2020},\n doi = {10.1038/s41586-020-03051-4},\n urldate = {2025-03-31},\n journal = {Nature},\n volume = {588},\n number = {7839},\n pages = {604--609},\n file = {/home/kellen/Downloads/pdfs/storage/I7Y4VSFZ/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},\n issn = {0028-0836, 1476-4687},\n primaryclass = {cs},\n eprint = {1911.08265},\n month = {December}\n}"}},{"title":"Making Sense of Reinforcement Learning and Probabilistic Inference","uri":"odonoghueMakingSenseReinforcement2020","taxon":"Reference","tags":[],"route":"/odonoghueMakingSenseReinforcement2020/","metas":{"doi":"10.48550/arXiv.2001.00805","external":"https://arxiv.org/abs/2001.00805","bibtex":"@misc{odonoghueMakingSenseReinforcement2020,\n title = {Making {{Sense}} of {{Reinforcement Learning}} and {{Probabilistic Inference}}},\n author = {O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},\n year = {2020},\n doi = {10.48550/arXiv.2001.00805},\n urldate = {2025-06-09},\n number = {arXiv:2001.00805},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/KKIHIKDE/O'Donoghue et al. - 2020 - Making Sense of Reinforcement Learning and Probabilistic Inference.pdf;/home/kellen/Downloads/pdfs/storage/YK9BPL3L/2001.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts `RL as inference' and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular `RL as inference' approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.},\n primaryclass = {cs},\n eprint = {2001.00805},\n month = {November}\n}"}},{"title":"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","uri":"levineOfflineReinforcementLearning2020","taxon":"Reference","tags":[],"route":"/levineOfflineReinforcementLearning2020/","metas":{"external":"https://arxiv.org/abs/2005.01643","bibtex":"@misc{levineOfflineReinforcementLearning2020,\n title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},\n author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},\n year = {2020},\n urldate = {2024-11-17},\n number = {arXiv:2005.01643},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/QV4MPWZ3/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},\n primaryclass = {cs},\n eprint = {2005.01643},\n month = {November},\n shorttitle = {Offline {{Reinforcement Learning}}}\n}"}},{"title":"Monte Carlo Tree Search With Reinforcement Learning for Motion Planning","uri":"weingertnerMonteCarloTree2020","taxon":"Reference","tags":[],"route":"/weingertnerMonteCarloTree2020/","metas":{"doi":"10.1109/ITSC45102.2020.9294697","bibtex":"@inproceedings{weingertnerMonteCarloTree2020,\n title = {Monte {{Carlo Tree Search With Reinforcement Learning}} for {{Motion Planning}}},\n author = {Weingertner, Philippe and Ho, Minnie and Timofeev, Andrey and Aubert, S{\\'e}bastien and {Pita-Gil}, Guillermo},\n year = {2020},\n doi = {10.1109/ITSC45102.2020.9294697},\n urldate = {2025-03-19},\n booktitle = {2020 {{IEEE}} 23rd {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},\n pages = {1--7},\n file = {/home/kellen/Downloads/pdfs/storage/YRS3JGCL/Weingertner et al. - 2020 - Monte Carlo Tree Search With Reinforcement Learning for Motion Planning.pdf;/home/kellen/Downloads/pdfs/storage/K56QQ2MD/9294697.html},\n keywords = {Acceleration,Complexity theory,Dynamics,Planning,Real-time systems,Safety,Search problems},\n abstract = {Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, A\\textsuperscript{*} search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.},\n month = {September}\n}"}},{"title":"New insights and perspectives on the natural gradient method","uri":"martensNewInsightsPerspectives2020","taxon":"Reference","tags":[],"route":"/martensNewInsightsPerspectives2020/","metas":{"doi":"10.48550/arXiv.1412.1193","external":"https://arxiv.org/abs/1412.1193","bibtex":"@misc{martensNewInsightsPerspectives2020,\n title = {New Insights and Perspectives on the Natural Gradient Method},\n author = {Martens, James},\n year = {2020},\n doi = {10.48550/arXiv.1412.1193},\n urldate = {2025-05-01},\n number = {arXiv:1412.1193},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/5C4TGVBJ/Martens - 2020 - New insights and perspectives on the natural gradient method.pdf;/home/kellen/Downloads/pdfs/storage/PKVJCN9T/1412.html},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used \"empirical\" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature, but notably not the Hessian).},\n primaryclass = {cs},\n eprint = {1412.1193},\n month = {September}\n}"}},{"title":"Bandit Algorithms","uri":"lattimoreBanditAlgorithms2020","taxon":"Reference","tags":[],"route":"/lattimoreBanditAlgorithms2020/","metas":{"doi":"10.1017/9781108571401","bibtex":"@book{lattimoreBanditAlgorithms2020,\n title = {Bandit {{Algorithms}}},\n author = {Lattimore, Tor and Szepesv{\\'a}ri, Csaba},\n year = {2020},\n isbn = {978-1-108-57140-1 978-1-108-48682-8},\n doi = {10.1017/9781108571401},\n urldate = {2024-10-24},\n edition = {1},\n publisher = {Cambridge University Press},\n file = {/home/kellen/Downloads/pdfs/storage/U3N2WASX/Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf},\n langid = {english},\n copyright = {https://www.cambridge.org/core/terms},\n month = {July}\n}"}},{"title":"Bandit Algorithms","uri":"lattimoreBanditAlgorithms2020a","taxon":"Reference","tags":[],"route":"/lattimoreBanditAlgorithms2020a/","metas":{"doi":"10.1017/9781108571401","bibtex":"@book{lattimoreBanditAlgorithms2020a,\n title = {Bandit {{Algorithms}}},\n author = {Lattimore, Tor and Szepesv{\\'a}ri, Csaba},\n year = {2020},\n isbn = {978-1-108-57140-1 978-1-108-48682-8},\n doi = {10.1017/9781108571401},\n urldate = {2025-06-09},\n edition = {1},\n publisher = {Cambridge University Press},\n file = {/home/kellen/Downloads/pdfs/storage/38L7NUXS/Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf},\n langid = {english},\n copyright = {https://www.cambridge.org/core/terms},\n month = {July}\n}"}},{"title":"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models","uri":"rajbhandariZeROMemoryOptimizations2020","taxon":"Reference","tags":[],"route":"/rajbhandariZeROMemoryOptimizations2020/","metas":{"external":"https://arxiv.org/abs/1910.02054","bibtex":"@misc{rajbhandariZeROMemoryOptimizations2020,\n title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},\n author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},\n year = {2020},\n urldate = {2024-09-18},\n number = {arXiv:1910.02054},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/FDXIZE3Y/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillion Parameter Models.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.},\n primaryclass = {cs, stat},\n eprint = {1910.02054},\n month = {May},\n shorttitle = {{{ZeRO}}}\n}"}},{"title":"Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies","uri":"sohnMetaReinforcementLearning2020","taxon":"Reference","tags":[],"route":"/sohnMetaReinforcementLearning2020/","metas":{"doi":"10.48550/arXiv.2001.00248","external":"https://arxiv.org/abs/2001.00248","bibtex":"@misc{sohnMetaReinforcementLearning2020,\n title = {Meta {{Reinforcement Learning}} with {{Autonomous Inference}} of {{Subtask Dependencies}}},\n author = {Sohn, Sungryull and Woo, Hyunjae and Choi, Jongwook and Lee, Honglak},\n year = {2020},\n doi = {10.48550/arXiv.2001.00248},\n urldate = {2025-06-26},\n number = {arXiv:2001.00248},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IHZTWXFM/Sohn et al. - 2020 - Meta Reinforcement Learning with Autonomous Infere.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference (MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiment results on two grid-world domains and StarCraft II environments show that the proposed method is able to accurately infer the latent task parameter, and to adapt more efficiently than existing meta RL and hierarchical RL methods 1.},\n primaryclass = {cs},\n eprint = {2001.00248},\n month = {April}\n}"}},{"title":"Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for Real-World Predictive Knowledge Architectures","uri":"guntherExaminingUseTemporalDifference2020","taxon":"Reference","tags":[],"route":"/guntherExaminingUseTemporalDifference2020/","metas":{"doi":"10.3389/frobt.2020.00034","bibtex":"@article{guntherExaminingUseTemporalDifference2020,\n title = {Examining the {{Use}} of {{Temporal-Difference Incremental Delta-Bar-Delta}} for {{Real-World Predictive Knowledge Architectures}}},\n author = {G{\\\"u}nther, Johannes and Ady, Nadia M. and Kearney, Alex and Dawson, Michael R. and Pilarski, Patrick M.},\n year = {2020},\n doi = {10.3389/frobt.2020.00034},\n urldate = {2024-10-29},\n journal = {Frontiers in Robotics and AI},\n volume = {7},\n pages = {34},\n file = {/home/kellen/Downloads/pdfs/storage/3N55F4WQ/Günther et al. - 2020 - Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for Real-World Predictive Knowl.pdf},\n langid = {english},\n issn = {2296-9144},\n month = {March}\n}"}},{"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","uri":"shoeybiMegatronLMTrainingMultiBillion2020","taxon":"Reference","tags":[],"route":"/shoeybiMegatronLMTrainingMultiBillion2020/","metas":{"external":"https://arxiv.org/abs/1909.08053","bibtex":"@misc{shoeybiMegatronLMTrainingMultiBillion2020,\n title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},\n author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},\n year = {2020},\n urldate = {2024-09-18},\n number = {arXiv:1909.08053},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/JMJ9WPZ9/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf},\n keywords = {Computer Science - Computation and Language},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\\% compared to SOTA accuracy of 63.2\\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\\% compared to SOTA accuracy of 89.4\\%).},\n primaryclass = {cs},\n eprint = {1909.08053},\n month = {March},\n shorttitle = {Megatron-{{LM}}}\n}"}},{"title":"Language models are few-shot learners","uri":"NEURIPS2020_1457c0d6","taxon":"Reference","tags":[],"route":"/NEURIPS2020_1457c0d6/","metas":{"bibtex":"@inproceedings{NEURIPS2020_1457c0d6,\n title = {Language Models Are Few-Shot Learners},\n author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},\n year = {2020},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},\n volume = {33},\n pages = {1877--1901},\n publisher = {Curran Associates, Inc.}\n}"}},{"title":"Optimization for deep learning: Theory and algorithms","uri":"sunOptimizationDeepLearning2019","taxon":"Reference","tags":[],"route":"/sunOptimizationDeepLearning2019/","metas":{"doi":"10.48550/arXiv.1912.08957","external":"https://arxiv.org/abs/1912.08957","bibtex":"@misc{sunOptimizationDeepLearning2019,\n title = {Optimization for Deep Learning: Theory and Algorithms},\n author = {Sun, Ruoyu},\n year = {2019},\n doi = {10.48550/arXiv.1912.08957},\n urldate = {2025-04-23},\n number = {arXiv:1912.08957},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/AB8NMQPP/Sun - 2019 - Optimization for deep learning theory and algorithms.pdf;/home/kellen/Downloads/pdfs/storage/PTFCY2M3/1912.html},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.},\n primaryclass = {cs},\n eprint = {1912.08957},\n month = {December},\n shorttitle = {Optimization for Deep Learning}\n}"}},{"title":"Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?","uri":"nachumWhyDoesHierarchy2019","taxon":"Reference","tags":[],"route":"/nachumWhyDoesHierarchy2019/","metas":{"doi":"10.48550/arXiv.1909.10618","external":"https://arxiv.org/abs/1909.10618","bibtex":"@misc{nachumWhyDoesHierarchy2019,\n title = {Why {{Does Hierarchy}} ({{Sometimes}}) {{Work So Well}} in {{Reinforcement Learning}}?},\n author = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},\n year = {2019},\n doi = {10.48550/arXiv.1909.10618},\n urldate = {2025-06-26},\n number = {arXiv:1909.10618},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/J2VECPI7/Nachum et al. - 2019 - Why Does Hierarchy (Sometimes) Work So Well in Rei.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard ``shallow'' RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},\n primaryclass = {cs},\n eprint = {1909.10618},\n month = {December}\n}"}},{"title":"Planning with Goal-Conditioned Policies","uri":"nasirianyPlanningGoalConditionedPolicies2019","taxon":"Reference","tags":[],"route":"/nasirianyPlanningGoalConditionedPolicies2019/","metas":{"doi":"10.48550/arXiv.1911.08453","external":"https://arxiv.org/abs/1911.08453","bibtex":"@misc{nasirianyPlanningGoalConditionedPolicies2019,\n title = {Planning with {{Goal-Conditioned Policies}}},\n author = {Nasiriany, Soroush and Pong, Vitchyr H. and Lin, Steven and Levine, Sergey},\n year = {2019},\n doi = {10.48550/arXiv.1911.08453},\n urldate = {2025-06-14},\n number = {arXiv:1911.08453},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/VFMFXBAY/Nasiriany et al. - 2019 - Planning with Goal-Conditioned Policies.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free reinforcement learning (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goalconditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly outperforms prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior.},\n primaryclass = {cs},\n eprint = {1911.08453},\n month = {November}\n}"}},{"title":"Stabilizing Transformers for Reinforcement Learning","uri":"parisottoStabilizingTransformersReinforcement2019","taxon":"Reference","tags":[],"route":"/parisottoStabilizingTransformersReinforcement2019/","metas":{"doi":"10.48550/arXiv.1910.06764","external":"https://arxiv.org/abs/1910.06764","bibtex":"@misc{parisottoStabilizingTransformersReinforcement2019,\n title = {Stabilizing {{Transformers}} for {{Reinforcement Learning}}},\n author = {Parisotto, Emilio and Song, H. Francis and Rae, Jack W. and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant M. and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew M. and Heess, Nicolas and Hadsell, Raia},\n year = {2019},\n doi = {10.48550/arXiv.1910.06764},\n urldate = {2025-02-10},\n number = {arXiv:1910.06764},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/QP6P6B9C/Parisotto et al. - 2019 - Stabilizing Transformers for Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.},\n primaryclass = {cs},\n eprint = {1910.06764},\n month = {October}\n}"}},{"title":"Learning Latent Dynamics for Planning from Pixels","uri":"hafnerLearningLatentDynamics2019","taxon":"Reference","tags":[],"route":"/hafnerLearningLatentDynamics2019/","metas":{"doi":"10.48550/arXiv.1811.04551","external":"https://arxiv.org/abs/1811.04551","bibtex":"@misc{hafnerLearningLatentDynamics2019,\n title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},\n author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},\n year = {2019},\n doi = {10.48550/arXiv.1811.04551},\n urldate = {2025-06-15},\n number = {arXiv:1811.04551},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/LBZWJU59/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},\n primaryclass = {cs},\n eprint = {1811.04551},\n month = {June}\n}"}},{"title":"FFJORD: Free-Form Continuous DYnamics for Scalable Reversible Generative Models","uri":"grathwohlFFJORDFreeFormContinuous2019","taxon":"Reference","tags":[],"route":"/grathwohlFFJORDFreeFormContinuous2019/","metas":{"bibtex":"@article{grathwohlFFJORDFreeFormContinuous2019,\n title = {{{FFJORD}}: {{Free-Form Continuous DYnamics}} for {{Scalable Reversible Generative Models}}},\n author = {Grathwohl, Will and Chen, Ricky T Q and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},\n year = {2019},\n file = {/home/kellen/Downloads/pdfs/storage/2H337KKY/Grathwohl et al. - 2019 - FFJORD FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE.pdf},\n langid = {english},\n abstract = {Reversible generative models map points from a simple distribution to a complex distribution through an easily invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the logdensity. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, improving the state-ofthe-art among exact likelihood methods with efficient sampling.}\n}"}},{"title":"Near-Optimal Representation Learning for Hierarchical Reinforcement Learning","uri":"nachumNearOptimalRepresentationLearning2019","taxon":"Reference","tags":[],"route":"/nachumNearOptimalRepresentationLearning2019/","metas":{"external":"https://arxiv.org/abs/1810.01257","bibtex":"@misc{nachumNearOptimalRepresentationLearning2019,\n title = {Near-{{Optimal Representation Learning}} for {{Hierarchical Reinforcement Learning}}},\n author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},\n year = {2019},\n urldate = {2024-11-07},\n number = {arXiv:1810.01257},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/SIK848RC/Nachum et al. - 2019 - Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).},\n primaryclass = {cs},\n eprint = {1810.01257},\n month = {January}\n}"}},{"title":"Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control","uri":"lowreyPlanOnlineLearn2019","taxon":"Reference","tags":[],"route":"/lowreyPlanOnlineLearn2019/","metas":{"doi":"10.48550/arXiv.1811.01848","external":"https://arxiv.org/abs/1811.01848","bibtex":"@misc{lowreyPlanOnlineLearn2019,\n title = {Plan {{Online}}, {{Learn Offline}}: {{Efficient Learning}} and {{Exploration}} via {{Model-Based Control}}},\n author = {Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor},\n year = {2019},\n doi = {10.48550/arXiv.1811.01848},\n urldate = {2025-06-15},\n number = {arXiv:1811.01848},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IRAHSQC4/Lowrey et al. - 2019 - Plan Online, Learn Offline Efficient Learning and Exploration via Model-Based Control.pdf;/home/kellen/Downloads/pdfs/storage/4MIV3P2X/1811.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.},\n primaryclass = {cs},\n eprint = {1811.01848},\n month = {January},\n shorttitle = {Plan {{Online}}, {{Learn Offline}}}\n}"}},{"title":"Representation Learning with Contrastive Predictive Coding","uri":"oordRepresentationLearningContrastive2019","taxon":"Reference","tags":[],"route":"/oordRepresentationLearningContrastive2019/","metas":{"external":"https://arxiv.org/abs/1807.03748","bibtex":"@misc{oordRepresentationLearningContrastive2019,\n title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},\n author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},\n year = {2019},\n urldate = {2024-11-21},\n number = {arXiv:1807.03748},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/8ZBPFQEK/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf;/home/kellen/Downloads/pdfs/storage/H9PLPKN6/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},\n primaryclass = {cs},\n eprint = {1807.03748},\n month = {January}\n}"}},{"title":"Universal Successor Features Approximators","uri":"borsaUniversalSuccessorFeatures2018","taxon":"Reference","tags":[],"route":"/borsaUniversalSuccessorFeatures2018/","metas":{"doi":"10.48550/arXiv.1812.07626","external":"https://arxiv.org/abs/1812.07626","bibtex":"@misc{borsaUniversalSuccessorFeatures2018,\n title = {Universal {{Successor Features Approximators}}},\n author = {Borsa, Diana and Barreto, Andr{\\'e} and Quan, John and Mankowitz, Daniel and Munos, R{\\'e}mi and van Hasselt, Hado and Silver, David and Schaul, Tom},\n year = {2018},\n doi = {10.48550/arXiv.1812.07626},\n urldate = {2025-06-30},\n number = {arXiv:1812.07626},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/XEY8N7SW/Borsa et al. - 2018 - Universal Successor Features Approximators.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.},\n primaryclass = {cs},\n eprint = {1812.07626},\n month = {December}\n}"}},{"title":"Data-Efficient Hierarchical Reinforcement Learning","uri":"nachumDataEfficientHierarchicalReinforcement2018","taxon":"Reference","tags":[],"route":"/nachumDataEfficientHierarchicalReinforcement2018/","metas":{"external":"https://arxiv.org/abs/1805.08296","bibtex":"@misc{nachumDataEfficientHierarchicalReinforcement2018,\n title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},\n author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},\n year = {2018},\n urldate = {2024-10-24},\n number = {arXiv:1805.08296},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/E5S647TN/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},\n primaryclass = {cs},\n eprint = {1805.08296},\n month = {October}\n}"}},{"title":"Diversity is All You Need: Learning Skills without a Reward Function","uri":"eysenbachDiversityAllYou2018","taxon":"Reference","tags":[],"route":"/eysenbachDiversityAllYou2018/","metas":{"doi":"10.48550/arXiv.1802.06070","external":"https://arxiv.org/abs/1802.06070","bibtex":"@misc{eysenbachDiversityAllYou2018,\n title = {Diversity Is {{All You Need}}: {{Learning Skills}} without a {{Reward Function}}},\n author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},\n year = {2018},\n doi = {10.48550/arXiv.1802.06070},\n urldate = {2025-06-30},\n number = {arXiv:1802.06070},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/RMASJINK/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose ``Diversity is All You Need''(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},\n primaryclass = {cs},\n eprint = {1802.06070},\n month = {October},\n shorttitle = {Diversity Is {{All You Need}}}\n}"}},{"title":"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning","uri":"chenTVMAutomatedEndtoEnd2018","taxon":"Reference","tags":[],"route":"/chenTVMAutomatedEndtoEnd2018/","metas":{"external":"https://arxiv.org/abs/1802.04799","bibtex":"@misc{chenTVMAutomatedEndtoEnd2018,\n title = {{{TVM}}: {{An Automated End-to-End Optimizing Compiler}} for {{Deep Learning}}},\n author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},\n year = {2018},\n urldate = {2024-11-07},\n number = {arXiv:1802.04799},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IX3XN5F4/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler for Deep Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},\n archiveprefix = {arXiv},\n abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},\n primaryclass = {cs},\n eprint = {1802.04799},\n month = {October},\n shorttitle = {{{TVM}}}\n}"}},{"title":"Minimap2: Pairwise alignment for nucleotide sequences","uri":"liMinimap2PairwiseAlignment2018","taxon":"Reference","tags":[],"route":"/liMinimap2PairwiseAlignment2018/","metas":{"doi":"10.1093/bioinformatics/bty191","bibtex":"@article{liMinimap2PairwiseAlignment2018,\n title = {Minimap2: Pairwise Alignment for Nucleotide Sequences},\n author = {Li, Heng},\n year = {2018},\n doi = {10.1093/bioinformatics/bty191},\n urldate = {2024-12-11},\n journal = {Bioinformatics},\n editor = {Birol, Inanc},\n volume = {34},\n number = {18},\n pages = {3094--3100},\n file = {/home/kellen/Downloads/pdfs/storage/U4V4YFWK/Li - 2018 - Minimap2 pairwise alignment for nucleotide sequen.pdf},\n langid = {english},\n copyright = {https://academic.oup.com/journals/pages/open\\_access/funder\\_policies/chorus/standard\\_publication\\_model},\n abstract = {Motivation: Recent advances in sequencing technologies promise ultra-long reads of \\$100 kb in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms.},\n issn = {1367-4803, 1367-4811},\n month = {September},\n shorttitle = {Minimap2}\n}"}},{"title":"Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency","uri":"maNoiseContrastiveEstimation2018","taxon":"Reference","tags":[],"route":"/maNoiseContrastiveEstimation2018/","metas":{"doi":"10.48550/arXiv.1809.01812","external":"https://arxiv.org/abs/1809.01812","bibtex":"@misc{maNoiseContrastiveEstimation2018,\n title = {Noise {{Contrastive Estimation}} and {{Negative Sampling}} for {{Conditional Models}}: {{Consistency}} and {{Statistical Efficiency}}},\n author = {Ma, Zhuang and Collins, Michael},\n year = {2018},\n doi = {10.48550/arXiv.1809.01812},\n urldate = {2025-02-03},\n number = {arXiv:1809.01812},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/BC92TTTC/Ma and Collins - 2018 - Noise Contrastive Estimation and Negative Sampling for Conditional Models Consistency and Statistic.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Methodology},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for loglinear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the rankingbased variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods.},\n primaryclass = {cs},\n eprint = {1809.01812},\n month = {September},\n shorttitle = {Noise {{Contrastive Estimation}} and {{Negative Sampling}} for {{Conditional Models}}}\n}"}},{"title":"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review","uri":"levineReinforcementLearningControl2018","taxon":"Reference","tags":[],"route":"/levineReinforcementLearningControl2018/","metas":{"doi":"10.48550/arXiv.1805.00909","external":"https://arxiv.org/abs/1805.00909","bibtex":"@misc{levineReinforcementLearningControl2018,\n title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},\n author = {Levine, Sergey},\n year = {2018},\n doi = {10.48550/arXiv.1805.00909},\n urldate = {2025-06-09},\n number = {arXiv:1805.00909},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/IYEPBARH/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf;/home/kellen/Downloads/pdfs/storage/RJMAVWQB/Massiani et al. - 2023 - Safe Value Functions.pdf;/home/kellen/Downloads/pdfs/storage/VLBM7SRT/1805.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},\n primaryclass = {cs},\n eprint = {1805.00909},\n month = {May},\n shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}}\n}"}},{"title":"Reinforcement learning: An introduction","uri":"suttonReinforcementLearningIntroduction2018","taxon":"Reference","tags":[],"route":"/suttonReinforcementLearningIntroduction2018/","metas":{"bibtex":"@book{suttonReinforcementLearningIntroduction2018,\n title = {Reinforcement Learning: An Introduction},\n author = {Sutton, Richard S. and Barto, Andrew G.},\n year = {2018},\n isbn = {978-0-262-03924-6},\n edition = {Second edition},\n series = {Adaptive Computation and Machine Learning Series},\n publisher = {The MIT Press},\n address = {Cambridge, Massachusetts},\n file = {/home/kellen/Downloads/pdfs/storage/DY4UI6G7/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},\n keywords = {Reinforcement learning},\n lccn = {Q325.6 .R45 2018},\n langid = {english},\n abstract = {\"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms.\"--},\n shorttitle = {Reinforcement Learning}\n}"}},{"title":"Proximal Policy Optimization Algorithms","uri":"schulmanProximalPolicyOptimization2017","taxon":"Reference","tags":[],"route":"/schulmanProximalPolicyOptimization2017/","metas":{"doi":"10.48550/arXiv.1707.06347","external":"https://arxiv.org/abs/1707.06347","bibtex":"@misc{schulmanProximalPolicyOptimization2017,\n title = {Proximal {{Policy Optimization Algorithms}}},\n author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},\n year = {2017},\n doi = {10.48550/arXiv.1707.06347},\n urldate = {2025-06-27},\n number = {arXiv:1707.06347},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/UUPSNPZG/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},\n primaryclass = {cs},\n eprint = {1707.06347},\n month = {August}\n}"}},{"title":"FeUdal Networks for Hierarchical Reinforcement Learning","uri":"vezhnevetsFeUdalNetworksHierarchical2017","taxon":"Reference","tags":[],"route":"/vezhnevetsFeUdalNetworksHierarchical2017/","metas":{"bibtex":"@inproceedings{vezhnevetsFeUdalNetworksHierarchical2017,\n title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},\n author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},\n year = {2017},\n urldate = {2025-02-10},\n booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},\n pages = {3540--3549},\n publisher = {PMLR},\n file = {/home/kellen/Downloads/pdfs/storage/U76B4QZ8/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf},\n langid = {english},\n abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a slower time scale and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.},\n issn = {2640-3498},\n month = {July}\n}"}},{"title":"An overview of gradient descent optimization algorithms","uri":"ruderOverviewGradientDescent2017","taxon":"Reference","tags":[],"route":"/ruderOverviewGradientDescent2017/","metas":{"doi":"10.48550/arXiv.1609.04747","external":"https://arxiv.org/abs/1609.04747","bibtex":"@misc{ruderOverviewGradientDescent2017,\n title = {An Overview of Gradient Descent Optimization Algorithms},\n author = {Ruder, Sebastian},\n year = {2017},\n doi = {10.48550/arXiv.1609.04747},\n urldate = {2025-02-28},\n number = {arXiv:1609.04747},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/L2GP2CTQ/Ruder - 2017 - An overview of gradient descent optimization algorithms.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},\n primaryclass = {cs},\n eprint = {1609.04747},\n month = {June}\n}"}},{"title":"A Deep Hierarchical Approach to Lifelong Learning in Minecraft","uri":"tesslerDeepHierarchicalApproach2017","taxon":"Reference","tags":[],"route":"/tesslerDeepHierarchicalApproach2017/","metas":{"doi":"10.1609/aaai.v31i1.10744","bibtex":"@article{tesslerDeepHierarchicalApproach2017,\n title = {A {{Deep Hierarchical Approach}} to {{Lifelong Learning}} in {{Minecraft}}},\n author = {Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},\n year = {2017},\n doi = {10.1609/aaai.v31i1.10744},\n urldate = {2025-03-17},\n journal = {Proceedings of the AAAI Conference on Artificial Intelligence},\n volume = {31},\n number = {1},\n file = {/home/kellen/Downloads/pdfs/storage/YRIUA9S2/Tessler et al. - 2017 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft.pdf},\n langid = {english},\n abstract = {We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledgebase. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two techniques: (1) a deep skill array and (2) skill distillation, our novel variation of policy distillation (Rusu et al. 2015) for learning skills. Skill distillation enables the HDRLN to efficiently retain knowledge and therefore scale in lifelong learning, by accumulating knowledge and encapsulating multiple reusable skills into a single distilled network. The H-DRLN exhibits superior performance and lower learning sample complexity compared to the regular Deep Q Network (Mnih et al. 2015) in sub-domains of Minecraft.},\n issn = {2374-3468, 2159-5399},\n month = {February}\n}"}},{"title":"Graph Theory","uri":"diestelGraphTheory2017","taxon":"Reference","tags":[],"route":"/diestelGraphTheory2017/","metas":{"doi":"10.1007/978-3-662-53622-3","bibtex":"@book{diestelGraphTheory2017,\n title = {Graph {{Theory}}},\n author = {Diestel, Reinhard},\n year = {2017},\n isbn = {978-3-662-53621-6 978-3-662-53622-3},\n doi = {10.1007/978-3-662-53622-3},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {173},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Downloads/pdfs/storage/I3MHRRZZ/Diestel - 2017 - Graph Theory.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Contextual Decision Processes with Low Bellman Rank are PAC-Learnable","uri":"jiangContextualDecisionProcesses2016","taxon":"Reference","tags":[],"route":"/jiangContextualDecisionProcesses2016/","metas":{"doi":"10.48550/arXiv.1610.09512","external":"https://arxiv.org/abs/1610.09512","bibtex":"@misc{jiangContextualDecisionProcesses2016,\n title = {Contextual {{Decision Processes}} with {{Low Bellman Rank}} Are {{PAC-Learnable}}},\n author = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E.},\n year = {2016},\n doi = {10.48550/arXiv.1610.09512},\n urldate = {2025-02-05},\n number = {arXiv:1610.09512},\n publisher = {arXiv},\n file = {/home/kellen/Downloads/pdfs/storage/RCXBGU3P/Jiang et al. - 2016 - Contextual Decision Processes with Low Bellman Rank are PAC-Learnable.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new model called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a complexity measure, the Bellman rank , that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman rank. Our algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.},\n primaryclass = {cs},\n eprint = {1610.09512},\n month = {December}\n}"}},{"title":"Essentials of Stochastic Processes","uri":"durrettEssentialsStochasticProcesses2016","taxon":"Reference","tags":[],"route":"/durrettEssentialsStochasticProcesses2016/","metas":{"doi":"10.1007/978-3-319-45614-0","bibtex":"@book{durrettEssentialsStochasticProcesses2016,\n title = {Essentials of {{Stochastic Processes}}},\n author = {Durrett, Richard},\n year = {2016},\n isbn = {978-3-319-45613-3 978-3-319-45614-0},\n doi = {10.1007/978-3-319-45614-0},\n urldate = {2024-10-18},\n series = {Springer {{Texts}} in {{Statistics}}},\n publisher = {Springer International Publishing},\n address = {Cham},\n file = {/home/kellen/Downloads/pdfs/storage/MVRAWH8Q/Durrett - 2016 - Essentials of Stochastic Processes.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Quasi-monotone Subgradient Methods for Nonsmooth Convex Minimization","uri":"nesterovQuasimonotoneSubgradientMethods2015","taxon":"Reference","tags":[],"route":"/nesterovQuasimonotoneSubgradientMethods2015/","metas":{"doi":"10.1007/s10957-014-0677-5","bibtex":"@article{nesterovQuasimonotoneSubgradientMethods2015,\n title = {Quasi-Monotone {{Subgradient Methods}} for {{Nonsmooth Convex Minimization}}},\n author = {Nesterov, {\\relax Yu}. and Shikhman, V.},\n year = {2015},\n doi = {10.1007/s10957-014-0677-5},\n urldate = {2025-05-01},\n journal = {Journal of Optimization Theory and Applications},\n volume = {165},\n number = {3},\n pages = {917--940},\n file = {/home/kellen/Downloads/pdfs/storage/RU7CP4EP/Nesterov and Shikhman - 2015 - Quasi-monotone Subgradient Methods for Nonsmooth Convex Minimization.pdf},\n keywords = {68Q25,90C25,90C47,Convex optimization,Nonsmooth optimzation,Primal-dual methods,Rate of convergence,Subgradient methods},\n langid = {english},\n abstract = {In this paper, we develop new subgradient methods for solving nonsmooth convex optimization problems. These methods guarantee the best possible rate of convergence for the whole sequence of test points. Our methods are applicable as efficient real-time stabilization tools for potential systems with infinite horizon. Preliminary numerical experiments confirm a high efficiency of the new schemes.},\n issn = {1573-2878},\n month = {June}\n}"}},{"title":"Tuning-free step-size adaptation","uri":"mahmoodTuningfreeStepsizeAdaptation2012","taxon":"Reference","tags":[],"route":"/mahmoodTuningfreeStepsizeAdaptation2012/","metas":{"doi":"10.1109/ICASSP.2012.6288330","bibtex":"@inproceedings{mahmoodTuningfreeStepsizeAdaptation2012,\n title = {Tuning-Free Step-Size Adaptation},\n author = {Mahmood, Ashique Rupam and Sutton, Richard S. and Degris, Thomas and Pilarski, Patrick M.},\n year = {2012},\n isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},\n doi = {10.1109/ICASSP.2012.6288330},\n urldate = {2025-02-05},\n booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},\n pages = {2121--2124},\n publisher = {IEEE},\n address = {Kyoto, Japan},\n file = {/home/kellen/Downloads/pdfs/storage/ER798P67/Mahmood et al. - 2012 - Tuning-free step-size adaptation.pdf},\n langid = {english},\n abstract = {Incremental learning algorithms based on gradient descent are effective and popular in online supervised learning, reinforcement learning, signal processing, and many other application areas. An oft-noted drawback of these algorithms is that they include a step-size parameter that needs to be tuned for best performance, which may require manual intervention and significant domain knowledge or additional data. In many cases, an entire vector of step-size parameters (e.g., one for each input feature) needs to be tuned in order to attain the best performance of the algorithm. To address this, several methods have been proposed for adapting step sizes online. For example, Sutton's IDBD method can find the best vector step size for the LMS algorithm, and Schraudolph's ELK1 method, an extension of IDBD to neural networks, has proven effective on large applications, such as 3D hand tracking. However, to date all such step-size adaptation methods have included a tunable step-size parameter of their own, which we call the meta-step-size parameter. In this paper we show that the performance of existing step-size adaptation methods are strongly dependent on the choice of their meta-step-size parameter and that their meta-step-size parameter cannot be set reliably in a problem-independent way. We introduce a series of modifications and normalizations to the IDBD method that together eliminate the need to tune the meta-step-size parameter to the particular problem. We show that the resulting overall algorithm, called Autostep, performs as well or better than the existing step-size adaptation methods on a number of idealized and robot prediction problems and does not require any tuning of its meta-step-size parameter. The ideas behind Autostep are not restricted to the IDBD method and the same principles are potentially applicable to other incremental learning settings, such as reinforcement learning.},\n month = {March}\n}"}},{"title":"Efficient BackProp","uri":"LeCun2012","taxon":"Reference","tags":[],"route":"/LeCun2012/","metas":{"doi":"10.1007/978-3-642-35289-8_3","bibtex":"@incollection{LeCun2012,\n title = {Efficient {{BackProp}}},\n author = {LeCun, Yann A. and Bottou, L{\\'e}on and Orr, Genevieve B. and M{\\\"u}ller, Klaus-Robert},\n year = {2012},\n isbn = {978-3-642-35289-8},\n doi = {10.1007/978-3-642-35289-8_3},\n booktitle = {Neural Networks: {{Tricks}} of the Trade: {{Second}} Edition},\n editor = {Montavon, Gr{\\'e}goire and Orr, Genevi{\\`e}ve B. and M{\\\"u}ller, Klaus-Robert},\n pages = {9--48},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.}\n}"}},{"title":"Introduction to Smooth Manifolds","uri":"leeIntroductionSmoothManifolds2012","taxon":"Reference","tags":[],"route":"/leeIntroductionSmoothManifolds2012/","metas":{"doi":"10.1007/978-1-4419-9982-5","bibtex":"@book{leeIntroductionSmoothManifolds2012,\n title = {Introduction to {{Smooth Manifolds}}},\n author = {Lee, John M.},\n year = {2012},\n isbn = {978-1-4419-9981-8 978-1-4419-9982-5},\n doi = {10.1007/978-1-4419-9982-5},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {218},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Downloads/pdfs/storage/GFNZMLM6/Lee - 2012 - Introduction to Smooth Manifolds.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Neural Networks: Tricks of the Trade: Second Edition","uri":"montavonNeuralNetworksTricks2012","taxon":"Reference","tags":[],"route":"/montavonNeuralNetworksTricks2012/","metas":{"doi":"10.1007/978-3-642-35289-8","bibtex":"@book{montavonNeuralNetworksTricks2012,\n title = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},\n year = {2012},\n isbn = {978-3-642-35288-1 978-3-642-35289-8},\n doi = {10.1007/978-3-642-35289-8},\n urldate = {2025-02-28},\n series = {Lecture {{Notes}} in {{Computer Science}}},\n editor = {Montavon, Gr{\\'e}goire and Orr, Genevi{\\`e}ve B. and M{\\\"u}ller, Klaus-Robert},\n volume = {7700},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Downloads/pdfs/storage/3R38Z36Y/Montavon et al. - 2012 - Neural Networks Tricks of the Trade Second Edition.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm},\n shorttitle = {Neural {{Networks}}}\n}"}},{"title":"Heuristic Search for Generalized Stochastic Shortest Path MDPs","uri":"kolobovHeuristicSearchGeneralized2011","taxon":"Reference","tags":[],"route":"/kolobovHeuristicSearchGeneralized2011/","metas":{"doi":"10.1609/icaps.v21i1.13452","bibtex":"@article{kolobovHeuristicSearchGeneralized2011,\n title = {Heuristic {{Search}} for {{Generalized Stochastic Shortest Path MDPs}}},\n author = {Kolobov, Andrey and Mausam, Mausam and Weld, Daniel and Geffner, Hector},\n year = {2011},\n doi = {10.1609/icaps.v21i1.13452},\n urldate = {2025-03-22},\n journal = {Proceedings of the International Conference on Automated Planning and Scheduling},\n volume = {21},\n pages = {130--137},\n file = {/home/kellen/Downloads/pdfs/storage/Q77EPBX6/Kolobov et al. - 2011 - Heuristic Search for Generalized Stochastic Shortest Path MDPs.pdf},\n langid = {english},\n copyright = {Copyright (c) 2021 Proceedings of the International Conference on Automated Planning and Scheduling},\n abstract = {Research in efficient methods for solving infinite-horizon MDPs has so far concentrated primarily on discounted MDPs and the more general stochastic shortest path problems (SSPs). These are MDPs with 1) an optimal value function V* that is the unique solution of Bellman equation and 2) optimal policies that are the greedy policies w.r.t. V*. This paper\\&rsquo;s main contribution is the description of a new class of MDPs, that have well-defined optimal solutions that do not comply with either 1 or 2 above. We call our new class Generalized Stochastic Shortest Path (GSSP) problems. GSSP allows more general reward structure than SSP and subsumes several established MDP types including SSP, positive-bounded, negative, and discounted-reward models. While existing efficient heuristic search algorithms like LAO* and LRTDP are not guaranteed to converge to the optimal value function for GSSPs, we present a new heuristic-search-based family of algorithms, FRET (Find, Revise, Eliminate Traps). A preliminary empirical evaluation shows that FRET solves GSSPs much more efficiently than Value Iteration.},\n issn = {2334-0843},\n month = {March}\n}"}},{"title":"Probability and Stochastics","uri":"cinlarProbabilityStochastics2011","taxon":"Reference","tags":[],"route":"/cinlarProbabilityStochastics2011/","metas":{"doi":"10.1007/978-0-387-87859-1","bibtex":"@book{cinlarProbabilityStochastics2011,\n title = {Probability and {{Stochastics}}},\n author = {{\\c C}inlar, Erhan},\n year = {2011},\n isbn = {978-0-387-87858-4 978-0-387-87859-1},\n doi = {10.1007/978-0-387-87859-1},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {261},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Downloads/pdfs/storage/GHJPXQJR/Çinlar - 2011 - Probability and Stochastics.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},{"title":"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models","uri":"gutmannNoisecontrastiveEstimationNew2010","taxon":"Reference","tags":[],"route":"/gutmannNoisecontrastiveEstimationNew2010/","metas":{"bibtex":"@inproceedings{gutmannNoisecontrastiveEstimationNew2010,\n title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},\n author = {Gutmann, Michael and Hyv{\\\"a}rinen, Aapo},\n year = {2010},\n urldate = {2025-01-24},\n booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},\n pages = {297--304},\n publisher = {{JMLR Workshop and Conference Proceedings}},\n file = {/home/kellenkanarios/Downloads/Papers/Papers/Contrastive RL/Gutmann_Hyvärinen_2010_Noise-contrastive estimation.pdf},\n langid = {english},\n abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},\n issn = {1938-7228},\n month = {March},\n shorttitle = {Noise-Contrastive Estimation}\n}"}},{"title":"Numerical optimization","uri":"nocedalNumericalOptimization2006","taxon":"Reference","tags":[],"route":"/nocedalNumericalOptimization2006/","metas":{"bibtex":"@book{nocedalNumericalOptimization2006,\n title = {Numerical Optimization},\n author = {Nocedal, Jorge and Wright, Stephen J.},\n year = {2006},\n isbn = {978-0-387-30303-1},\n edition = {2nd ed},\n series = {Springer Series in Operations Research},\n publisher = {Springer},\n address = {New York},\n file = {/home/kellen/Downloads/pdfs/storage/ZNRAHE63/Nocedal and Wright - 2006 - Numerical optimization.pdf},\n annotation = {OCLC: ocm68629100},\n keywords = {Mathematical optimization},\n lccn = {QA402.5 .N62 2006},\n langid = {english}\n}"}},{"title":"Path Planning and Obstacle Avoidance for Autonomous Mobile Robots: A Review","uri":"kunchevPathPlanningObstacle2006","taxon":"Reference","tags":[],"route":"/kunchevPathPlanningObstacle2006/","metas":{"doi":"10.1007/11893004_70","bibtex":"@inproceedings{kunchevPathPlanningObstacle2006,\n title = {Path {{Planning}} and {{Obstacle Avoidance}} for {{Autonomous Mobile Robots}}: {{A Review}}},\n author = {Kunchev, Voemir and Jain, Lakhmi and Ivancevic, Vladimir and Finn, Anthony},\n year = {2006},\n isbn = {978-3-540-46539-3},\n doi = {10.1007/11893004_70},\n booktitle = {Knowledge-{{Based Intelligent Information}} and {{Engineering Systems}}},\n editor = {Gabrys, Bogdan and Howlett, Robert J. and Jain, Lakhmi C.},\n pages = {537--544},\n publisher = {Springer},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Downloads/pdfs/storage/WZKW4AGM/Kunchev et al. - 2006 - Path Planning and Obstacle Avoidance for Autonomous Mobile Robots A Review.pdf},\n keywords = {Goal Position,Mobile Robot,Obstacle Avoidance,Path Planning,Voronoi Diagram},\n langid = {english},\n abstract = {Recent advances in the area of mobile robotics caused growing attention of the armed forces, where the necessity for unmanned vehicles being able to carry out the ``dull and dirty'' operations, thus avoid endangering the life of the military personnel. UAV offers a great advantage in supplying reconnaissance data to the military personnel on the ground, thus lessening the life risk of the troops. In this paper we analyze various techniques for path planning and obstacle avoidance and cooperation issues for multiple mobile robots. We also present a generic dynamics and control model for steering a UAV along a collision free path from a start to a goal position.},\n shorttitle = {Path {{Planning}} and {{Obstacle Avoidance}} for {{Autonomous Mobile Robots}}}\n}"}},{"title":"Stochastic Differential Equations","uri":"oksendalStochasticDifferentialEquations2003","taxon":"Reference","tags":[],"route":"/oksendalStochasticDifferentialEquations2003/","metas":{"doi":"10.1007/978-3-642-14394-6","bibtex":"@book{oksendalStochasticDifferentialEquations2003,\n title = {Stochastic {{Differential Equations}}},\n author = {{\\O}ksendal, Bernt},\n year = {2003},\n isbn = {978-3-540-04758-2 978-3-642-14394-6},\n doi = {10.1007/978-3-642-14394-6},\n urldate = {2025-01-25},\n series = {Universitext},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Downloads/pdfs/storage/WSH4RNVC/Øksendal - 2003 - Stochastic Differential Equations.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm}\n}"}},{"title":"Stochastic Differential Equations","uri":"oksendalStochasticDifferentialEquations2003a","taxon":"Reference","tags":[],"route":"/oksendalStochasticDifferentialEquations2003a/","metas":{"doi":"10.1007/978-3-642-14394-6","bibtex":"@book{oksendalStochasticDifferentialEquations2003a,\n title = {Stochastic {{Differential Equations}}},\n author = {{\\O}ksendal, Bernt},\n year = {2003},\n isbn = {978-3-540-04758-2 978-3-642-14394-6},\n doi = {10.1007/978-3-642-14394-6},\n urldate = {2025-06-23},\n series = {Universitext},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Downloads/pdfs/storage/CKXWK7Z2/Øksendal - 2003 - Stochastic Differential Equations.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm}\n}"}},{"title":"Differential Equations and Dynamical Systems","uri":"perkoDifferentialEquationsDynamical2001","taxon":"Reference","tags":[],"route":"/perkoDifferentialEquationsDynamical2001/","metas":{"doi":"10.1007/978-1-4613-0003-8","bibtex":"@book{perkoDifferentialEquationsDynamical2001,\n title = {Differential {{Equations}} and {{Dynamical Systems}}},\n author = {Perko, Lawrence},\n year = {2001},\n isbn = {978-1-4612-6526-9 978-1-4613-0003-8},\n doi = {10.1007/978-1-4613-0003-8},\n urldate = {2025-01-25},\n series = {Texts in {{Applied Mathematics}}},\n editor = {Marsden, J. E. and Sirovich, L. and Golubitsky, M.},\n volume = {7},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Downloads/pdfs/storage/FRK58JEY/Perko - 2001 - Differential Equations and Dynamical Systems.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm}\n}"}},{"title":"An analysis of temporal-difference learning with function approximation","uri":"tsitsiklisAnalysisTemporaldifferenceLearning1997","taxon":"Reference","tags":[],"route":"/tsitsiklisAnalysisTemporaldifferenceLearning1997/","metas":{"doi":"10.1109/9.580874","bibtex":"@article{tsitsiklisAnalysisTemporaldifferenceLearning1997,\n title = {An Analysis of Temporal-Difference Learning with Function Approximation},\n author = {Tsitsiklis, J.N. and Van Roy, B.},\n year = {1997},\n doi = {10.1109/9.580874},\n urldate = {2025-01-16},\n journal = {IEEE Transactions on Automatic Control},\n volume = {42},\n number = {5},\n pages = {674--690},\n file = {/home/kellen/Downloads/pdfs/storage/58WIJRVR/Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with function approximation.pdf},\n langid = {english},\n copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},\n issn = {00189286},\n month = {May}\n}"}},{"title":"Neuro-dynamic programming","uri":"bertsekasNeurodynamicProgramming1996","taxon":"Reference","tags":[],"route":"/bertsekasNeurodynamicProgramming1996/","metas":{"bibtex":"@book{bertsekasNeurodynamicProgramming1996,\n title = {Neuro-Dynamic Programming},\n author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},\n year = {1996},\n isbn = {978-1-886529-10-6},\n series = {Optimization and Neural Computation Series},\n publisher = {Athena Scientific},\n address = {Belmont, Mass},\n file = {/home/kellen/Downloads/pdfs/storage/JEIDXXG5/Bertsekas and Tsitsiklis - 1996 - Neuro-dynamic programming.pdf},\n keywords = {Dynamic programming,Mathematical optimization,Neural networks (Computer Science)},\n lccn = {QA76.87 .B47 1996},\n langid = {english}\n}"}},{"title":"Probability and measure","uri":"billingsleyProbabilityMeasure1995","taxon":"Reference","tags":[],"route":"/billingsleyProbabilityMeasure1995/","metas":{"bibtex":"@book{billingsleyProbabilityMeasure1995,\n title = {Probability and Measure},\n author = {Billingsley, Patrick},\n year = {1995},\n isbn = {978-0-471-00710-4},\n edition = {3. ed},\n series = {Wiley Series in Probability and Mathematical Statistics},\n publisher = {Wiley},\n address = {New York, NY},\n file = {/home/kellen/Downloads/pdfs/storage/JSCDN2UH/Billingsley - 1995 - Probability and measure.pdf},\n langid = {english}\n}"}},{"title":"Learning to predict by the methods of temporal differences","uri":"suttonLearningPredictMethods1988","taxon":"Reference","tags":[],"route":"/suttonLearningPredictMethods1988/","metas":{"doi":"10.1007/BF00115009","bibtex":"@article{suttonLearningPredictMethods1988,\n title = {Learning to Predict by the Methods of Temporal Differences},\n author = {Sutton, Richard S.},\n year = {1988},\n doi = {10.1007/BF00115009},\n urldate = {2025-02-16},\n journal = {Machine Learning},\n volume = {3},\n number = {1},\n pages = {9--44},\n file = {/home/kellen/Downloads/pdfs/storage/Q953STTS/Sutton - 1988 - Learning to predict by the methods of temporal differences.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm},\n abstract = {This article introduces a class of incremental learning procedures specialized for prediction---that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporaldifference methods can be applied to advantage.},\n issn = {0885-6125, 1573-0565},\n month = {August}\n}"}},{"title":"https://kellenkanarios.com/005E/","uri":"005E","taxon":null,"tags":[],"route":"/005E/","metas":{}},{"title":"https://kellenkanarios.com/latex-preamble/","uri":"latex-preamble","taxon":null,"tags":[],"route":"/latex-preamble/","metas":{}},{"title":"https://kellenkanarios.com/007Q/","uri":"007Q","taxon":null,"tags":[],"route":"/007Q/","metas":{}},{"title":"\n  #Deliberate-Practice\n","uri":"007U","taxon":null,"tags":[],"route":"/007U/","metas":{}},{"title":"\n  #Deliberate-Practice\n: LLMs Lecture 1 (Overview, tokenization)","uri":"007X","taxon":null,"tags":[],"route":"/007X/","metas":{}},{"title":"Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps","uri":"ellisAdamLocalTime","taxon":"Reference","tags":[],"route":"/ellisAdamLocalTime/","metas":{"bibtex":"@article{ellisAdamLocalTime,\n title = {Adam on {{Local Time}}: {{Addressing Nonstationarity}} in {{RL}} with {{Relative Adam Timesteps}}},\n author = {Ellis, Benjamin and Jackson, Matthew T and Lupu, Andrei and Goldie, Alexander D},\n file = {/home/kellen/Downloads/pdfs/storage/ATS4JHHU/Ellis et al. - Adam on Local Time Addressing Nonstationarity in RL with Relative Adam Timesteps.pdf},\n langid = {english},\n abstract = {In reinforcement learning (RL), it is common to apply techniques used broadly in machine learning such as neural network function approximators and momentumbased optimizers [1, 2]. However, such tools were largely developed for supervised learning rather than nonstationary RL, leading practitioners to adopt target networks [3], clipped policy updates [4], and other RL-specific implementation tricks [5, 6] to combat this mismatch, rather than directly adapting this toolchain for use in RL. In this paper, we take a different approach and instead address the effect of nonstationarity by adapting the widely used Adam optimiser [7]. We first analyse the impact of nonstationary gradient magnitude---such as that caused by a change in target network---on Adam's update size, demonstrating that such a change can lead to large updates and hence sub-optimal performance. To address this, we introduce Adam with Relative Timesteps, or Adam-Rel. Rather than using the global timestep in the Adam update, Adam-Rel uses the local timestep within an epoch, essentially resetting Adam's timestep to 0 after target changes. We demonstrate that this avoids large updates and reduces to learning rate annealing in the absence of such increases in gradient magnitude. Evaluating Adam-Rel in both on-policy and off-policy RL, we demonstrate improved performance in both Atari and Craftax. We then show that increases in gradient norm occur in RL in practice, and examine the differences between our theoretical model and the observed data.}\n}"}},{"title":"Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta","uri":"suttonAdaptingBiasGradient","taxon":"Reference","tags":[],"route":"/suttonAdaptingBiasGradient/","metas":{"bibtex":"@article{suttonAdaptingBiasGradient,\n title = {Adapting {{Bias}} by {{Gradient Descent}}: {{An Incremental Version}} of {{Delta-Bar-Delta}}},\n author = {Sutton, Richard S},\n file = {/home/kellen/Downloads/pdfs/storage/NJPU4WDD/Sutton - Adapting Bias by Gradient Descent An Incremental Version of Delta-Bar-Delta.pdf},\n langid = {english},\n abstract = {Appropriate bias is widely viewed as the key to efficient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system---the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate testbeds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation.}\n}"}},{"title":"Automatic Goal Generation for Reinforcement Learning Agents","uri":"florensaAutomaticGoalGeneration","taxon":"Reference","tags":[],"route":"/florensaAutomaticGoalGeneration/","metas":{"bibtex":"@article{florensaAutomaticGoalGeneration,\n title = {Automatic {{Goal Generation}} for {{Reinforcement Learning Agents}}},\n author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},\n file = {/home/kellen/Downloads/pdfs/storage/HZV43KRX/Florensa et al. - Automatic Goal Generation for Reinforcement Learni.pdf},\n langid = {english},\n abstract = {Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available. Videos and code available at: https://sites.google.com/view/ goalgeneration4rl.}\n}"}},{"title":"Benjamin Eysenbach","uri":"beneysenbach","taxon":"Person","tags":[],"route":"/beneysenbach/","metas":{"external":"https://ben-eysenbach.github.io/","institution":"Princeton University","position":"Assistant Professor"}},{"title":"Cal Newport","uri":"calnewport","taxon":"Person","tags":[],"route":"/calnewport/","metas":{"external":"https://calnewport.com/","institution":"University of Georgetown","position":"Professor"}},{"title":"Cambridge Computer Laboratory","uri":"camcl","taxon":"Department","tags":[],"route":"/camcl/","metas":{"external":"https://www.cst.cam.ac.uk/"}},{"title":"Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning","uri":"yaoConstraintConditionedPolicyOptimization","taxon":"Reference","tags":[],"route":"/yaoConstraintConditionedPolicyOptimization/","metas":{"bibtex":"@article{yaoConstraintConditionedPolicyOptimization,\n title = {Constraint-{{Conditioned Policy Optimization}} for {{Versatile Safe Reinforcement Learning}}},\n author = {Yao, Yihang and Liu, Zuxin and Cen, Zhepeng and Zhu, Jiacheng and Yu, Wenhao and Zhang, Tingnan and Zhao, Ding},\n file = {/home/kellen/Downloads/pdfs/storage/L9L6VL45/Yao et al. - Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning.pdf},\n langid = {english},\n abstract = {Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Constraint-Conditioned Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance, while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.}\n}"}},{"title":"Curiosity-Driven Exploration via Temporal Contrastive Learning","uri":"mohamedCuriosityDrivenExplorationTemporal","taxon":"Reference","tags":[],"route":"/mohamedCuriosityDrivenExplorationTemporal/","metas":{"bibtex":"@inproceedings{mohamedCuriosityDrivenExplorationTemporal,\n title = {Curiosity-{{Driven Exploration}} via {{Temporal Contrastive Learning}}},\n author = {Mohamed, Faisal and Ji, Catherine and Eysenbach, Benjamin and Berseth, Glen},\n urldate = {2025-07-07},\n booktitle = {Workshop on {{Reinforcement Learning Beyond Rewards}} @ {{Reinforcement Learning Conference}} 2025},\n file = {/home/kellen/Downloads/pdfs/storage/L8AUHEYW/pdf.pdf}\n}"}},{"title":"Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates","uri":"elsayedDeepReinforcementLearning","taxon":"Reference","tags":[],"route":"/elsayedDeepReinforcementLearning/","metas":{"bibtex":"@article{elsayedDeepReinforcementLearning,\n title = {Deep {{Reinforcement Learning Without Experience Replay}}, {{Target Networks}}, or {{Batch Updates}}},\n author = {Elsayed, Mohamed and Vasan, Gautham and Mahmood, A Rupam},\n file = {/home/kellen/Downloads/pdfs/storage/XDJZFXVT/Elsayed et al. - Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates.pdf},\n langid = {english},\n abstract = {Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resource-constrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.}\n}"}},{"title":"Deep Reinforcement Learning at the Edge of the Statistical Precipice","uri":"agarwalDeepReinforcementLearning","taxon":"Reference","tags":[],"route":"/agarwalDeepReinforcementLearning/","metas":{"bibtex":"@article{agarwalDeepReinforcementLearning,\n title = {Deep {{Reinforcement Learning}} at the {{Edge}} of the {{Statistical Precipice}}},\n author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G},\n file = {/home/kellen/Downloads/pdfs/storage/5AQZ3YSU/Agarwal et al. - Deep Reinforcement Learning at the Edge of the Statistical Precipice.pdf},\n langid = {english},\n abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few-run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable2, to prevent unreliable results from stagnating the field.}\n}"}},{"title":"Deterministic Policy Gradient Algorithms","uri":"silverDeterministicPolicyGradient","taxon":"Reference","tags":[],"route":"/silverDeterministicPolicyGradient/","metas":{"bibtex":"@article{silverDeterministicPolicyGradient,\n title = {Deterministic {{Policy Gradient Algorithms}}},\n author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},\n file = {/home/kellen/Downloads/pdfs/storage/HYF79JZL/Silver et al. - Deterministic Policy Gradient Algorithms.pdf},\n langid = {english},\n abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.}\n}"}},{"title":"ELEMENTS OF INFORMATION THEORY","uri":"coverELEMENTSINFORMATIONTHEORY","taxon":"Reference","tags":[],"route":"/coverELEMENTSINFORMATIONTHEORY/","metas":{"bibtex":"@article{coverELEMENTSINFORMATIONTHEORY,\n title = {{{ELEMENTS OF INFORMATION THEORY}}},\n author = {Cover, Thomas M and Thomas, Joy A},\n file = {/home/kellen/Downloads/pdfs/storage/C6AJGW5I/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf},\n langid = {english}\n}"}},{"title":"Efficient Skill Learning using Abstraction Selection","uri":"konidarisEfficientSkillLearning","taxon":"Reference","tags":[],"route":"/konidarisEfficientSkillLearning/","metas":{"bibtex":"@article{konidarisEfficientSkillLearning,\n title = {Efficient {{Skill Learning}} Using {{Abstraction Selection}}},\n author = {Konidaris, George and Barto, Andrew},\n file = {/home/kellen/Downloads/pdfs/storage/P465ZLK5/Konidaris and Barto - Efficient Skill Learning using Abstraction Selection.pdf},\n langid = {english},\n abstract = {We present an algorithm for selecting an appropriate abstraction when learning a new skill. We show empirically that it can consistently select an appropriate abstraction using very little sample data, and that it significantly improves skill learning performance in a reasonably large real-valued reinforcement learning domain.}\n}"}},{"title":"Entropy","uri":"0081","taxon":"Definition","tags":[],"route":"/0081/","metas":{}},{"title":"Explore to Generalize in Zero-Shot RL","uri":"zisselmanExploreGeneralizeZeroShot","taxon":"Reference","tags":[],"route":"/zisselmanExploreGeneralizeZeroShot/","metas":{"bibtex":"@article{zisselmanExploreGeneralizeZeroShot,\n title = {Explore to {{Generalize}} in {{Zero-Shot RL}}},\n author = {Zisselman, Ev and Lavie, Itai and Soudry, Daniel and Tamar, Aviv},\n file = {/home/kellen/Downloads/pdfs/storage/9D7NJ4H9/Zisselman et al. - Explore to Generalize in Zero-Shot RL.pdf},\n langid = {english},\n abstract = {We study zero-shot generalization in reinforcement learning---optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariancebased approaches. Our Explore to Generalize algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions, which generalize well and drive us to a novel part of the state space, where the ensemble may potentially agree again. We show that our approach is the state-of-the-art on tasks of the ProcGen challenge that have thus far eluded effective generalization, yielding a success rate of 83\\% on the Maze task and 74\\% on Heist with 200 training levels. ExpGen can also be combined with an invariance based approach to gain the best of both worlds, setting new state-of-the-art results on ProcGen. Code available at https://github.com/EvZissel/expgen.}\n}"}},{"title":"Fleeting Notes","uri":"007R","taxon":null,"tags":[],"route":"/007R/","metas":{}},{"title":"Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies","uri":"sohnHierarchicalReinforcementLearning","taxon":"Reference","tags":[],"route":"/sohnHierarchicalReinforcementLearning/","metas":{"bibtex":"@article{sohnHierarchicalReinforcementLearning,\n title = {Hierarchical {{Reinforcement Learning}} for {{Zero-shot Generalization}} with {{Subtask Dependencies}}},\n author = {Sohn, Sungryull and Oh, Junhyuk and Lee, Honglak},\n file = {/home/kellen/Downloads/pdfs/storage/6TK6ATBU/Sohn et al. - Hierarchical Reinforcement Learning for Zero-shot .pdf},\n langid = {english},\n abstract = {We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradientbased policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.}\n}"}},{"title":"High-Dimensional Probability","uri":"vershyninHighDimensionalProbability","taxon":"Reference","tags":[],"route":"/vershyninHighDimensionalProbability/","metas":{"bibtex":"@article{vershyninHighDimensionalProbability,\n title = {High-{{Dimensional Probability}}},\n author = {Vershynin, Roman},\n file = {/home/kellen/Downloads/pdfs/storage/DC3KWF38/Vershynin - High-Dimensional Probability.pdf},\n langid = {english}\n}"}},{"title":"Improving Intrinsic Exploration with Language Abstractions","uri":"muImprovingIntrinsicExploration","taxon":"Reference","tags":[],"route":"/muImprovingIntrinsicExploration/","metas":{"bibtex":"@article{muImprovingIntrinsicExploration,\n title = {Improving {{Intrinsic Exploration}} with {{Language Abstractions}}},\n author = {Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rockt{\\\"a}schel, Tim and Grefenstette, Edward},\n file = {/home/kellen/Downloads/pdfs/storage/BSEUWMD8/Mu et al. - Improving Intrinsic Exploration with Language Abst.pdf},\n langid = {english},\n abstract = {Reinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 47--85\\% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.}\n}"}},{"title":"Jon Sterling","uri":"jonmsterling","taxon":"Person","tags":[],"route":"/jonmsterling/","metas":{"external":"https://www.jonmsterling.com/index/","institution":"Cambridge Computer Laboratory","orcid":"0000-0002-0585-5564","position":"Associate Professor"}},{"title":"Kellen Kanarios","uri":"kellenkanarios","taxon":"Person","tags":[],"route":"/kellenkanarios/","metas":{"external":"https://kellenkanarios.github.io/","institution":"University of Michigan","position":"PhD Student"}},{"title":"Kellen Kanarios","uri":"0001","taxon":null,"tags":["home"],"route":"/0001/","metas":{"author":"false"}},{"title":"Lecture Notes on Game Theory","uri":"borgersLectureNotesGame","taxon":"Reference","tags":[],"route":"/borgersLectureNotesGame/","metas":{"bibtex":"@article{borgersLectureNotesGame,\n title = {Lecture {{Notes}} on {{Game Theory}}},\n author = {Borgers, Tilman},\n file = {/home/kellen/Downloads/pdfs/storage/CYJDCVHC/Borgers - Lecture Notes on Game Theory.pdf},\n langid = {english}\n}"}},{"title":"Lei Ying","uri":"leiying","taxon":"Person","tags":[],"route":"/leiying/","metas":{"external":"https://leiying.engin.umich.edu/","institution":"University of Michigan","position":"Professor"}},{"title":"Math 395 Notes","uri":"analysis","taxon":"Reference","tags":["texnote"],"route":"/analysis/","metas":{"paper":"https://kellenkanarios.com/bafkrmidkpdxz7j2mgqwwynwmv2ra5rzw45hem6sjgnhtjmgfxoakm2ys64.pdf"}},{"title":"Math 494 Notes","uri":"algebra","taxon":"Reference","tags":["texnote"],"route":"/algebra/","metas":{"paper":"https://kellenkanarios.com/bafkrmidjjjqif75agkmvsmz2bz627dgif5ark52gsvbyfbpzfthjwamyrq.pdf"}},{"title":"Math 597 Notes","uri":"measure","taxon":"Reference","tags":["texnote"],"route":"/measure/","metas":{"paper":"https://kellenkanarios.com/bafkrmiecxju5az4ffp7waq3lzh5nmftn2jdbb5xehtg5c63dmn2nm3d5qe.pdf"}},{"title":"Math 602 Notes","uri":"functional","taxon":"Reference","tags":["texnote"],"route":"/functional/","metas":{"paper":"https://kellenkanarios.com/bafkrmicgvuccv3l6jiuvek7r7rt4ojemnnwbd2qc7jztuxtesfghqjnrpm.pdf"}},{"title":"Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning","uri":"pitisMaximumEntropyGain","taxon":"Reference","tags":[],"route":"/pitisMaximumEntropyGain/","metas":{"bibtex":"@article{pitisMaximumEntropyGain,\n title = {Maximum {{Entropy Gain Exploration}} for {{Long Horizon Multi-goal Reinforcement Learning}}},\n author = {Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly},\n file = {/home/kellen/Downloads/pdfs/storage/XG7ZYYIG/Pitis et al. - Maximum Entropy Gain Exploration for Long Horizon .pdf},\n langid = {english}\n}"}},{"title":"Nan Jiang","uri":"nanjiang","taxon":"Person","tags":[],"route":"/nanjiang/","metas":{"external":"https://nanjiang.cs.illinois.edu/","institution":"University of Illinois Urbana-Champaign","position":"Professor"}},{"title":"Neural Ordinary Differential Equations","uri":"chenNeuralOrdinaryDifferential","taxon":"Reference","tags":[],"route":"/chenNeuralOrdinaryDifferential/","metas":{"bibtex":"@article{chenNeuralOrdinaryDifferential,\n title = {Neural {{Ordinary Differential Equations}}},\n author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},\n file = {/home/kellen/Downloads/pdfs/storage/VFWL2F6U/Chen et al. - Neural Ordinary Differential Equations.pdf},\n langid = {english},\n abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.}\n}"}},{"title":"Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics","uri":"gutmannNoiseContrastiveEstimationUnnormalized","taxon":"Reference","tags":[],"route":"/gutmannNoiseContrastiveEstimationUnnormalized/","metas":{"bibtex":"@article{gutmannNoiseContrastiveEstimationUnnormalized,\n title = {Noise-{{Contrastive Estimation}} of {{Unnormalized Statistical Models}}, with {{Applications}} to {{Natural Image Statistics}}},\n author = {Gutmann, Michael U and Gutmann, Michael and Hyvarinen, Aapo and Hyvarinen, Aapo},\n file = {/home/kellen/Downloads/pdfs/storage/7HQNNPRL/Gutmann et al. - Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image.pdf},\n langid = {english},\n abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.}\n}"}},{"title":"Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality","uri":"huangOccupancybasedPolicyGradient","taxon":"Reference","tags":[],"route":"/huangOccupancybasedPolicyGradient/","metas":{"bibtex":"@article{huangOccupancybasedPolicyGradient,\n title = {Occupancy-Based {{Policy Gradient}}: {{Estimation}}, {{Convergence}}, and {{Optimality}}},\n author = {Huang, Audrey and Jiang, Nan},\n file = {/home/kellen/Downloads/pdfs/storage/P7NI2N95/Huang and Jiang - Occupancy-based Policy Gradient Estimation, Convergence, and Optimality.pdf},\n langid = {english},\n abstract = {Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.}\n}"}},{"title":"On the Convergence of Stochastic Iterative Dynamic Programming Algorithms","uri":"jaakkolaConvergenceStochasticIterative","taxon":"Reference","tags":[],"route":"/jaakkolaConvergenceStochasticIterative/","metas":{"bibtex":"@article{jaakkolaConvergenceStochasticIterative,\n title = {On the {{Convergence}} of {{Stochastic Iterative Dynamic Programming Algorithms}}},\n author = {Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},\n file = {/home/kellen/Downloads/pdfs/storage/95ZWX95X/Jaakkola et al. - On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf},\n langid = {english},\n abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.}\n}"}},{"title":"Operating System Concepts","uri":"silberschatzOperatingSystemConcepts","taxon":"Reference","tags":[],"route":"/silberschatzOperatingSystemConcepts/","metas":{"bibtex":"@article{silberschatzOperatingSystemConcepts,\n title = {Operating {{System Concepts}}},\n author = {Silberschatz, Abraham and Galvin, Peter Baer and Gagne, Greg},\n file = {/home/kellen/Downloads/pdfs/storage/GI3ESXU4/Silberschatz et al. - Operating System Concepts.pdf}\n}"}},{"title":"Operating Systems: Three Easy Pieces","uri":"arpaci2018operating","taxon":"Reference","tags":[],"route":"/arpaci2018operating/","metas":{"external":"https://pages.cs.wisc.edu/~remzi/OSTEP/","bibtex":"@article{arpaci2018operating,\n  title={Operating systems: Three easy pieces},\n  author={Arpaci-Dusseau, Remzi H and Arpaci-Dusseau, Andrea C},\n  year={2018},\n  publisher={Arpaci-Dusseau Books, LLC Madison, WI, USA}\n}"}},{"title":"Princeton University","uri":"princeton","taxon":"Institute","tags":[],"route":"/princeton/","metas":{"external":"https://www.princeton.edu/"}},{"title":"Probabilistic Reinforcement Learning: Using Data to Define Desired Outcomes and Inferring How to Get There","uri":"eysenbachProbabilisticReinforcementLearning","taxon":"Reference","tags":[],"route":"/eysenbachProbabilisticReinforcementLearning/","metas":{"bibtex":"@article{eysenbachProbabilisticReinforcementLearning,\n title = {Probabilistic {{Reinforcement Learning}}: {{Using Data}} to {{Define Desired Outcomes}} and {{Inferring How}} to {{Get There}}},\n author = {Eysenbach, Benjamin},\n file = {/home/kellen/Downloads/pdfs/storage/PYCTKQX2/Eysenbach - Probabilistic Reinforcement Learning Using Data to Define Desired Outcomes and Inferring How to Get.pdf},\n langid = {english}\n}"}},{"title":"Qining Zhang","uri":"qiningzhang","taxon":"Person","tags":[],"route":"/qiningzhang/","metas":{"external":"https://thumichzqn.github.io/","institution":"University of Michigan","position":"PhD Student"}},{"title":"Random Features for Large-Scale Kernel Machines","uri":"rahimiRandomFeaturesLargeScale","taxon":"Reference","tags":[],"route":"/rahimiRandomFeaturesLargeScale/","metas":{"bibtex":"@article{rahimiRandomFeaturesLargeScale,\n title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},\n author = {Rahimi, Ali and Recht, Benjamin},\n file = {/home/kellen/Downloads/pdfs/storage/9NSANMTT/013a006f03dbc5392effeb8f18fda755-Paper.pdf},\n langid = {english},\n abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.}\n}"}},{"title":"Reinforcement Learning Conference","uri":"rlc","taxon":"Conference","tags":[],"route":"/rlc/","metas":{"external":"https://rl-conference.cc/"}},{"title":"Reinforcement Learning and Decision Making","uri":"rldm","taxon":"Conference","tags":[],"route":"/rldm/","metas":{"external":"https://rldm.org/"}},{"title":"Reinforcement Learning: Theory and Algorithms","uri":"agarwalReinforcementLearningTheory","taxon":"Reference","tags":[],"route":"/agarwalReinforcementLearningTheory/","metas":{"bibtex":"@article{agarwalReinforcementLearningTheory,\n title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},\n author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},\n file = {/home/kellen/Downloads/pdfs/storage/Z3QU86C2/Agarwal et al. - Reinforcement Learning Theory and Algorithms.pdf},\n langid = {english}\n}"}},{"title":"Richard Sutton","uri":"richardsutton","taxon":"Person","tags":[],"route":"/richardsutton/","metas":{"external":"http://incompleteideas.net","institution":"University of Alberta","position":"Professor"}},{"title":"STOCHASTIC APPROXIMATION : A DYNAMICAL SYSTEMS VIEWPOINT","uri":"borkarSTOCHASTICAPPROXIMATIONDYNAMICAL","taxon":"Reference","tags":[],"route":"/borkarSTOCHASTICAPPROXIMATIONDYNAMICAL/","metas":{"bibtex":"@article{borkarSTOCHASTICAPPROXIMATIONDYNAMICAL,\n title = {{{STOCHASTIC APPROXIMATION}} : {{A DYNAMICAL SYSTEMS VIEWPOINT}}},\n author = {Borkar, Vivek S},\n file = {/home/kellen/Downloads/pdfs/storage/E8YY7W7Z/Borkar - STOCHASTIC APPROXIMATION  A DYNAMICAL SYSTEMS VIEWPOINT.pdf},\n langid = {english}\n}"}},{"title":"Safe Reinforcement Learning by Imagining the Near Future","uri":"thomasSafeReinforcementLearning","taxon":"Reference","tags":[],"route":"/thomasSafeReinforcementLearning/","metas":{"bibtex":"@article{thomasSafeReinforcementLearning,\n title = {Safe {{Reinforcement Learning}} by {{Imagining}} the {{Near Future}}},\n author = {Thomas, Garrett and Luo, Yuping and Ma, Tengyu},\n file = {/home/kellen/Downloads/pdfs/storage/U6CCSU6W/Thomas et al. - Safe Reinforcement Learning by Imagining the Near Future.pdf},\n langid = {english},\n abstract = {Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.}\n}"}},{"title":"Sanjay Shakkottai","uri":"sanjayshakkottai","taxon":"Person","tags":[],"route":"/sanjayshakkottai/","metas":{"external":"https://www.ece.utexas.edu/people/faculty/sanjay-shakkottai","institution":"University of Texas","position":"Professor"}},{"title":"Stanford University","uri":"stanford","taxon":"Institute","tags":[],"route":"/stanford/","metas":{"external":"https://www.stanford.edu/"}},{"title":"State Entropy Maximization with Random Encoders for Efficient Exploration","uri":"seoStateEntropyMaximization","taxon":"Reference","tags":[],"route":"/seoStateEntropyMaximization/","metas":{"bibtex":"@article{seoStateEntropyMaximization,\n title = {State {{Entropy Maximization}} with {{Random Encoders}} for {{Efficient Exploration}}},\n author = {Seo, Younggyo and Chen, Lili and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin},\n file = {/home/kellen/Downloads/pdfs/storage/WN9MVGSS/Seo et al. - State Entropy Maximization with Random Encoders fo.pdf},\n langid = {english},\n abstract = {Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks.}\n}"}},{"title":"Stochastic Approximation Algorithms and Applications","uri":"StochasticApproximationAlgorithms","taxon":"Reference","tags":[],"route":"/StochasticApproximationAlgorithms/","metas":{"bibtex":"@book{StochasticApproximationAlgorithms,\n title = {Stochastic {{Approximation Algorithms}} and {{Applications}}},\n file = {/home/kellen/Downloads/pdfs/storage/6IGQEBVP/Stochastic Approximation Algorithms and Applications.pdf}\n}"}},{"title":"Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning","uri":"hoangSuccessorFeatureLandmarks","taxon":"Reference","tags":[],"route":"/hoangSuccessorFeatureLandmarks/","metas":{"bibtex":"@article{hoangSuccessorFeatureLandmarks,\n title = {Successor {{Feature Landmarks}} for {{Long-Horizon Goal-Conditioned Reinforcement Learning}}},\n author = {Hoang, Christopher and Sohn, Sungryull and Choi, Jongwook and Carvalho, Wilka and Lee, Honglak},\n file = {/home/kellen/Downloads/pdfs/storage/A5JJMGPW/Hoang et al. - Successor Feature Landmarks for Long-Horizon Goal-.pdf},\n langid = {english},\n abstract = {Operating in the real-world often requires agents to learn about a complex environment and apply this understanding to achieve a breadth of goals. This problem, known as goal-conditioned reinforcement learning (GCRL), becomes especially challenging for long-horizon goals. Current methods have tackled this problem by augmenting goal-conditioned policies with graph-based planning algorithms. However, they struggle to scale to large, high-dimensional state spaces and assume access to exploration mechanisms for efficiently collecting training data. In this work, we introduce Successor Feature Landmarks (SFL), a framework for exploring large, high-dimensional environments so as to obtain a policy that is proficient for any goal. SFL leverages the ability of successor features (SF) to capture transition dynamics, using it to drive exploration by estimating state-novelty and to enable high-level planning by abstracting the state-space as a non-parametric landmarkbased graph. We further exploit SF to directly compute a goal-conditioned policy for inter-landmark traversal, which we use to execute plans to ``frontier'' landmarks at the edge of the explored state space. We show in our experiments on MiniGrid and ViZDoom that SFL enables efficient exploration of large, high-dimensional state spaces and outperforms state-of-the-art baselines on long-horizon GCRL tasks1.}\n}"}},{"title":"Toward a New Approach to Model-based Reinforcement Learning","uri":"suttonNewApproachModelbased","taxon":"Reference","tags":[],"route":"/suttonNewApproachModelbased/","metas":{"bibtex":"@article{suttonNewApproachModelbased,\n title = {Toward a {{New Approach}} to {{Model-based Reinforcement Learning}}},\n author = {Sutton, Richard S},\n file = {/home/kellen/Downloads/pdfs/storage/VJQU89XE/Sutton - Toward a New Approach to Model-based Reinforcement Learning.pdf},\n langid = {english}\n}"}},{"title":"University of Georgetown","uri":"georgetown","taxon":"Institute","tags":[],"route":"/georgetown/","metas":{"external":"https://www.georgetown.edu/"}},{"title":"University of Michigan","uri":"umich","taxon":"Institute","tags":[],"route":"/umich/","metas":{"external":"https://umich.edu/"}},{"title":"University of Texas","uri":"utaustin","taxon":"Institute","tags":[],"route":"/utaustin/","metas":{"external":"https://www.ece.utexas.edu/"}},{"title":"Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning","uri":"choiVariationalEmpowermentRepresentation","taxon":"Reference","tags":[],"route":"/choiVariationalEmpowermentRepresentation/","metas":{"bibtex":"@article{choiVariationalEmpowermentRepresentation,\n title = {Variational {{Empowerment}} as {{Representation Learning}} for {{Goal-Based Reinforcement Learning}}},\n author = {Choi, Jongwook and Sharma, Archit and Lee, Honglak and Levine, Sergey and Gu, Shixiang Shane},\n file = {/home/kellen/Downloads/pdfs/storage/GSN79TEM/Choi et al. - Variational Empowerment as Representation Learning.pdf},\n langid = {english},\n abstract = {Learning to reach goal states and learning diverse skills through mutual information (MI) maximization have been proposed as principled frameworks for self-supervised reinforcement learning, allowing agents to acquire broadly applicable multitask policies with minimal reward engineering. Starting from a simple observation that the standard goal-conditioned RL (GCRL) is encapsulated by the optimization objective of variational empowerment, we discuss how GCRL and MIbased RL can be generalized into a single family of methods, which we name variational GCRL (VGCRL), interpreting variational MI maximization, or variational empowerment, as representation learning methods that acquire functionallyaware state representations for goal reaching. This novel perspective allows us to: (1) derive simple but unexplored variants of GCRL to study how adding small representation capacity can already expand its capabilities; (2) investigate how discriminator function capacity and smoothness determine the quality of discovered skills, or latent goals, through modifying latent dimensionality and applying spectral normalization; (3) adapt techniques such as hindsight experience replay (HER) from GCRL to MI-based RL; and lastly, (4) propose a novel evaluation metric, named latent goal reaching (LGR), for comparing empowerment algorithms with different choices of latent dimensionality and discriminator parameterization. Through principled mathematical derivations and careful experimental studies, our work lays a novel foundation from which to evaluate, analyze, and develop representation learning techniques in goal-based RL.}\n}"}},{"title":"Weekly Review 2025-W26","uri":"2025-W26","taxon":null,"tags":[],"route":"/2025-W26/","metas":{}},{"title":"basic macros","uri":"base-macros","taxon":null,"tags":[],"route":"/base-macros/","metas":{}},{"title":"Kellen Kanarios › About this website","uri":"about","taxon":null,"tags":[],"route":"/about/","metas":{}},{"title":"Kellen Kanarios › News","uri":"rn","taxon":null,"tags":[],"route":"/rn/","metas":{}},{"title":"Kellen Kanarios › Research","uri":"pub","taxon":null,"tags":[],"route":"/pub/","metas":{}}]