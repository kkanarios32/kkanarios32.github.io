<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Mohamed Elsayed</fr:author>
      <fr:author>A. Rupam Mahmood</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2023</fr:year>
      <fr:month>4</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/elsayedUtilitybasedPerturbedGradient2023/</fr:uri>
    <fr:display-uri>elsayedUtilitybasedPerturbedGradient2023</fr:display-uri>
    <fr:route>/elsayedUtilitybasedPerturbedGradient2023/</fr:route>
    <fr:title text="Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning">Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2302.03281</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2302.03281</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{elsayedUtilitybasedPerturbedGradient2023,
 title = {Utility-Based {{Perturbed Gradient Descent}}: {{An Optimizer}} for {{Continual Learning}}},
 author = {Elsayed, Mohamed and Mahmood, A. Rupam},
 year = {2023},
 doi = {10.48550/arXiv.2302.03281},
 urldate = {2025-06-27},
 number = {arXiv:2302.03281},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/2RCAQS2F/Elsayed and Mahmood - 2023 - Utility-based Perturbed Gradient Descent An Optim.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.},
 primaryclass = {cs},
 eprint = {2302.03281},
 month = {April},
 shorttitle = {Utility-Based {{Perturbed Gradient Descent}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
