<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Seohong Park</fr:author>
      <fr:author>Tobias Kreiman</fr:author>
      <fr:author>Sergey Levine</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>5</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/parkFoundationPoliciesHilbert2024/</fr:uri>
    <fr:display-uri>parkFoundationPoliciesHilbert2024</fr:display-uri>
    <fr:route>/parkFoundationPoliciesHilbert2024/</fr:route>
    <fr:title text="Foundation Policies with Hilbert Representations">Foundation Policies with Hilbert Representations</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2402.15567</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2402.15567</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{parkFoundationPoliciesHilbert2024,
 title = {Foundation {{Policies}} with {{Hilbert Representations}}},
 author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},
 year = {2024},
 doi = {10.48550/arXiv.2402.15567},
 urldate = {2024-09-18},
 number = {arXiv:2402.15567},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/WEHZ63LJ/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Downloads/pdfs/storage/YNKYPBF3/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Downloads/pdfs/storage/5H2B6U6Y/2402.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
 archiveprefix = {arXiv},
 abstract = {Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy "prompting" schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.},
 primaryclass = {cs},
 eprint = {2402.15567},
 month = {May}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
