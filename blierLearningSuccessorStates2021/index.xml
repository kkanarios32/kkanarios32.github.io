<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/L%C3%A9onard%20Blier/" type="external">LÃ©onard Blier</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Corentin%20Tallec/" type="external">Corentin Tallec</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Yann%20Ollivier/" type="external">Yann Ollivier</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2021</fr:year>
      <fr:month>1</fr:month>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/blierLearningSuccessorStates2021/</fr:uri>
    <fr:display-uri>blierLearningSuccessorStates2021</fr:display-uri>
    <fr:route>/blierLearningSuccessorStates2021/</fr:route>
    <fr:title text="Learning Successor States and Goal-Dependent Values: A Mathematical Viewpoint">Learning Successor States and Goal-Dependent Values: A Mathematical Viewpoint</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2101.07123</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2101.07123</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{blierLearningSuccessorStates2021,
 title = {Learning {{Successor States}} and {{Goal-Dependent Values}}: {{A Mathematical Viewpoint}}},
 author = {Blier, L{\'e}onard and Tallec, Corentin and Ollivier, Yann},
 year = {2021},
 doi = {10.48550/arXiv.2101.07123},
 urldate = {2025-02-27},
 number = {arXiv:2101.07123},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/LUN44LMX/Blier et al. - 2021 - Learning Successor States and Goal-Dependent Values A Mathematical Viewpoint.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {In reinforcement learning, temporal difference-based algorithms can be sample-inefficient: for instance, with sparse rewards, no learning occurs until a reward is observed. This can be remedied by learning richer objects, such as a model of the environment, or successor states. Successor states model the expected future state occupancy from any given state [Dayan, 1993, Kulkarni et al., 2016], and summarize all paths in the environment for a given policy. They are related to goal-dependent value functions, which learn how to reach arbitrary states.},
 primaryclass = {cs},
 eprint = {2101.07123},
 month = {January},
 shorttitle = {Learning {{Successor States}} and {{Goal-Dependent Values}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
