<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Jie Ren</fr:author>
      <fr:author>Samyam Rajbhandari</fr:author>
      <fr:author>Reza Yazdani Aminabadi</fr:author>
      <fr:author>Olatunji Ruwase</fr:author>
      <fr:author>Shuangyan Yang</fr:author>
      <fr:author>Minjia Zhang</fr:author>
      <fr:author>Dong Li</fr:author>
      <fr:author>Yuxiong He</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2021</fr:year>
      <fr:month>1</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/renZeROOffloadDemocratizingBillionScale2021/</fr:uri>
    <fr:display-uri>renZeROOffloadDemocratizingBillionScale2021</fr:display-uri>
    <fr:route>/renZeROOffloadDemocratizingBillionScale2021/</fr:route>
    <fr:title text="ZeRO-Offload: Democratizing Billion-Scale Model Training">ZeRO-Offload: Democratizing Billion-Scale Model Training</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2101.06840</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2101.06840</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{renZeROOffloadDemocratizingBillionScale2021,
 title = {{{ZeRO-Offload}}: {{Democratizing Billion-Scale Model Training}}},
 author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
 year = {2021},
 doi = {10.48550/arXiv.2101.06840},
 urldate = {2025-01-30},
 number = {arXiv:2101.06840},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/EQNAIMY3/Ren et al. - 2021 - ZeRO-Offload Democratizing Billion-Scale Model Training.pdf},
 keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency.},
 primaryclass = {cs},
 eprint = {2101.06840},
 month = {January},
 shorttitle = {{{ZeRO-Offload}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
