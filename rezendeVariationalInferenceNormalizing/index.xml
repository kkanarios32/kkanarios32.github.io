<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Danilo Jimenez Rezende</fr:author>
      <fr:author>Shakir Mohamed</fr:author>
    </fr:authors>
    <fr:uri>https://kellenkanarios.com/rezendeVariationalInferenceNormalizing/</fr:uri>
    <fr:display-uri>rezendeVariationalInferenceNormalizing</fr:display-uri>
    <fr:route>/rezendeVariationalInferenceNormalizing/</fr:route>
    <fr:title text="Variational Inference with Normalizing Flows">Variational Inference with Normalizing Flows</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="bibtex"><![CDATA[@article{rezendeVariationalInferenceNormalizing,
 title = {Variational {{Inference}} with {{Normalizing Flows}}},
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
 file = {/home/kellen/Downloads/pdfs/storage/IRPF789A/Rezende and Mohamed - Variational Inference with Normalizing Flows.pdf},
 langid = {english},
 abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>15</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/B9J9/</fr:uri>
            <fr:display-uri>B9J9</fr:display-uri>
            <fr:route>/B9J9/</fr:route>
            <fr:title text="Normalizing Flows">Normalizing Flows</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>Brief notes on <fr:link href="/rezendeVariationalInferenceNormalizing/" title="Variational Inference with Normalizing Flows" uri="https://kellenkanarios.com/rezendeVariationalInferenceNormalizing/" display-uri="rezendeVariationalInferenceNormalizing" type="local">Variational Inference with Normalizing Flows</fr:link> to motivate <fr:link href="https://kellenkanarios.com/008A/" type="external">flow matching</fr:link>.</html:p>
            <html:p>We are concerned with the problem of <html:em>density estimation</html:em>. Given a dataset <fr:tex display="inline"><![CDATA[\mathcal {X}]]></fr:tex>, consisting of samples <fr:tex display="inline"><![CDATA[\mathbf {x}_i \sim  p(\mathbf {x})]]></fr:tex>, we want to learn <fr:tex display="inline"><![CDATA[p(\mathbf {x})]]></fr:tex>. One popular way to do this is via <html:em>variational inference</html:em>. As a brief recap, in variational inference, we assume that there is actually a joint distribution <fr:tex display="inline"><![CDATA[p(\mathbf {x}, \mathbf {z})]]></fr:tex>, where <fr:tex display="inline"><![CDATA[\mathbf {z}]]></fr:tex> is a <fr:link href="/0083/" title="Latent Variable" uri="https://kellenkanarios.com/0083/" display-uri="0083" type="local">latent variable</fr:link>. We then find <fr:tex display="inline"><![CDATA[p(\mathbf {x})]]></fr:tex> by maximizing the log-likelihood <fr:tex display="inline"><![CDATA[\log  \int  p(\mathbf {x} \mid  \mathbf {z}) \mathrm {d}\mathbf {z}]]></fr:tex></html:p>
            <html:p>
 For an example of this, see my post on <fr:link href="/0088/" title="Expectation Maximization" uri="https://kellenkanarios.com/0088/" display-uri="0088" type="local">expectation maximization</fr:link>. In that post, we utilize the <fr:link href="/0082/" title="Evidence Lower Bound" uri="https://kellenkanarios.com/0082/" display-uri="0082" type="local">ELBO</fr:link> to learn a latent variable model <fr:tex display="inline"><![CDATA[q_{\phi }(\mathbf {z} | \mathbf {x})]]></fr:tex>. As discussed in my post on <fr:link href="/0082/" title="Evidence Lower Bound" uri="https://kellenkanarios.com/0082/" display-uri="0082" type="local">ELBO</fr:link>, in order to learn <fr:tex display="inline"><![CDATA[p(\mathbf {x})]]></fr:tex>, we must have that <fr:tex display="inline"><![CDATA[p(\mathbf {z} | \mathbf {x})]]></fr:tex> is in the family <fr:tex display="inline"><![CDATA[q_{\phi }]]></fr:tex>. However, <fr:tex display="inline"><![CDATA[q_
 {\phi }]]></fr:tex> is usually a Gaussian parametrized by <fr:tex display="inline"><![CDATA[\phi ]]></fr:tex>, so that we can easily learn <fr:tex display="inline"><![CDATA[\phi ]]></fr:tex>. In this case, it is very unlikely that <fr:tex display="inline"><![CDATA[p(\mathbf {z} \mid  x)]]></fr:tex> lies in the family of <fr:tex display="inline"><![CDATA[q_{\phi }]]></fr:tex> and therefore we have no hope of learning the true <fr:tex display="inline"><![CDATA[p(\mathbf {x})]]></fr:tex>.
</html:p>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
