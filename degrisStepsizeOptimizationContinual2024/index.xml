<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Thomas%20Degris/" type="external">Thomas Degris</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Khurram%20Javed/" type="external">Khurram Javed</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Arsalan%20Sharifnassab/" type="external">Arsalan Sharifnassab</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Yuxin%20Liu/" type="external">Yuxin Liu</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Richard%20Sutton/" type="external">Richard Sutton</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>1</fr:month>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/degrisStepsizeOptimizationContinual2024/</fr:uri>
    <fr:display-uri>degrisStepsizeOptimizationContinual2024</fr:display-uri>
    <fr:route>/degrisStepsizeOptimizationContinual2024/</fr:route>
    <fr:title text="Step-size Optimization for Continual Learning">Step-size Optimization for Continual Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/2401.17401v1</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{degrisStepsizeOptimizationContinual2024,
 title = {Step-Size {{Optimization}} for {{Continual Learning}}},
 author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu, Yuxin and Sutton, Richard},
 year = {2024},
 urldate = {2024-09-06},
 howpublished = {https://arxiv.org/abs/2401.17401v1},
 journal = {arXiv.org},
 file = {/home/kellen/Zotero/storage/YSGENB8H/Degris et al. - 2024 - Step-size Optimization for Continual Learning.pdf},
 langid = {english},
 abstract = {In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.},
 month = {January}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
