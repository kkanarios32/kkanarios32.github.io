<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Hongyao Tang</fr:author>
      <fr:author>Johan Obando-Ceron</fr:author>
      <fr:author>Pablo Samuel Castro</fr:author>
      <fr:author>Aaron Courville</fr:author>
      <fr:author>Glen Berseth</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>5</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/tangMitigatingPlasticityLoss2025/</fr:uri>
    <fr:display-uri>tangMitigatingPlasticityLoss2025</fr:display-uri>
    <fr:route>/tangMitigatingPlasticityLoss2025/</fr:route>
    <fr:title text="Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn">Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2506.00592</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2506.00592</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{tangMitigatingPlasticityLoss2025,
 title = {Mitigating {{Plasticity Loss}} in {{Continual Reinforcement Learning}} by {{Reducing Churn}}},
 author = {Tang, Hongyao and {Obando-Ceron}, Johan and Castro, Pablo Samuel and Courville, Aaron and Berseth, Glen},
 year = {2025},
 doi = {10.48550/arXiv.2506.00592},
 urldate = {2025-06-27},
 number = {arXiv:2506.00592},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/PWAQ3ZB5/Tang et al. - 2025 - Mitigating Plasticity Loss in Continual Reinforcem.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.},
 primaryclass = {cs},
 eprint = {2506.00592},
 month = {May}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
