<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Harrison%20Lee/" type="external">Harrison Lee</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Samrat%20Phatale/" type="external">Samrat Phatale</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Hassan%20Mansoor/" type="external">Hassan Mansoor</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Thomas%20Mesnard/" type="external">Thomas Mesnard</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Johan%20Ferret/" type="external">Johan Ferret</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Kellie%20Lu/" type="external">Kellie Lu</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Colton%20Bishop/" type="external">Colton Bishop</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Ethan%20Hall/" type="external">Ethan Hall</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Victor%20Carbune/" type="external">Victor Carbune</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Abhinav%20Rastogi/" type="external">Abhinav Rastogi</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Sushant%20Prakash/" type="external">Sushant Prakash</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>9</fr:month>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/leeRLAIFVsRLHF2024/</fr:uri>
    <fr:display-uri>leeRLAIFVsRLHF2024</fr:display-uri>
    <fr:route>/leeRLAIFVsRLHF2024/</fr:route>
    <fr:title text="RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/2309.00267</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{leeRLAIFVsRLHF2024,
 title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},
 author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
 year = {2024},
 urldate = {2024-09-11},
 number = {arXiv:2309.00267},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/IWPVFZCM/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards ``self-improvement'' by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
 primaryclass = {cs},
 eprint = {2309.00267},
 month = {September},
 shorttitle = {{{RLAIF}} vs. {{RLHF}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
