<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Tianqi%20Chen/" type="external">Tianqi Chen</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Thierry%20Moreau/" type="external">Thierry Moreau</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Ziheng%20Jiang/" type="external">Ziheng Jiang</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Lianmin%20Zheng/" type="external">Lianmin Zheng</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Eddie%20Yan/" type="external">Eddie Yan</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Meghan%20Cowan/" type="external">Meghan Cowan</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Haichen%20Shen/" type="external">Haichen Shen</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Leyuan%20Wang/" type="external">Leyuan Wang</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Yuwei%20Hu/" type="external">Yuwei Hu</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Luis%20Ceze/" type="external">Luis Ceze</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Carlos%20Guestrin/" type="external">Carlos Guestrin</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Arvind%20Krishnamurthy/" type="external">Arvind Krishnamurthy</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2018</fr:year>
      <fr:month>10</fr:month>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/chenTVMAutomatedEndtoEnd2018/</fr:uri>
    <fr:display-uri>chenTVMAutomatedEndtoEnd2018</fr:display-uri>
    <fr:route>/chenTVMAutomatedEndtoEnd2018/</fr:route>
    <fr:title text="TVM: An Automated End-to-End Optimizing Compiler for Deep Learning">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/1802.04799</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{chenTVMAutomatedEndtoEnd2018,
 title = {{{TVM}}: {{An Automated End-to-End Optimizing Compiler}} for {{Deep Learning}}},
 author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
 year = {2018},
 urldate = {2024-11-07},
 number = {arXiv:1802.04799},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/IX3XN5F4/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler for Deep Learning.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
 archiveprefix = {arXiv},
 abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
 primaryclass = {cs},
 eprint = {1802.04799},
 month = {October},
 shorttitle = {{{TVM}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
