<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Junsu Kim</fr:author>
      <fr:author>Seohong Park</fr:author>
      <fr:author>Sergey Levine</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>8</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/kimUnsupervisedtoOnlineReinforcementLearning2024/</fr:uri>
    <fr:display-uri>kimUnsupervisedtoOnlineReinforcementLearning2024</fr:display-uri>
    <fr:route>/kimUnsupervisedtoOnlineReinforcementLearning2024/</fr:route>
    <fr:title text="Unsupervised-to-Online Reinforcement Learning">Unsupervised-to-Online Reinforcement Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/2408.14785v1</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{kimUnsupervisedtoOnlineReinforcementLearning2024,
 title = {Unsupervised-to-{{Online Reinforcement Learning}}},
 author = {Kim, Junsu and Park, Seohong and Levine, Sergey},
 year = {2024},
 urldate = {2024-09-06},
 howpublished = {https://arxiv.org/abs/2408.14785v1},
 journal = {arXiv.org},
 file = {/home/kellen/Downloads/pdfs/storage/FRV2CM6E/Kim et al. - 2024 - Unsupervised-to-Online Reinforcement Learning.pdf},
 langid = {english},
 abstract = {Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL, has been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL), which replaces domain-specific supervised offline RL with unsupervised offline RL, as a better alternative to offline-to-online RL. U2O RL not only enables reusing a single pre-trained model for multiple downstream tasks, but also learns better representations, which often result in even better performance and stability than supervised offline-to-online RL. To instantiate U2O RL in practice, we propose a general recipe for U2O RL to bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning. Throughout our experiments in nine state-based and pixel-based environments, we empirically demonstrate that U2O RL achieves strong performance that matches or even outperforms previous offline-to-online RL approaches, while being able to reuse a single pre-trained model for a number of different downstream tasks.},
 month = {August}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
