<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Moses Charikar</fr:author>
      <fr:author>Chirag Pabbaraju</fr:author>
      <fr:author>Kirankumar Shiragur</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>10</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/charikarQuantifyingGainWeaktoStrong2024/</fr:uri>
    <fr:display-uri>charikarQuantifyingGainWeaktoStrong2024</fr:display-uri>
    <fr:route>/charikarQuantifyingGainWeaktoStrong2024/</fr:route>
    <fr:title text="Quantifying the Gain in Weak-to-Strong Generalization">Quantifying the Gain in Weak-to-Strong Generalization</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="external">https://arxiv.org/abs/2405.15116</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{charikarQuantifyingGainWeaktoStrong2024,
 title = {Quantifying the {{Gain}} in {{Weak-to-Strong Generalization}}},
 author = {Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},
 year = {2024},
 urldate = {2024-10-24},
 number = {arXiv:2405.15116},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/Y3LYJ8Q8/Charikar et al. - 2024 - Quantifying the Gain in Weak-to-Strong Generalization.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. [BIK+23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts---a phenomenon they term weak-to-strong generalization.},
 primaryclass = {cs},
 eprint = {2405.15116},
 month = {October}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
