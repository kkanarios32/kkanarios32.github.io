<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <author>
    <name>Kellen Kanarios</name>
    <uri>https://kkanarios32.github.io/kellenkanarios/</uri>
  </author>
  <updated>2025-03-31</updated>
  <title>Blog</title>
  <id>https://kkanarios32.github.io/0002/</id>
  <link rel="alternate" href="https://kkanarios32.github.io/0002/" />
  <link rel="self" href="https://kkanarios32.github.io/0002/atom.xml" />
  <entry>
    <title>Test Time Compute in Reinforcement Learning</title>
    <published>2025-03-31T00:00:00Z</published>
    <updated>2025-03-31T00:00:00Z</updated>
    <link rel="alternate" type="text/html" href="https://kkanarios32.github.io/006Q/" />
    <id>https://kkanarios32.github.io/006Q/</id>
    <content type="xhtml">
      <div xmlns="http://www.w3.org/1999/xhtml">
<p><em>One thing that should be learned [...] is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.</em> - <a href="https://kkanarios32.github.io/richardsutton/">Richard Sutton</a></p>
  
    <section><header><h2>What is Planning?</h2></header>
  <p>
    In <a href="https://kkanarios32.github.io/suttonReinforcementLearningIntroduction2018/">Reinforcement learning: An introduction</a>, they define planning as <em>a computational process that takes a model as input and outputs a policy</em>. Like everything Sutton writes, I agree with it for the most part. I have struggled with this question for a long time. In the RL community, you often here this vague term "planning" thrown around in all sorts of different situations. I think the key distinction between traditional methods is shown when looking at these methods directly. The only definition I have come up with that leaves me somewhat satisfied is "policy improvement in the absence of learning.
  </p>
  <p>
    As an example, in <a href="https://kkanarios32.github.io/suttonReinforcementLearningIntroduction2018/">Q-learning</a> you interact with the environment and learn via updating your Q-function. You then immediately recover an action via <code>a \in  \arg \max _{a} Q(s, a)</code> This does not require that you input a model rather you only input the current state and receive the corresponding action. Learning can be seen as distilling everything needed into the model, where planning allows the model to see and plan based on the consequences of potential actions using a model of the environment.
  </p>
</section>
  
<section><header><h2>Alpha-Zero</h2></header><p>The major break through of planning was in <a href="https://kkanarios32.github.io/schrittwieserMasteringAtariGo2020/">alpha-zero</a>, or more accurately at the time was just alpha-go. However, the core idea remained the same. The idea is to learn some notion of a "good" state in a game like chess or go and then leverage this information in combination with some planning.</p></section><script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="mcts" theme="boxy-light" crossorigin="anonymous" async="" />      </div>
    </content>
  </entry>
  <entry>
    <title>Rebuilding My (Neo)Vim Config From Scratch</title>
    <published>2025-03-04T00:00:00Z</published>
    <updated>2025-03-04T00:00:00Z</updated>
    <link rel="alternate" type="text/html" href="https://kkanarios32.github.io/005F/" />
    <id>https://kkanarios32.github.io/005F/</id>
    <content type="xhtml">
      <div xmlns="http://www.w3.org/1999/xhtml">
<p>I have been using LazyVim for some time now, but I have now run into issues multiple times where understanding how LazyVim is doing something is far more difficult than if I had written my own setup. I allocated one day for this adventure and really just wanted to make sure I had support for <code>\TeX </code>, python, forester, and C/C++. Due to my (self-imposed) time constraint, I do not have the associated resources linked for each of the things discussed below. At some point, I hope to come back and more thoroughly cover each of the components.
</p>
  
    <section><header><h2>Sane Defaults</h2></header>
To my surprise, a lot of the features that I had come to take for granted were actually options set up internally by Lazyvim. For example, I was shocked with 8 space indents!! and I could not even copy from one terminal instance to another... Due to this, I went and found all of the options I liked from Lazyvim and added them to my new configuration in <code>configs/options.lua</code>.
</section>
  

  
    <section><header><h2>Installing a Plugin Manager</h2></header>
For this, we will be using the defacto standard <code>lazy.nvim</code>. This is actually straightforward and kind of "just works". Just follow the installation guide in their documentation.
</section>
  

  
    <section><header><h2>Setting up Auto Complete</h2></header>
This is one of the main motivations for me making the switch. It seems <code>nvim-cmp</code> has finally been replaced with a new <code>blink.cmp</code>, so that is what we will be using.


  
    <section><header><h3>Language Server Protocol</h3></header>
It turns out there is a lot that goes into getting LSP setup correctly.
<ol><li>First we must actually install the language servers. To do this the easiest way, we use the <code>mason.nvim</code> and <code>mason.nvim-lspconfig</code> plugins. At some point, I might actually figure out how to set up lsp myself without lspconfig but that point is not now.
</li>
<li>Through <code>nvim-lspconfig</code>, we can set up each of the servers we want to have LSP support. I just set up clangd, pyright, and texlab.</li></ol>
This was a bit ridiculous. The first of many challenges was around import resolution in python. To remedy this, I needed to write a function to find the virtual environment directory and then set the <code>pythonPath</code> to the venv python binary. Previously, I think I was just using pylsp and installing it as a pip package to each python venv. I much prefer the new way, and I think pyright is overall a much better lsp.
</section>
  


  
    <section><header><h3>Forester Completion</h3></header>
  Another necessary completion source for me is the one provided by <code>forester.nvim</code>. Similar to vimtex, the reference completion support is VERY useful. Obviously, I need completion when I am writing this blog!!! This was a little more involved. The first difficulty was that the completion source provided by the <code>forester.nvim</code> plugin was for <code>nvim-cmp</code>. It turns out this is a prevalent enough problem that the author of <code>blink.cmp</code> wrote an additional plugin <code>blink.compat</code> to allow for <code>nvim-cmp</code> completion sources. While this sounds all fine and good, <code>nvim-compat</code> expects plugins that return the completion source themselves, whereas in <code>forester.nvim</code> the completion source is just one submodule of a more feature-rich plugin. To get around this, I needed to look into the <code>blink.compat</code> code and find how they are registering the sources and just do it myself.
</section>
  

</section>
  

  
    <section><header><h2>Snippets</h2></header>
  <p>
  Going all the way back to the <a href="https://castel.dev/post/lecture-notes-1/">Gilles Castel blog post</a>, I have always been partial to snippets that auto-expand. I had them set up prior to Lazyvim but with Lazyvim I had resigned to using friendly-snippets with native nvim snippets. Since I was already redoing everything, this time around I decided not to compromise. Once upon a time (right when it came out I think?) I tried out Luasnips, but it seems that they now have far more extensive features. They are also natively supported by <code>blink.cmp</code>!! It feels necessary that I plug the <a href="https://github.com/iurimateus/luasnip-latex-snippets.nvim">awesome repo</a> that ports the original Ultisnips snippets to Luasnip. With this, I was able to easily add my own forester snippets!!!
  </p>
  <p>
  A fun little thing that I had been hoping to do for awhile and is finally now possible - I can load latex snippets when inside math environments in forester!!!! To do this, I looked into the forester treesitter grammar and found the corresponding nodetypes for math envs. It was then straightforward to detect whether we were in a math env and to load the associated latex snippets.
  </p>
</section>
  

  
    <section><header><h2>Anki</h2></header>
  <p>
  There is a very cool add-on to anki called <a href="https://git.sr.ht/~foosoft/anki-connectdeck-actions">AnkiConnect</a> that has an associated plugin <code>anki.nvim</code>. Basically, AnkiConnects allows you to make requests to Anki and receive/send useful information from/to your decks. Unfortunately, <code>anki.nvim</code> built-in commands didn't seem all that useful to me. However, it provided the necessary infrastructure for me to accomplish my desired workflow.
  </p>
  <p>
  Namely, I made my own command that queries Anki for the deck names, which you can then pick from using telescope pickers. When you select one it will create a new flashcard in a specified flashcard directory under a directory created based on the deck name. You can then send this card to that deck using the existing <code>AnkiSend</code> command.
  </p>
  <p>I also continued the snippet fun here. When writing Anki cards, you can write latex code between [latex] [/latex] delimiters. I wrote a quick function to detect whether we are in these delimiters and if we are then to load the latex snippets from the previous section. I also added some basic anki filetype plugins to insert things like these delimiters.</p>
</section>
  

  
    <section><header><h2>Formatters and Linters</h2></header>
  <code>compat.nvim</code>, <code>mason.nvim</code>, black, isort.
</section>
  
<script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="nvim" theme="boxy-light" crossorigin="anonymous" async="" />      </div>
    </content>
  </entry>
  <entry>
    <title>Contrastive Reinforcement Learning</title>
    <published>2024-10-29T00:00:00Z</published>
    <updated>2024-10-29T00:00:00Z</updated>
    <author>
      <name>Kellen Kanarios</name>
      <uri>https://kkanarios32.github.io/kellenkanarios/</uri>
    </author>
    <link rel="alternate" type="text/html" href="https://kkanarios32.github.io/0005/" />
    <id>https://kkanarios32.github.io/0005/</id>
    <content type="xhtml">
      <div xmlns="http://www.w3.org/1999/xhtml">
        <p>In this blog post, we aim to demistify <a href="https://kkanarios32.github.io/gcrl/"><em>Contrastive Reinforcement Learning</em></a>. This term often gets thrown around in the dark inner circles of the reinforcement learning community. However, for those that are not familiar with contrastive learning, what does contrastive even mean? For those that are, how can reinforcement learning be contrastive? Throughout this blog post, we will answer these questions and many more.</p>
        <section>
          <header>
            <h2>Contrastive Learning</h2>
          </header>
<p>Prior to understanding contrastive reinforcement learning, it is important to have an at least rudimentary understanding of contrastive learning. Historically, contrastive learning has been used to learn representations. The fundamental idea behind contrastive learning is to encourage the representations of similar outputs to be similar in representation space.</p><p><strong>Supervised setting:</strong> For now, assume we are in the supervised setting (we have access to lables). Suppose that we are learning a representation in <code>\mathbb {R}^d</code>. Our model is a classifier on dogs and cats. If we have two dogs <code>y_1</code> and <code>y_2</code> then we want the learned representation map <code>\phi : \{\text {dogs}, \text {cats}\} \to  \mathbb {R}^d</code> to be such that <code>\phi (y_1)</code> and <code>\phi (y_2)</code> are "close" in <code>\mathbb {R}^d</code>. Now the notion of "close" is to be determined by the user. An example could be to minimize the inner product between their representation maps i.e. we could learn a feature map parametrized by <code>\theta </code> with the following objective <code>\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle .</code> Similarly, we want dissimilar outputs to be far apart in representation space. If <code>y_3</code> is a cat, then we can introduce a regularization to encourage this i.e.
<code>\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle  - \sum _{i \in  \{1, 2\}} \langle  \phi _{\theta }(y_i), \phi _{\theta }(y_3) \rangle .</code></p><p><strong>Unsupervised setting:</strong> Now suppose that we get rid of labels and are just given <code>n</code> dog samples <code>\mathcal {D}</code> from some distribution <code>p_{\mathcal {D}}</code>. We now want to be able to learn <code>p_{\theta }</code> to somehow estimate this distribution. An approach is to learn to distinguish the sample dogs given from random noise. To do so, we generate <code>n</code> random images <code>\mathcal {R}</code> according to some distribution <code>p_{\mathcal {R}}</code>. We can now return to the supervised learning setting, where we treat <code>\mathcal {D}</code> and <code>\mathcal {R}</code> as two classes. If we recall standard supervised learning practice, given a sample <code>x</code>, we then want to find <code>p(\mathcal {D} \mid  x) = 1 - p(\mathcal {R} \mid  X).</code> 
As an explicit example, we will use logistic regression. Namely, we will model <code>p(x) = p(\mathcal {D} \mid  x)</code> as <code>p_{\theta }(x) = \frac {1}{1 + e^{-G_{\theta }(x)}}.</code> However, <code>p_{\theta }(x)</code> is estimating <code>p(\mathcal {D} \mid  x)</code>, where we care about <code>p(x \mid  \mathcal {D})</code>. To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if <code>p_{\theta }(x) = p(\mathcal {D} \mid  x)</code> then <code>G_{\theta }(x) = \log  \frac {p(x \mid  \mathcal {D})}{p(x \mid  \mathcal {R})}</code>. Since we generated the samples from <code>\mathcal {R}</code>, we have the explicit distribution i.e. <code>p(x \mid  \mathcal {R}) = p_{\mathcal {R}}(x)</code>. Therefore, we can restrict <code>G_{\theta }</code> to explicitly learn <code>p(x \mid  \mathcal {D})</code> by considering <code>G_{\theta }(x) = \log  p_{\theta }(x \mid  \mathcal {D}) - \log  p_{\mathcal {R}}(x),</code> considering the cross entropy loss we get the <a href="https://kkanarios32.github.io/nce/">NCE loss</a></p><section><header><h3>NCE loss</h3></header><p>The <em><a href="https://kkanarios32.github.io/nce/">NCE</a></em> loss aims to minimize the following objective <code>\mathcal {L}_{N} = - \sum _{t} \log  \left [h(x_t; \theta )\right ] + \log \left [1 - h(y_t; \theta )\right ],</code> where <code>x_t</code> are samples from the data distribution and <code>y_t</code> are randomly generated samples and <code>\begin {array}{r c l}{{h({\bf  u};\theta )}}&amp;{{=}}&amp;{{\frac {1}{1+\exp \left [-G({\bf  u};\theta )\right ]},}}\\ {{G({\bf  u};\theta )}}&amp;{{=}}&amp;{{\ln  p_{m}({\bf  u};\theta )-\ln  p_{n}({\bf  u}).}}\end {array}</code></p></section><p>In <a href="https://kkanarios32.github.io/gutmann2012/">Noise-contrastive Estimation</a>, they show under mild conditions that the estimator <code>p_{\theta }(x \mid  D) \to  p_{\mathcal {D}}(x)</code> in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is <a href="https://kkanarios32.github.io/000F/">consistent</a>.</p><p><strong>Time series:</strong> Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner. For a <code>k</code> step prediction, this would just be your model predicting what will happen in <code>k</code> steps then seeing if it matches what occured <code>k</code> steps in the future in the sample trajectory. However, if your sample space <code>\mathcal {X}</code> is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.</p><p>Fast forwarding to contrastive RL, current work is primarily considered with a particular contrastive objective.</p><section><header><h3>InfoNCE</h3></header><p>The <em><a href="https://kkanarios32.github.io/vandenOord2018/">InfoNCE</a></em> loss aims to minimize the following information-theoretic objective <code>\mathcal {L}_{N} = - \mathbb {E}_{\mathcal {X}} \left [\log  \frac {f_k(x_{t + k}, c_t)}{\sum _{x_j \in  \mathcal {X}} f_k(x_j, c_t)}\right ]</code></p></section><p>Now we need to unpack this very ominous loss. To start, what are <code>x_k</code> and <code>c_t</code>?</p><section><header><h3>Maximize Mutual info</h3></header><p><code>\mathcal {L}_N</code> from <a href="https://kkanarios32.github.io/vandenOord2018/">Contrastive Predictive Decoding</a> maximizes a lower bound on the <a href="https://kkanarios32.github.io/000V/">Mutual Information</a> between <code>x_{t + k}</code> and <code>c_t</code>.</p>
   
   <section>All we must do is plug <code>\frac {p(x \mid  c)}{p(x)}</code> back into the objective.
  <code>\begin {align*}
    \mathcal {L}_{\mathbb {N}}^{\text {opt}}&amp;=-\,\mathbb {E}\log \left [\frac {\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}}{\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}+\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}}\right ] \\
    &amp;=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\
    &amp;\approx \mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}(N-1)\,\mathbb {E}\,\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\
    &amp;=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k} \mid  c_t)}N\right ] \\
    &amp;\geq  \mathbb {E} \log  \left [\frac {p(x_{t + k})}{p(x_{t + k} \mid  c_t)}N \right ] \\
    &amp;= - I(x_{t + k}, c_t) + \log  N
 \end {align*}
  </code>
</section>
 

</section><code>
\operatorname *{max}_{f(u,v)}\mathbb {E}_{(u,v^{+})\sim  p(u,v)}\left [\log \sigma (\underbrace {f(u,{\green  v^{+}})}_{\phi (u)^{T}\psi ({\green  v^{+}})})+\log (1-\sigma (\underbrace {f(u,{\red  v^{-}})}_{\phi (u)^{T}\psi ({\red  v^{-}})}))\right ]
</code><code>
\begin {align*}
&amp;\operatorname *{max}_{f}\mathbb {E}_{(s,a)\sim  p(s,a),s_{f}^{-}\sim  p(s_{f})}\left [\mathcal {L}(s,a,s_{f}^{+},s_{f}^{-})\right ] \\
\end {align*}
</code><code>
\mathcal {L}_1(\theta ) = \log \sigma (f_{\theta }(s_1,a_1,{\color {green} s_{8}})) + \log (1-\sigma (f_{\theta }(s_1,a_1, {\color {red} s_3})))
</code><code>
\begin {align*}
\widehat {\mathcal {L}}(\theta ) &amp;= \frac {1}{n} \sum _{i = 1}^{n} \mathcal {L}_i \\
&amp;= \frac {1}{n} \sum _{i = 1}^{n} \Big [\log \sigma (f_{\theta }(s_i,a_i,{\color {green} s_{f}^{+}})) + \log (1-\sigma (f_{\theta }(s_i,a_i, {\color {red} s_{f}^{-}})))\Big ]
\end {align*}
</code><code>
\mathcal {L}(\theta ) = \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ]
</code><code>f^*(s, a, s_g) = \log \left (\frac {p^{\pi (\cdot  \mid  \cdot )}(s_g \mid  s, a)}{p(s_g)}\right )</code>
   
   <section>
    We want to maximize
<code>
    \begin {align*}
\mathcal {L}(\theta ) &amp;= \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] \\
&amp;= \int  \log \sigma (f_{\theta }(x)) P_X(x) + \int  \log (1-\sigma (f_{\theta }(y))) P_Y(y) \\
&amp;= \int  \log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)
    \end {align*}
    </code>
    Since we are maximizing <code>f(s)</code>, we can just maximize the integrand i.e.
<code>
      \begin {align*}
        \frac {\mathrm {d}}{\mathrm {d}f(z)} \Big [\log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)\Big ] = 0
      \end {align*}
    </code>
    Solving,
    <code>
        \begin {align*}
          P_X(z)\big (1 - \sigma (f(z))\big ) - P_Y(z)\sigma (f(z)) = 0 &amp;\iff  \sigma (f(z)) = \frac {P_X(z)}{P_X(z) + P_Y(z)} \\
          &amp;\iff  f(z) = \log \left (\frac {P_X(z)}{P_Y(z)}\right )
        \end {align*}
      </code>
  </section>
 


   
   <section>
The first step is to prove that the average Q-values are close to the task-conditioned Q-values. Below, we will use <code>R_{c}(\tau )\triangleq \sum _{\ell =0}^{\infty }\gamma ^{\ell }r_{\ell }(s_{\ell },a_{\ell })</code>:

<code>
\begin {align*}
\left |Q^{\beta (\cdot |\cdot ,a)}(s,a,e)-Q^{\beta (\cdot |\cdot ,\epsilon ^{\prime })}(s,a,e)\right |&amp;=\left |\int \beta (\tau \mid  s,a,e)R_{e}(\tau )d\tau -\int \beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right |\\ 
&amp;=\left |\int \beta (\tau \mid  s,a,e)-\beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right | \\
&amp;=\left |\int \beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )R_{e}(\tau )d\tau \right | \\
&amp;\leq \int \left |\beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )\right |d\tau \cdot \operatorname *{max}_{\tau }|R_{e}(\tau )d\tau | \\
&amp;\leq \int \beta (\tau \mid  s,a,e)\left |1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right |d\tau \cdot  1 \\
&amp;=\mathbb {E}_{\beta (\tau |s,a,e)}\left [\left |1-{\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}}\right |\right ] \\
&amp;\leq  \epsilon .
\end {align*}
  </code>
</section>
 

        </section>
        <script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="contrastive-rl" theme="boxy-light" crossorigin="anonymous" async="" />
      </div>
    </content>
  </entry>
</feed>
