<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Ruoyu Sun</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2019</fr:year>
      <fr:month>12</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/sunOptimizationDeepLearning2019/</fr:uri>
    <fr:display-uri>sunOptimizationDeepLearning2019</fr:display-uri>
    <fr:route>/sunOptimizationDeepLearning2019/</fr:route>
    <fr:title text="Optimization for deep learning: Theory and algorithms">Optimization for deep learning: Theory and algorithms</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.1912.08957</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/1912.08957</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{sunOptimizationDeepLearning2019,
 title = {Optimization for Deep Learning: Theory and Algorithms},
 author = {Sun, Ruoyu},
 year = {2019},
 doi = {10.48550/arXiv.1912.08957},
 urldate = {2025-04-23},
 number = {arXiv:1912.08957},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/AB8NMQPP/Sun - 2019 - Optimization for deep learning theory and algorithms.pdf;/home/kellen/Downloads/pdfs/storage/PTFCY2M3/1912.html},
 keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.},
 primaryclass = {cs},
 eprint = {1912.08957},
 month = {December},
 shorttitle = {Optimization for Deep Learning}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
