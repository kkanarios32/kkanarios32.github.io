<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Richard%20S.%20Sutton" type="external">Richard S. Sutton</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Andrew%20G.%20Barto" type="external">Andrew G. Barto</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2018</fr:year>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/suttonReinforcementLearningIntroduction2018/</fr:uri>
    <fr:display-uri>suttonReinforcementLearningIntroduction2018</fr:display-uri>
    <fr:route>/suttonReinforcementLearningIntroduction2018/</fr:route>
    <fr:title text="Reinforcement learning: An introduction">Reinforcement learning: An introduction</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="bibtex"><![CDATA[@book{suttonReinforcementLearningIntroduction2018,
 title = {Reinforcement Learning: An Introduction},
 author = {Sutton, Richard S. and Barto, Andrew G.},
 year = {2018},
 isbn = {978-0-262-03924-6},
 edition = {Second edition},
 series = {Adaptive Computation and Machine Learning Series},
 publisher = {The MIT Press},
 address = {Cambridge, Massachusetts},
 file = {/home/kellen/Zotero/storage/DY4UI6G7/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
 keywords = {Reinforcement learning},
 lccn = {Q325.6 .R45 2018},
 langid = {english},
 abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
 shorttitle = {Reinforcement Learning}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>3</fr:month>
              <fr:day>31</fr:day>
            </fr:date>
            <fr:uri>https://kkanarios32.github.io/006Q/</fr:uri>
            <fr:display-uri>006Q</fr:display-uri>
            <fr:route>/006Q/</fr:route>
            <fr:title text="Test Time Compute in Reinforcement Learning">Test Time Compute in Reinforcement Learning</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><html:em>One thing that should be learned [...] is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.</html:em> - <fr:link href="/richardsutton/" title="Richard Sutton" uri="https://kkanarios32.github.io/richardsutton/" display-uri="richardsutton" type="local">Richard Sutton</fr:link></html:p>
  
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>31</fr:day></fr:date><fr:title text="What is Planning?">What is Planning?</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
    In <fr:link href="/suttonReinforcementLearningIntroduction2018/" title="Reinforcement learning: An introduction" uri="https://kkanarios32.github.io/suttonReinforcementLearningIntroduction2018/" display-uri="suttonReinforcementLearningIntroduction2018" type="local">Reinforcement learning: An introduction</fr:link>, they define planning as <html:em>a computational process that takes a model as input and outputs a policy</html:em>. Like everything Sutton writes, I agree with it for the most part. I have struggled with this question for a long time. In the RL community, you often here this vague term "planning" thrown around in all sorts of different situations. I think the key distinction between traditional methods is shown when looking at these methods directly. The only definition I have come up with that leaves me somewhat satisfied is "policy improvement in the absence of learning.
  </html:p>
  <html:p>
    As an example, in <fr:link href="/suttonReinforcementLearningIntroduction2018/" title="Reinforcement learning: An introduction" uri="https://kkanarios32.github.io/suttonReinforcementLearningIntroduction2018/" display-uri="suttonReinforcementLearningIntroduction2018" type="local">Q-learning</fr:link> you interact with the environment and learn via updating your Q-function. You then immediately recover an action via <fr:tex display="inline"><![CDATA[a \in  \arg \max _{a} Q(s, a)]]></fr:tex> This does not require that you input a model rather you only input the current state and receive the corresponding action. Learning can be seen as distilling everything needed into the model, where planning allows the model to see and plan based on the consequences of potential actions using a model of the environment.
  </html:p>
</fr:mainmatter></fr:tree>
  
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>31</fr:day></fr:date><fr:uri>https://kkanarios32.github.io/006R/</fr:uri><fr:display-uri>006R</fr:display-uri><fr:route>/006R/</fr:route><fr:title text="Alpha-Zero">Alpha-Zero</fr:title></fr:frontmatter><fr:mainmatter><html:p>The major break through of planning was in <fr:link href="/schrittwieserMasteringAtariGo2020/" title="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" uri="https://kkanarios32.github.io/schrittwieserMasteringAtariGo2020/" display-uri="schrittwieserMasteringAtariGo2020" type="local">alpha-zero</fr:link>, or more accurately at the time was just alpha-go. However, the core idea remained the same. The idea is to learn some notion of a "good" state in a game like chess or go and then leverage this information in combination with some planning.</html:p></fr:mainmatter></fr:tree><html:script src="https://utteranc.es/client.js" repo="kkanarios32/website-comments" issue-term="mcts" theme="boxy-light" crossorigin="anonymous" async="" /></fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
