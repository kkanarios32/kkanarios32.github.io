<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>7</fr:month>
      <fr:day>14</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/W4YP/</fr:uri>
    <fr:display-uri>W4YP</fr:display-uri>
    <fr:route>/W4YP/</fr:route>
    <fr:title text="Multi Query Attention">Multi Query Attention</fr:title>
  </fr:frontmatter>
  <fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>Noam Shazeer</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2019</fr:year>
              <fr:month>11</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/shazeerFastTransformerDecoding2019/</fr:uri>
            <fr:display-uri>shazeerFastTransformerDecoding2019</fr:display-uri>
            <fr:route>/shazeerFastTransformerDecoding2019/</fr:route>
            <fr:title text="Fast Transformer Decoding: One Write-Head is All You Need">Fast Transformer Decoding: One Write-Head is All You Need</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="doi">10.48550/arXiv.1911.02150</fr:meta>
            <fr:meta name="external">https://arxiv.org/abs/1911.02150</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{shazeerFastTransformerDecoding2019,
 title = {Fast {{Transformer Decoding}}: {{One Write-Head}} Is {{All You Need}}},
 author = {Shazeer, Noam},
 year = {2019},
 doi = {10.48550/arXiv.1911.02150},
 urldate = {2025-07-14},
 number = {arXiv:1911.02150},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/BQ6F9K2H/Shazeer - 2019 - Fast Transformer Decoding One Write-Head is All Y.pdf},
 keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
 primaryclass = {cs},
 eprint = {1911.02150},
 month = {November},
 shorttitle = {Fast {{Transformer Decoding}}}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>13</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/ESQ3/</fr:uri>
            <fr:display-uri>ESQ3</fr:display-uri>
            <fr:route>/ESQ3/</fr:route>
            <fr:title text="CS336 Lecture 3"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 3</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate practice</fr:link> (5/66).</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4WB/</fr:uri><fr:display-uri>W4WB</fr:display-uri><fr:route>/W4WB/</fr:route><fr:title text="Architecture Variations">Architecture Variations</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Norms">Norms</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:strong>Pre-norm vs. Post-norm</html:strong>: The first architecture variation discussed is when to apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link>. As can be seen in the figure below, rather than apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> in the residual stream. They instead place it before the FFN and MHA layers.</html:p>
<html:figure><html:img width="40%" src="/bafkrmiawz4eenenwuvnx3pzxjniwghlortvgc5ytxzq5z5oyznugozl22y.png" />
<html:figcaption>Post norm (a) vs. pre norm (b).</html:figcaption></html:figure>
<html:p>They actually found that adding <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer normalization</fr:link> before and after the MHA and FFN works the best. This is known as <html:em>double norm</html:em>.</html:p>
<html:p><html:strong><fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> vs. <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link></html:strong>:
Another useful trick is to use the <fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> instead of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link>.
</html:p>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/1XNO/</fr:uri><fr:display-uri>1XNO</fr:display-uri><fr:route>/1XNO/</fr:route><fr:title text="RMS Norm">RMS Norm</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The RMS norm is simply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> without the added bias term and centering i.e. 
<fr:tex display="block"><![CDATA[\mathrm {RMSNorm}: \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \frac {\mathbf {x}}{\sqrt {\mathrm {Var}[\mathbf {x}_i] + \epsilon }}*\gamma .]]></fr:tex>
This is used to spare memory movement of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layernorm</fr:link>, where the bias term <fr:tex display="inline"><![CDATA[\beta  \in  \mathbb {R}^d]]></fr:tex> and has been found to be empirically almost if not as good.
</html:p></fr:mainmatter></fr:tree>
<html:p>One might wonder if matrix multiplies are the only thing that matters what can such a small change really accomplish.</html:p>
<html:ul><html:li>Due to memory movement, despite being .17<![CDATA[%]]> of <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link>, provided 25<![CDATA[%]]> speedup in runtime!!</html:li></html:ul>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Most people apparently just drop the bias term and keep the centering i.e. subtracting the mean. This makes sense because computing the mean does not require loading any additional information back to memory.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Activations">Activations</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Despite a long list of activations, the two focused on are <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> and <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link>.</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/BEA2/</fr:uri><fr:display-uri>BEA2</fr:display-uri><fr:route>/BEA2/</fr:route><fr:title text="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>rectified linear unit</html:em>(ReLU) is defined as
<fr:tex display="block"><![CDATA[\mathrm {ReLU} : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \max (0, \mathbf {x}),]]></fr:tex>
where the <fr:tex display="inline"><![CDATA[\max ]]></fr:tex> is done elementwise i.e. <fr:tex display="inline"><![CDATA[\mathrm {ReLU}(\mathbf {x})_i = \max (0, x_i)]]></fr:tex>
This provides "nice" gradients, making it common when training neural networks.
</html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/5SPM/</fr:uri><fr:display-uri>5SPM</fr:display-uri><fr:route>/5SPM/</fr:route><fr:title text="Gaussian Error Linear Unit (GELU)">Gaussian Error Linear Unit (GELU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Gaussian Error Linear Unit</html:em> (GELU) is a slight modification to the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> to account for the non-differentiability at <fr:tex display="inline"><![CDATA[0]]></fr:tex>. Namely,
<fr:tex display="block"><![CDATA[\mathrm {GELU}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} \cdot  \psi (\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[\psi (\mathbf {x}) = \mathrm {CDF}(\mathcal {N}(\mathbf {x}, \mathbf {I}))]]></fr:tex></html:p><html:figure><html:img width="70%" src="/bafkrmian7h7ke2dp6nqdq4624wx4off5lk346f7d5pp6mv7bbimcy7azni.png" />
<html:figcaption>GELU vs. ReLU activation and derivative. Taken from <fr:link href="https://www.baeldung.com/cs/gelu-activation-function" type="external">here</fr:link>.</html:figcaption></html:figure></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/OZD8/</fr:uri><fr:display-uri>OZD8</fr:display-uri><fr:route>/OZD8/</fr:route><fr:title text="Swish Activation">Swish Activation</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Swish</html:em> activation is given by
<fr:tex display="block"><![CDATA[\mathrm {Swish}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} s(\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[s : \mathbb {R}^d \to  \mathbb {R}^d]]></fr:tex> is the <fr:link href="/NNDS/" title="Sigmoid" uri="https://kellenkanarios.com/NNDS/" display-uri="NNDS" type="local">sigmoid function</fr:link>.
</html:p></fr:mainmatter></fr:tree>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  The extra computation required by <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link> or <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> is a non-factor because <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> are dominated by matrix multiplication and there is no increase in memory pressure.
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/K2N7/</fr:uri><fr:display-uri>K2N7</fr:display-uri><fr:route>/K2N7/</fr:route><fr:title text="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>A <html:em>gated linear unit</html:em> is an activation function combined with an elementwise multiplication i.e. 
<html:ul><html:li><fr:tex display="inline"><![CDATA[\mathrm {ReGLU} = \mathrm {ReLU}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li>
  <html:li><fr:tex display="inline"><![CDATA[\mathrm {SwiGLU} = \mathrm {Swish}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li></html:ul>
where <fr:tex display="inline"><![CDATA[\mathrm {ReLU}]]></fr:tex> is the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> activation and <fr:tex display="inline"><![CDATA[\mathrm {Swish}]]></fr:tex> is the <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> activation respectively.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The matrix <fr:tex display="inline"><![CDATA[\mathbf {V}]]></fr:tex> introduces additional learnable parameters. Therefore, an architecture that uses gating typically reduces their other parameters by a factor of <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>.
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Serial vs. Parallel">Serial vs. Parallel</fr:title></fr:frontmatter><fr:mainmatter>
Another small trick is to parallelize computation. Traditionally, one computes the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> output and then passes this to the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> i.e.
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x} + \mathrm {MHA}(\mathrm {LN}(\mathbf {x})))).]]></fr:tex>
However, this requires waiting for the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> to complete before performing the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> computation. It has been found that performing these in parallel does not cause any severe degradation in performance. Explicitly, instead they do
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x})) + \mathrm {MHA}(\mathrm {LN}(\mathbf {x}))]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/TOD6/</fr:uri><fr:display-uri>TOD6</fr:display-uri><fr:route>/TOD6/</fr:route><fr:title text="Rotary Position Embeddings (ROPE)">Rotary Position Embeddings (ROPE)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Notation</html:strong>:
For this section, our notion of an <html:em>embedding</html:em> is a function <fr:tex display="inline"><![CDATA[f]]></fr:tex> that takes a token and a position i.e. <fr:tex display="inline"><![CDATA[f(x, i)]]></fr:tex> is the embedding of token <fr:tex display="inline"><![CDATA[x]]></fr:tex>, occurring at position <fr:tex display="inline"><![CDATA[i]]></fr:tex>.
</html:p><html:p><html:strong>Idea:</html:strong> the <fr:link href="/B1RI/" title="Inner Product" uri="https://kellenkanarios.com/B1RI/" display-uri="B1RI" type="local">inner product</fr:link> of two embeddings should only need relative positioning i.e. for any two <fr:tex display="inline"><![CDATA[\mathbf {x}]]></fr:tex>, <fr:tex display="inline"><![CDATA[\mathbf {y}]]></fr:tex> with positions <fr:tex display="inline"><![CDATA[i]]></fr:tex> and <fr:tex display="inline"><![CDATA[j]]></fr:tex> respectively, there should exist some <fr:tex display="inline"><![CDATA[g]]></fr:tex>, such that
<fr:tex display="block"><![CDATA[\langle  f(\mathbf {x}, i), f(\mathbf {y}, j) \rangle  = g(\mathbf {x}, \mathbf {y}, i - j)]]></fr:tex>
Namely, this can be written as a function of just the relative positioning between the embeddings.
</html:p><html:p>The simplest transformation that only preserves this relative information are <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link>.</html:p><html:figure><html:img width="50%" src="/bafkrmiba4muc7qqk2agccjalqpm24hxw7u4gkfpozmnhz7if6ncagxpsgi.png" />
<html:figcaption>Rotating embeddings does not change relative position.</html:figcaption></html:figure><html:p>However, <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link> are only easily defined for <fr:tex display="inline"><![CDATA[\mathbb {R}^2]]></fr:tex>. To get around this, they simply partition their input 2d slices and apply the rotation via
<fr:tex display="block"><![CDATA[\mathrm {ROT} = \begin {bmatrix} \cos  m \theta _{1} & - \sin  m \theta _{1} & & & \\
\sin  m \theta _{1} & \cos  m \theta _{1} & & & \\
& & \cos  m \theta _{2} & -\sin  m \theta _{2} & \\
& & \sin  m \theta _{2} & \cos  m \theta _{2} & \\
& & & & \ddots 
\end {bmatrix} ]]></fr:tex></html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The <fr:tex display="inline"><![CDATA[\theta _i]]></fr:tex>'s are fixed and chosen according to some schedule to capture different "frequencies". 
</fr:mainmatter></fr:tree>
<html:p>The embedding are used by appling them to the query and key matrices separately i.e. 
<fr:tex display="block"><![CDATA[W_K = \mathrm {ROT}(W_K), \quad  W_Q = \mathrm {ROT}(W_Q)]]></fr:tex>
These are then what is used in self-attention.
</html:p></fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/9VZH/</fr:uri><fr:display-uri>9VZH</fr:display-uri><fr:route>/9VZH/</fr:route><fr:title text="Hyperparameter Rules (CS336)">Hyperparameter Rules (<fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Rule 1</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {ff}} = d_{\text {model}}]]></fr:tex>
<html:ul><html:li><fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex> is the input dimension</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d_{\text {ff}}]]></fr:tex> is the hidden dimension</html:li></html:ul></html:p><html:p><html:strong>Exception 1</html:strong>: in the case of <fr:link href="/K2N7/" title="Gated Linear Unit (GLU)" uri="https://kellenkanarios.com/K2N7/" display-uri="K2N7" type="local">GLUs</fr:link>, we recall that to account for extra parameters we scale <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>. Therefore, <fr:tex display="inline"><![CDATA[d_{\text {ff}} = \frac {8}{3}d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Exception 2</html:strong>: In T5, they use <fr:tex display="inline"><![CDATA[d_{\text {ff}} = 64d_{\text {model}}]]></fr:tex>. The logic for this was that we could maximize the <fr:link href="/99G8/" title="Model FLOP Utilization (MFU)" uri="https://kellenkanarios.com/99G8/" display-uri="99G8" type="local">MFU</fr:link> by increasing matrix size. Ended up going back to small <fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Rule 2</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {head}} = d_{\text {model}} / \text {num heads}]]></fr:tex>
<html:ul><html:li>I believe the <fr:tex display="inline"><![CDATA[d_{\text {head}}]]></fr:tex> is the output dim of each head.</html:li>
  <html:li>This just says the total output dim for <fr:tex display="inline"><![CDATA[n]]></fr:tex> heads is the same as if we did one head.</html:li>
  <html:li>Means there is something important about splitting up the heads.</html:li></html:ul></html:p><html:p><html:strong>Rule 3</html:strong>: Aspect ratio = <fr:tex display="inline"><![CDATA[\frac {d_{\text {model}}}{n_{\text {layer}}} \approx  100-200]]></fr:tex>
<html:ul><html:li>Pipeline dependent: if network speed is fast than parallelizing is easier and shallow networks are better.</html:li>
  <html:li>If poor network speed then pipeline parallel might be more viable and deeper networks would be more parallelizable.</html:li></html:ul></html:p><html:p><html:strong>Rule 4:</html:strong> Vocab size
<html:ul><html:li>For single language models, vocab size is typically <fr:tex display="inline"><![CDATA[30-50k]]></fr:tex></html:li>
  <html:li>For multi language models, vocab size is typically <fr:tex display="inline"><![CDATA[100-250k]]></fr:tex></html:li></html:ul></html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Regularization and Dropout">Regularization and Dropout</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Since there is so much data, it is not feasible to train your model for multiple epochs. This is actually nice in the sense that we do not have to worry about overfitting and performing regularization.</html:p>
<html:p>However, many of the large models are still trained with weight decay. Tatsu claims that this is not to do with regularization but actually due to some weird interaction with the learning rate schedule. I am not sure I entirely understood this part.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Stability Tricks">Stability Tricks</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link> is ill-behaved due to the exponentials. In the transformer, we have two softmaxes: one at the end and one in the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>.
<html:ul><html:li>For the first, note <fr:tex display="block"><![CDATA[\log  \sigma (\mathbf {x})_i = \log (x_i) - \log  \underbrace {\sum _{j=1}^{d} e^{x_j}}_{D(\mathbf {x})}]]></fr:tex>
   The only problem is the denominator term. The idea is to enforce the <fr:tex display="inline"><![CDATA[D(\mathbf {x}) = 1]]></fr:tex> by regularizing via a penalty on <fr:tex display="inline"><![CDATA[\log  D(\mathbf {x})]]></fr:tex> ie.
<fr:tex display="block"><![CDATA[\mathcal {L}_{\text {aux}} = 10^{-4} \log ^2(D(\mathbf {x}))]]></fr:tex>
With the <fr:tex display="inline"><![CDATA[\nabla  D(\mathbf {x})]]></fr:tex> should be <fr:tex display="inline"><![CDATA[0]]></fr:tex> and we are effectively only considering the non-exponential term.
   </html:li>
   <html:li>For the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>, they primarily operate on the <html:em>logits</html:em> prior to the softmax. One way is via <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> before the <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link>. Another is via <html:em>softcapping</html:em> i.e.
   <fr:tex display="block"><![CDATA[\mathrm {logits} = \mathrm {softcap} \cdot  \tanh  \left ( \frac {\mathrm {logits}}{\mathrm {softcap}} \right ) ]]></fr:tex></html:li></html:ul></html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Attention Variants">Attention Variants</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4YP/</fr:uri><fr:display-uri>W4YP</fr:display-uri><fr:route>/W4YP/</fr:route><fr:title text="Multi Query Attention">Multi Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>3</fr:month>
              <fr:day>22</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/005G/</fr:uri>
            <fr:display-uri>005G</fr:display-uri>
            <fr:route>/005G/</fr:route>
            <fr:title text="Research Bible">Research Bible</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>I will be reading and reviewing ~2ish paper a week (tentatively). More accurately, I have allocated roughly 8hrs a week to reading papers. However much I accomplish in this time will determine the actual rate of these reviews. These reviews are mainly just for my own understanding. Rather than techinical details, I will aim to cover
<html:ol><html:li>Existing / related work and their issues.</html:li>
  <html:li>Key components of the methodology and how it solves these issues.</html:li>
  <html:li>Limitations / future directions or relationships to my work.</html:li></html:ol></html:p>
            <html:p>For more in-depth coverage of papers I find particularly interesting, see my <fr:link href="/0002/" title="Blog" uri="https://kellenkanarios.com/0002/" display-uri="0002" type="local">blog</fr:link>. There will likely be overlap i.e. most blogs will start here...</html:p>
            <html:p>See <fr:link href="/007N/" title="Upcoming papers of the week" uri="https://kellenkanarios.com/007N/" display-uri="007N" type="local">upcoming papers</fr:link>!</html:p>
            <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>14</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/W4YP/</fr:uri>
                <fr:display-uri>W4YP</fr:display-uri>
                <fr:route>/W4YP/</fr:route>
                <fr:title text="Multi Query Attention">Multi Query Attention</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:uri>https://kellenkanarios.com/2025-W29/</fr:uri>
            <fr:display-uri>2025-W29</fr:display-uri>
            <fr:route>/2025-W29/</fr:route>
            <fr:title text="Weekly Review 2025-W29">Weekly Review 2025-W29</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="The Quantified Me">The Quantified Me</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>This section is in large part to gaslight myself on the internet into being more productive but with the information provided let us introspect on what happened this week. </html:p>
  <html:p><html:strong>Coding:</html:strong> I will now proceed to justify and provide excuses on what went wrong this week. On Monday, we see that I spent a large part of my time on this forest. However, this was actually writing up my notes on <fr:link href="/ESQ3/" title="CS336 Lecture 3" uri="https://kellenkanarios.com/ESQ3/" display-uri="ESQ3" type="local">lecture 3</fr:link> of <fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>. My goal of roughly 1ish hour of miscellaneous learning a day snowballed into what was seemingly <fr:tex display="inline"><![CDATA[4]]></fr:tex> hours of random activity, where I spiraled into <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link>. I am thinking of putting this self-study on the backburner in favor of focusing on <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">three easy pieces</fr:link> until the semester starts. I intend to take two courses on what is essentially LLMs systems this fall (very excited), so I might as well not double dip now. I am thinking about using projects 1 and 2 of this course to develop my own working LLM implementation. I can then use it as reference to try out and learn any new optimization tricks like <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link> or <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, etc. 
  <html:p>These plots also finally confirm what I noticed as a general trend in my working life. Namely, I spend a lot of time working Monday-Thu because of my weekly meetings with my advisor on Thursday. However, I do not properly make use of the rest of the week. This is a trend I hope to improve upon in future entries.</html:p></html:p>
<html:figure><html:img width="100%" src="/bafkrmibvtysmzc5gyedcfaj2sxjpviob6lxrcdiontte4cg4zu3cueiv7u.png" />
  <html:figcaption>Wakatime stats for week 2025-W29.</html:figcaption></html:figure>
  <html:p><html:strong>Rest of (computer) life:</html:strong> This week I did a pretty good job staying on task (at least on my computer). Very little brainrot was consumed and a decent amount of work was accomplished. Tune in next week to see if we can keep that up!
  </html:p>
<html:figure><html:img width="100%" src="/bafkrmie3tohm4dhy7zqt47zdirbm4xxk432rr4n3yncjdggmq4zetriwku.png" />
  <html:figcaption>Arbtt-stats for week 2025-W29.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="VO2 Max(ing)">VO2 Max(ing)</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ul><html:li><html:strong>Current VO2 Max</html:strong>: 53</html:li>
    <html:li><html:strong>Weekly Mileage</html:strong>: 17.2</html:li></html:ul>
  <html:p>Unfortunately, I missed one run this week. The same one I miss every week: Sunday. I will still blame the shin splints, but this may be the last week I can do so (finally!). I am still maintaining a pretty good SPM, but I need to get some more lower HR runs in. From here on out, we should not dip below 20 miles a week. I have some travel coming up at <fr:link href="/rlc/" title="Reinforcement Learning Conference" uri="https://kellenkanarios.com/rlc/" display-uri="rlc" type="local">RLC</fr:link>, but I should be able to still run while I am there.</html:p>
<html:figure><html:img width="100%" src="/bafkrmicxnrelkrspghbameeb74obgst3bk2dj5vlirqeq73qymd7jsjuny.png" />
  <html:figcaption>Running stats for this week.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="Goals for next week">Goals for next week</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>This is another section, where I gaslight myself into some accountability on the internet.</html:p>
<html:p>For my imaginary audience, you should have a lot to look forward to in the next week. Namely, 
<html:ol><html:li>Look forward to an in-depth blog on <fr:link href="/lipmanFlowMatchingGenerative2023/" title="Flow Matching for Generative Modeling" uri="https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/" display-uri="lipmanFlowMatchingGenerative2023" type="local">flow matching</fr:link> to accompany my <fr:link href="/FMTD/" title="Flow Matching and TD Flows" uri="https://kellenkanarios.com/FMTD/" display-uri="FMTD" type="local">slides</fr:link>.</html:li>
<html:li>I also should have two new <fr:link href="/005G/" title="Research Bible" uri="https://kellenkanarios.com/005G/" display-uri="005G" type="local">research bible</fr:link> entries one on the <fr:link href="/touatiLearningOneRepresentation2021/" title="Learning One Representation to Optimize All Rewards" uri="https://kellenkanarios.com/touatiLearningOneRepresentation2021/" display-uri="touatiLearningOneRepresentation2021" type="local">forward backward representation</fr:link> and the other on <fr:link href="/nachumNearOptimalRepresentationLearning2019/" title="Near-Optimal Representation Learning for Hierarchical Reinforcement Learning" uri="https://kellenkanarios.com/nachumNearOptimalRepresentationLearning2019/" display-uri="nachumNearOptimalRepresentationLearning2019" type="local">hierarchical rl</fr:link>.</html:li>
<html:li>For my <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">OS adventure</fr:link>, you should see me (hopefully) get through memory virtualization.
However, I should definitely get to at least the TLB. Unlikely, I will continue to do all the exercises (because I spend more time writing them down than doing them). I might do one or two of the projects, but they are quite intimidating TBD.</html:li></html:ol>
Check back in next week to see if I am productive! Also TBD if in the future I will put research-related stuff in this review.
</html:p>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>12</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/4K5H/</fr:uri>
            <fr:display-uri>4K5H</fr:display-uri>
            <fr:route>/4K5H/</fr:route>
            <fr:title text="Floating Point Operations">Floating Point Operations</fr:title>
            <fr:taxon>Definition</fr:taxon>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>We refer to the <html:strong>total</html:strong> number of floating point operatings as <html:em>FLOPS</html:em>. Alternatively, we refer to the number of floating point operations per second as <html:em>FLOP/s</html:em>.</html:p>
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>2</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/004G/</fr:uri>
            <fr:display-uri>004G</fr:display-uri>
            <fr:route>/004G/</fr:route>
            <fr:title text="Self-Attention">Self-Attention</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>
    TLDR: Learned weighting of token embeddings. Essentially, learning which words to "attend" to in the input sequence. Have matrices
<fr:tex display="inline"><![CDATA[\mathbf {Q} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {q}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {q}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_q}]]></fr:tex>, 
<fr:tex display="inline"><![CDATA[
\mathbf {K} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {k}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {k}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_k}
]]></fr:tex>
<fr:tex display="inline"><![CDATA[
\mathbf {V} = \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \vdots  \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(n)} & \text {---}
  \end {bmatrix}
\end {bmatrix} \in  \mathbb {R}^{n \times  d_v}
]]></fr:tex></html:p><html:p><html:strong>Intuition 1:</html:strong> Convex re-weighting of input tokens.
  Note that
<fr:tex display="block"><![CDATA[
\begin {align*}
  \begin {bmatrix}
    p_1 & p_2 & p_3
  \end {bmatrix} \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(2)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(3)} & \text {---}
  \end {bmatrix}
  \end {bmatrix} = p_1 \mathbf {v}^{(1)} + p_2 \mathbf {v}^{(2)} + p_3 \mathbf {v}^{(3)}
\end {align*}
  ]]></fr:tex>
<fr:tex display="block"><![CDATA[
\begin {align*}
  \begin {bmatrix}
    p_{11} & 0 & 0 \\
    p_{21} & p_{22} & 0 \\
    p_{31} & p_{32} & p_{33}
  \end {bmatrix} \begin {bmatrix}
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(1)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(2)} & \text {---}
  \end {bmatrix} \\
  \begin {bmatrix}
    \text {---} & \mathbf {v}^{(3)} & \text {---}
  \end {bmatrix}
  \end {bmatrix} = 
  \begin {bmatrix}
  p_{11} \mathbf {v}^{(1)}  \\
  p_{21} \mathbf {v}^{(1)} + p_{22} \mathbf {v}^{(2)} \\
  p_{31} \mathbf {v}^{(1)} + p_{32} \mathbf {v}^{(2)} + p_{33} \mathbf {v}^{(3)}
  \end {bmatrix}
\end {align*}
  ]]></fr:tex>
  <html:strong>Intuition 2:</html:strong> Context dependent re-weighting.
      If <fr:tex display="inline"><![CDATA[\mathbf {p} = \mathbb {S}(\mathbf {Q} \mathbf {K}^T)]]></fr:tex> then
      <fr:tex display="block"><![CDATA[
        \begin {align*}
          p_{ij} = \frac {\mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}{\sum _{j} \mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}
        \end {align*}
      ]]></fr:tex></html:p>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
    Suppose that <fr:tex display="inline"><![CDATA[\mathbf {x} = \text {I play with the ball}]]></fr:tex>. Then 
<fr:tex display="block"><![CDATA[
    \begin {align*}
      \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``ball"})
    \end {align*}
  ]]></fr:tex>
  A feasible query for "ball" would be a verb describing the action of the ball, so maybe
  <fr:tex display="block"><![CDATA[
  \begin {align*}
      W_q \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``play"})
  \end {align*}
  ]]></fr:tex>
  and a key for "play" would be what you are playing with like a ball, so 
  <fr:tex display="block"><![CDATA[
  \begin {align*}
      W_k \mathbf {x}^{(2)} = \mathrm {Embed}(\text {``ball"})
  \end {align*}
  ]]></fr:tex>
  i.e.
<fr:tex display="block"><![CDATA[
  \begin {align*}
    \mathrm {Query}(\text {``quantum"}) \cdot  \mathrm {Key}(\text {``mechanics"}) \approx 
    ||\mathrm {Query}(\text {``quantum"})|| \cdot  ||\mathrm {Key}(\text {``mechanics"})||
  \end {align*}
]]></fr:tex>

</fr:mainmatter></fr:tree>
 
<html:p><fr:tex display="block"><![CDATA[
  \begin {align*}
\left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} &= \mathbb {S}\left (\begin {bmatrix}
\mathbf {q}^{(4)} \cdot  \mathbf {k}^{(1)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(2)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(3)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(4)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(5)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(6)}
\end {bmatrix}
\right ) \\
&= \begin {bmatrix}
0 & 0.2 & 0.3 & 0.5 & 0 & 0
\end {bmatrix}
  \end {align*}
]]></fr:tex>
<fr:tex display="block"><![CDATA[
\left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} \mathbf {V} = 0.2 \mathbf {v}^{(2)} + 0.3 \mathbf {v}^{(3)} + 0.5 \mathbf {v}^{(5)}
]]></fr:tex></html:p></fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
