<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:uri>https://kellenkanarios.com/2025-W29/</fr:uri>
    <fr:display-uri>2025-W29</fr:display-uri>
    <fr:route>/2025-W29/</fr:route>
    <fr:title text="Weekly Review 2025-W29">Weekly Review 2025-W29</fr:title>
  </fr:frontmatter>
  <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="The Quantified Me">The Quantified Me</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>This section is in large part to gaslight myself on the internet into being more productive but with the information provided let us introspect on what happened this week. </html:p>
  <html:p><html:strong>Coding:</html:strong> I will now proceed to justify and provide excuses on what went wrong this week. On Monday, we see that I spent a large part of my time on this forest. However, this was actually writing up my notes on <fr:link href="/ESQ3/" title="CS336 Lecture 3" uri="https://kellenkanarios.com/ESQ3/" display-uri="ESQ3" type="local">lecture 3</fr:link> of <fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>. My goal of roughly 1ish hour of miscellaneous learning a day snowballed into what was seemingly <fr:tex display="inline"><![CDATA[4]]></fr:tex> hours of random activity, where I spiraled into <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link>. I am thinking of putting this self-study on the backburner in favor of focusing on <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">three easy pieces</fr:link> until the semester starts. I intend to take two courses on what is essentially LLMs systems this fall (very excited), so I might as well not double dip now. I am thinking about using projects 1 and 2 of this course to develop my own working LLM implementation. I can then use it as reference to try out and learn any new optimization tricks like <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link> or <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, etc. 
  <html:p>These plots also finally confirm what I noticed as a general trend in my working life. Namely, I spend a lot of time working Monday-Thu because of my weekly meetings with my advisor on Thursday. However, I do not properly make use of the rest of the week. This is a trend I hope to improve upon in future entries.</html:p></html:p>
<html:figure><html:img width="100%" src="/bafkrmibvtysmzc5gyedcfaj2sxjpviob6lxrcdiontte4cg4zu3cueiv7u.png" />
  <html:figcaption>Wakatime stats for week 2025-W29.</html:figcaption></html:figure>
  <html:p><html:strong>Rest of (computer) life:</html:strong> This week I did a pretty good job staying on task (at least on my computer). Very little brainrot was consumed and a decent amount of work was accomplished. Tune in next week to see if we can keep that up!
  </html:p>
<html:figure><html:img width="100%" src="/bafkrmie3tohm4dhy7zqt47zdirbm4xxk432rr4n3yncjdggmq4zetriwku.png" />
  <html:figcaption>Arbtt-stats for week 2025-W29.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="VO2 Max(ing)">VO2 Max(ing)</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ul><html:li><html:strong>Current VO2 Max</html:strong>: 53</html:li>
    <html:li><html:strong>Weekly Mileage</html:strong>: 17.2</html:li></html:ul>
  <html:p>Unfortunately, I missed one run this week. The same one I miss every week: Sunday. I will still blame the shin splints, but this may be the last week I can do so (finally!). I am still maintaining a pretty good SPM, but I need to get some more lower HR runs in. From here on out, we should not dip below 20 miles a week. I have some travel coming up at <fr:link href="/rlc/" title="Reinforcement Learning Conference" uri="https://kellenkanarios.com/rlc/" display-uri="rlc" type="local">RLC</fr:link>, but I should be able to still run while I am there.</html:p>
<html:figure><html:img width="100%" src="/bafkrmicxnrelkrspghbameeb74obgst3bk2dj5vlirqeq73qymd7jsjuny.png" />
  <html:figcaption>Running stats for this week.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="Goals for next week">Goals for next week</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>This is another section, where I gaslight myself into some accountability on the internet.</html:p>
<html:p>For my imaginary audience, you should have a lot to look forward to in the next week. Namely, 
<html:ol><html:li>Look forward to an in-depth blog on <fr:link href="/lipmanFlowMatchingGenerative2023/" title="Flow Matching for Generative Modeling" uri="https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/" display-uri="lipmanFlowMatchingGenerative2023" type="local">flow matching</fr:link> to accompany my <fr:link href="/FMTD/" title="Flow Matching and TD Flows" uri="https://kellenkanarios.com/FMTD/" display-uri="FMTD" type="local">slides</fr:link>.</html:li>
<html:li>I also should have two new <fr:link href="/005G/" title="Research Bible" uri="https://kellenkanarios.com/005G/" display-uri="005G" type="local">research bible</fr:link> entries one on the <fr:link href="/touatiLearningOneRepresentation2021/" title="Learning One Representation to Optimize All Rewards" uri="https://kellenkanarios.com/touatiLearningOneRepresentation2021/" display-uri="touatiLearningOneRepresentation2021" type="local">forward backward representation</fr:link> and the other on <fr:link href="/nachumNearOptimalRepresentationLearning2019/" title="Near-Optimal Representation Learning for Hierarchical Reinforcement Learning" uri="https://kellenkanarios.com/nachumNearOptimalRepresentationLearning2019/" display-uri="nachumNearOptimalRepresentationLearning2019" type="local">hierarchical rl</fr:link>.</html:li>
<html:li>For my <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">OS adventure</fr:link>, you should see me (hopefully) get through memory virtualization.
However, I should definitely get to at least the TLB. Unlikely, I will continue to do all the exercises (because I spend more time writing them down than doing them). I might do one or two of the projects, but they are quite intimidating TBD.</html:li></html:ol>
Check back in next week to see if I am productive! Also TBD if in the future I will put research-related stuff in this review.
</html:p>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>Yaron Lipman</fr:author>
              <fr:author>Ricky T. Q. Chen</fr:author>
              <fr:author>Heli Ben-Hamu</fr:author>
              <fr:author>Maximilian Nickel</fr:author>
              <fr:author>Matt Le</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2023</fr:year>
              <fr:month>2</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/</fr:uri>
            <fr:display-uri>lipmanFlowMatchingGenerative2023</fr:display-uri>
            <fr:route>/lipmanFlowMatchingGenerative2023/</fr:route>
            <fr:title text="Flow Matching for Generative Modeling">Flow Matching for Generative Modeling</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="doi">10.48550/arXiv.2210.02747</fr:meta>
            <fr:meta name="external">https://arxiv.org/abs/2210.02747</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{lipmanFlowMatchingGenerative2023,
 title = {Flow {{Matching}} for {{Generative Modeling}}},
 author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},
 year = {2023},
 doi = {10.48550/arXiv.2210.02747},
 urldate = {2025-06-20},
 number = {arXiv:2210.02747},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/WMMQDW4B/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/home/kellen/Downloads/pdfs/storage/R693NNL4/2210.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
 primaryclass = {cs},
 eprint = {2210.02747},
 month = {February}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>Ahmed Touati</fr:author>
              <fr:author>Yann Ollivier</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2021</fr:year>
              <fr:month>10</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/touatiLearningOneRepresentation2021/</fr:uri>
            <fr:display-uri>touatiLearningOneRepresentation2021</fr:display-uri>
            <fr:route>/touatiLearningOneRepresentation2021/</fr:route>
            <fr:title text="Learning One Representation to Optimize All Rewards">Learning One Representation to Optimize All Rewards</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="doi">10.48550/arXiv.2103.07945</fr:meta>
            <fr:meta name="external">https://arxiv.org/abs/2103.07945</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{touatiLearningOneRepresentation2021,
 title = {Learning {{One Representation}} to {{Optimize All Rewards}}},
 author = {Touati, Ahmed and Ollivier, Yann},
 year = {2021},
 doi = {10.48550/arXiv.2103.07945},
 urldate = {2024-09-18},
 number = {arXiv:2103.07945},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/FN8MPGES/Touati and Ollivier - 2021 - Learning One Representation to Optimize All Rewards.pdf;/home/kellen/Downloads/pdfs/storage/SXVNRKWC/2103.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
 archiveprefix = {arXiv},
 abstract = {We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.},
 primaryclass = {cs, math},
 eprint = {2103.07945},
 month = {October}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>Ofir Nachum</fr:author>
              <fr:author>Shixiang Gu</fr:author>
              <fr:author>Honglak Lee</fr:author>
              <fr:author>Sergey Levine</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2019</fr:year>
              <fr:month>1</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/nachumNearOptimalRepresentationLearning2019/</fr:uri>
            <fr:display-uri>nachumNearOptimalRepresentationLearning2019</fr:display-uri>
            <fr:route>/nachumNearOptimalRepresentationLearning2019/</fr:route>
            <fr:title text="Near-Optimal Representation Learning for Hierarchical Reinforcement Learning">Near-Optimal Representation Learning for Hierarchical Reinforcement Learning</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="external">https://arxiv.org/abs/1810.01257</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{nachumNearOptimalRepresentationLearning2019,
 title = {Near-{{Optimal Representation Learning}} for {{Hierarchical Reinforcement Learning}}},
 author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
 year = {2019},
 urldate = {2024-11-07},
 number = {arXiv:1810.01257},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/SIK848RC/Nachum et al. - 2019 - Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.pdf},
 keywords = {Computer Science - Artificial Intelligence},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).},
 primaryclass = {cs},
 eprint = {1810.01257},
 month = {January}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>6</fr:month>
              <fr:day>12</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/007S/</fr:uri>
            <fr:display-uri>007S</fr:display-uri>
            <fr:route>/007S/</fr:route>
            <fr:title text="Weeknotes">Weeknotes</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>Here I will document my progress for the week along with other miscellaneous things I feel like disclosing publicly.</html:p>
            <fr:tree show-metadata="true" expanded="false" numbered="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:uri>https://kellenkanarios.com/2025-W29/</fr:uri>
                <fr:display-uri>2025-W29</fr:display-uri>
                <fr:route>/2025-W29/</fr:route>
                <fr:title text="Weekly Review 2025-W29">Weekly Review 2025-W29</fr:title>
              </fr:frontmatter>
              <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="The Quantified Me">The Quantified Me</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>This section is in large part to gaslight myself on the internet into being more productive but with the information provided let us introspect on what happened this week. </html:p>
  <html:p><html:strong>Coding:</html:strong> I will now proceed to justify and provide excuses on what went wrong this week. On Monday, we see that I spent a large part of my time on this forest. However, this was actually writing up my notes on <fr:link href="/ESQ3/" title="CS336 Lecture 3" uri="https://kellenkanarios.com/ESQ3/" display-uri="ESQ3" type="local">lecture 3</fr:link> of <fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>. My goal of roughly 1ish hour of miscellaneous learning a day snowballed into what was seemingly <fr:tex display="inline"><![CDATA[4]]></fr:tex> hours of random activity, where I spiraled into <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link>. I am thinking of putting this self-study on the backburner in favor of focusing on <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">three easy pieces</fr:link> until the semester starts. I intend to take two courses on what is essentially LLMs systems this fall (very excited), so I might as well not double dip now. I am thinking about using projects 1 and 2 of this course to develop my own working LLM implementation. I can then use it as reference to try out and learn any new optimization tricks like <fr:link href="/W4YP/" title="Multi Query Attention" uri="https://kellenkanarios.com/W4YP/" display-uri="W4YP" type="local">MQA</fr:link> or <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, etc. 
  <html:p>These plots also finally confirm what I noticed as a general trend in my working life. Namely, I spend a lot of time working Monday-Thu because of my weekly meetings with my advisor on Thursday. However, I do not properly make use of the rest of the week. This is a trend I hope to improve upon in future entries.</html:p></html:p>
<html:figure><html:img width="100%" src="/bafkrmibvtysmzc5gyedcfaj2sxjpviob6lxrcdiontte4cg4zu3cueiv7u.png" />
  <html:figcaption>Wakatime stats for week 2025-W29.</html:figcaption></html:figure>
  <html:p><html:strong>Rest of (computer) life:</html:strong> This week I did a pretty good job staying on task (at least on my computer). Very little brainrot was consumed and a decent amount of work was accomplished. Tune in next week to see if we can keep that up!
  </html:p>
<html:figure><html:img width="100%" src="/bafkrmie3tohm4dhy7zqt47zdirbm4xxk432rr4n3yncjdggmq4zetriwku.png" />
  <html:figcaption>Arbtt-stats for week 2025-W29.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="VO2 Max(ing)">VO2 Max(ing)</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ul><html:li><html:strong>Current VO2 Max</html:strong>: 53</html:li>
    <html:li><html:strong>Weekly Mileage</html:strong>: 17.2</html:li></html:ul>
  <html:p>Unfortunately, I missed one run this week. The same one I miss every week: Sunday. I will still blame the shin splints, but this may be the last week I can do so (finally!). I am still maintaining a pretty good SPM, but I need to get some more lower HR runs in. From here on out, we should not dip below 20 miles a week. I have some travel coming up at <fr:link href="/rlc/" title="Reinforcement Learning Conference" uri="https://kellenkanarios.com/rlc/" display-uri="rlc" type="local">RLC</fr:link>, but I should be able to still run while I am there.</html:p>
<html:figure><html:img width="100%" src="/bafkrmicxnrelkrspghbameeb74obgst3bk2dj5vlirqeq73qymd7jsjuny.png" />
  <html:figcaption>Running stats for this week.</html:figcaption></html:figure>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:title text="Goals for next week">Goals for next week</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>This is another section, where I gaslight myself into some accountability on the internet.</html:p>
<html:p>For my imaginary audience, you should have a lot to look forward to in the next week. Namely, 
<html:ol><html:li>Look forward to an in-depth blog on <fr:link href="/lipmanFlowMatchingGenerative2023/" title="Flow Matching for Generative Modeling" uri="https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/" display-uri="lipmanFlowMatchingGenerative2023" type="local">flow matching</fr:link> to accompany my <fr:link href="/FMTD/" title="Flow Matching and TD Flows" uri="https://kellenkanarios.com/FMTD/" display-uri="FMTD" type="local">slides</fr:link>.</html:li>
<html:li>I also should have two new <fr:link href="/005G/" title="Research Bible" uri="https://kellenkanarios.com/005G/" display-uri="005G" type="local">research bible</fr:link> entries one on the <fr:link href="/touatiLearningOneRepresentation2021/" title="Learning One Representation to Optimize All Rewards" uri="https://kellenkanarios.com/touatiLearningOneRepresentation2021/" display-uri="touatiLearningOneRepresentation2021" type="local">forward backward representation</fr:link> and the other on <fr:link href="/nachumNearOptimalRepresentationLearning2019/" title="Near-Optimal Representation Learning for Hierarchical Reinforcement Learning" uri="https://kellenkanarios.com/nachumNearOptimalRepresentationLearning2019/" display-uri="nachumNearOptimalRepresentationLearning2019" type="local">hierarchical rl</fr:link>.</html:li>
<html:li>For my <fr:link href="/0084/" title="Notebook: Three Easy Pieces" uri="https://kellenkanarios.com/0084/" display-uri="0084" type="local">OS adventure</fr:link>, you should see me (hopefully) get through memory virtualization.
However, I should definitely get to at least the TLB. Unlikely, I will continue to do all the exercises (because I spend more time writing them down than doing them). I might do one or two of the projects, but they are quite intimidating TBD.</html:li></html:ol>
Check back in next week to see if I am productive! Also TBD if in the future I will put research-related stuff in this review.
</html:p>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>14</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/W4YP/</fr:uri>
            <fr:display-uri>W4YP</fr:display-uri>
            <fr:route>/W4YP/</fr:route>
            <fr:title text="Multi Query Attention">Multi Query Attention</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>13</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/FMTD/</fr:uri>
            <fr:display-uri>FMTD</fr:display-uri>
            <fr:route>/FMTD/</fr:route>
            <fr:title text="Flow Matching and TD Flows"><fr:link href="/lipmanFlowMatchingGenerative2023/" title="Flow Matching for Generative Modeling" uri="https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/" display-uri="lipmanFlowMatchingGenerative2023" type="local">Flow Matching</fr:link> and <fr:link href="/farebrotherTemporalDifferenceFlows2025/" title="Temporal Difference Flows" uri="https://kellenkanarios.com/farebrotherTemporalDifferenceFlows2025/" display-uri="farebrotherTemporalDifferenceFlows2025" type="local">TD Flows</fr:link></fr:title>
            <fr:meta name="slides">/bafkrmie5b7t53zb2qsrluxkeuaeklf6cq7kb6aqdio56mvuop2jggrq6fa.pdf</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>13</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/ESQ3/</fr:uri>
            <fr:display-uri>ESQ3</fr:display-uri>
            <fr:route>/ESQ3/</fr:route>
            <fr:title text="CS336 Lecture 3"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 3</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate practice</fr:link> (5/66).</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4WB/</fr:uri><fr:display-uri>W4WB</fr:display-uri><fr:route>/W4WB/</fr:route><fr:title text="Architecture Variations">Architecture Variations</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Norms">Norms</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:strong>Pre-norm vs. Post-norm</html:strong>: The first architecture variation discussed is when to apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link>. As can be seen in the figure below, rather than apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> in the residual stream. They instead place it before the FFN and MHA layers.</html:p>
<html:figure><html:img width="40%" src="/bafkrmiawz4eenenwuvnx3pzxjniwghlortvgc5ytxzq5z5oyznugozl22y.png" />
<html:figcaption>Post norm (a) vs. pre norm (b).</html:figcaption></html:figure>
<html:p>They actually found that adding <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer normalization</fr:link> before and after the MHA and FFN works the best. This is known as <html:em>double norm</html:em>.</html:p>
<html:p><html:strong><fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> vs. <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link></html:strong>:
Another useful trick is to use the <fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> instead of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link>.
</html:p>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/1XNO/</fr:uri><fr:display-uri>1XNO</fr:display-uri><fr:route>/1XNO/</fr:route><fr:title text="RMS Norm">RMS Norm</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The RMS norm is simply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> without the added bias term and centering i.e. 
<fr:tex display="block"><![CDATA[\mathrm {RMSNorm}: \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \frac {\mathbf {x}}{\sqrt {\mathrm {Var}[\mathbf {x}_i] + \epsilon }}*\gamma .]]></fr:tex>
This is used to spare memory movement of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layernorm</fr:link>, where the bias term <fr:tex display="inline"><![CDATA[\beta  \in  \mathbb {R}^d]]></fr:tex> and has been found to be empirically almost if not as good.
</html:p></fr:mainmatter></fr:tree>
<html:p>One might wonder if matrix multiplies are the only thing that matters what can such a small change really accomplish.</html:p>
<html:ul><html:li>Due to memory movement, despite being .17<![CDATA[%]]> of <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link>, provided 25<![CDATA[%]]> speedup in runtime!!</html:li></html:ul>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Most people apparently just drop the bias term and keep the centering i.e. subtracting the mean. This makes sense because computing the mean does not require loading any additional information back to memory.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Activations">Activations</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Despite a long list of activations, the two focused on are <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> and <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link>.</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/BEA2/</fr:uri><fr:display-uri>BEA2</fr:display-uri><fr:route>/BEA2/</fr:route><fr:title text="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>rectified linear unit</html:em>(ReLU) is defined as
<fr:tex display="block"><![CDATA[\mathrm {ReLU} : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \max (0, \mathbf {x}),]]></fr:tex>
where the <fr:tex display="inline"><![CDATA[\max ]]></fr:tex> is done elementwise i.e. <fr:tex display="inline"><![CDATA[\mathrm {ReLU}(\mathbf {x})_i = \max (0, x_i)]]></fr:tex>
This provides "nice" gradients, making it common when training neural networks.
</html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/5SPM/</fr:uri><fr:display-uri>5SPM</fr:display-uri><fr:route>/5SPM/</fr:route><fr:title text="Gaussian Error Linear Unit (GELU)">Gaussian Error Linear Unit (GELU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Gaussian Error Linear Unit</html:em> (GELU) is a slight modification to the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> to account for the non-differentiability at <fr:tex display="inline"><![CDATA[0]]></fr:tex>. Namely,
<fr:tex display="block"><![CDATA[\mathrm {GELU}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} \cdot  \psi (\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[\psi (\mathbf {x}) = \mathrm {CDF}(\mathcal {N}(\mathbf {x}, \mathbf {I}))]]></fr:tex></html:p><html:figure><html:img width="70%" src="/bafkrmian7h7ke2dp6nqdq4624wx4off5lk346f7d5pp6mv7bbimcy7azni.png" />
<html:figcaption>GELU vs. ReLU activation and derivative. Taken from <fr:link href="https://www.baeldung.com/cs/gelu-activation-function" type="external">here</fr:link>.</html:figcaption></html:figure></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/OZD8/</fr:uri><fr:display-uri>OZD8</fr:display-uri><fr:route>/OZD8/</fr:route><fr:title text="Swish Activation">Swish Activation</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Swish</html:em> activation is given by
<fr:tex display="block"><![CDATA[\mathrm {Swish}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} s(\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[s : \mathbb {R}^d \to  \mathbb {R}^d]]></fr:tex> is the <fr:link href="/NNDS/" title="Sigmoid" uri="https://kellenkanarios.com/NNDS/" display-uri="NNDS" type="local">sigmoid function</fr:link>.
</html:p></fr:mainmatter></fr:tree>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  The extra computation required by <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link> or <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> is a non-factor because <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> are dominated by matrix multiplication and there is no increase in memory pressure.
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/K2N7/</fr:uri><fr:display-uri>K2N7</fr:display-uri><fr:route>/K2N7/</fr:route><fr:title text="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>A <html:em>gated linear unit</html:em> is an activation function combined with an elementwise multiplication i.e. 
<html:ul><html:li><fr:tex display="inline"><![CDATA[\mathrm {ReGLU} = \mathrm {ReLU}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li>
  <html:li><fr:tex display="inline"><![CDATA[\mathrm {SwiGLU} = \mathrm {Swish}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li></html:ul>
where <fr:tex display="inline"><![CDATA[\mathrm {ReLU}]]></fr:tex> is the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> activation and <fr:tex display="inline"><![CDATA[\mathrm {Swish}]]></fr:tex> is the <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> activation respectively.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The matrix <fr:tex display="inline"><![CDATA[\mathbf {V}]]></fr:tex> introduces additional learnable parameters. Therefore, an architecture that uses gating typically reduces their other parameters by a factor of <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>.
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Serial vs. Parallel">Serial vs. Parallel</fr:title></fr:frontmatter><fr:mainmatter>
Another small trick is to parallelize computation. Traditionally, one computes the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> output and then passes this to the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> i.e.
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x} + \mathrm {MHA}(\mathrm {LN}(\mathbf {x})))).]]></fr:tex>
However, this requires waiting for the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> to complete before performing the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> computation. It has been found that performing these in parallel does not cause any severe degradation in performance. Explicitly, instead they do
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x})) + \mathrm {MHA}(\mathrm {LN}(\mathbf {x}))]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/TOD6/</fr:uri><fr:display-uri>TOD6</fr:display-uri><fr:route>/TOD6/</fr:route><fr:title text="Rotary Position Embeddings (ROPE)">Rotary Position Embeddings (ROPE)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Notation</html:strong>:
For this section, our notion of an <html:em>embedding</html:em> is a function <fr:tex display="inline"><![CDATA[f]]></fr:tex> that takes a token and a position i.e. <fr:tex display="inline"><![CDATA[f(x, i)]]></fr:tex> is the embedding of token <fr:tex display="inline"><![CDATA[x]]></fr:tex>, occurring at position <fr:tex display="inline"><![CDATA[i]]></fr:tex>.
</html:p><html:p><html:strong>Idea:</html:strong> the <fr:link href="/B1RI/" title="Inner Product" uri="https://kellenkanarios.com/B1RI/" display-uri="B1RI" type="local">inner product</fr:link> of two embeddings should only need relative positioning i.e. for any two <fr:tex display="inline"><![CDATA[\mathbf {x}]]></fr:tex>, <fr:tex display="inline"><![CDATA[\mathbf {y}]]></fr:tex> with positions <fr:tex display="inline"><![CDATA[i]]></fr:tex> and <fr:tex display="inline"><![CDATA[j]]></fr:tex> respectively, there should exist some <fr:tex display="inline"><![CDATA[g]]></fr:tex>, such that
<fr:tex display="block"><![CDATA[\langle  f(\mathbf {x}, i), f(\mathbf {y}, j) \rangle  = g(\mathbf {x}, \mathbf {y}, i - j)]]></fr:tex>
Namely, this can be written as a function of just the relative positioning between the embeddings.
</html:p><html:p>The simplest transformation that only preserves this relative information are <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link>.</html:p><html:figure><html:img width="50%" src="/bafkrmiba4muc7qqk2agccjalqpm24hxw7u4gkfpozmnhz7if6ncagxpsgi.png" />
<html:figcaption>Rotating embeddings does not change relative position.</html:figcaption></html:figure><html:p>However, <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link> are only easily defined for <fr:tex display="inline"><![CDATA[\mathbb {R}^2]]></fr:tex>. To get around this, they simply partition their input 2d slices and apply the rotation via
<fr:tex display="block"><![CDATA[\mathrm {ROT} = \begin {bmatrix} \cos  m \theta _{1} & - \sin  m \theta _{1} & & & \\
\sin  m \theta _{1} & \cos  m \theta _{1} & & & \\
& & \cos  m \theta _{2} & -\sin  m \theta _{2} & \\
& & \sin  m \theta _{2} & \cos  m \theta _{2} & \\
& & & & \ddots 
\end {bmatrix} ]]></fr:tex></html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The <fr:tex display="inline"><![CDATA[\theta _i]]></fr:tex>'s are fixed and chosen according to some schedule to capture different "frequencies". 
</fr:mainmatter></fr:tree>
<html:p>The embedding are used by appling them to the query and key matrices separately i.e. 
<fr:tex display="block"><![CDATA[W_K = \mathrm {ROT}(W_K), \quad  W_Q = \mathrm {ROT}(W_Q)]]></fr:tex>
These are then what is used in self-attention.
</html:p></fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/9VZH/</fr:uri><fr:display-uri>9VZH</fr:display-uri><fr:route>/9VZH/</fr:route><fr:title text="Hyperparameter Rules (CS336)">Hyperparameter Rules (<fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Rule 1</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {ff}} = d_{\text {model}}]]></fr:tex>
<html:ul><html:li><fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex> is the input dimension</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d_{\text {ff}}]]></fr:tex> is the hidden dimension</html:li></html:ul></html:p><html:p><html:strong>Exception 1</html:strong>: in the case of <fr:link href="/K2N7/" title="Gated Linear Unit (GLU)" uri="https://kellenkanarios.com/K2N7/" display-uri="K2N7" type="local">GLUs</fr:link>, we recall that to account for extra parameters we scale <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>. Therefore, <fr:tex display="inline"><![CDATA[d_{\text {ff}} = \frac {8}{3}d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Exception 2</html:strong>: In T5, they use <fr:tex display="inline"><![CDATA[d_{\text {ff}} = 64d_{\text {model}}]]></fr:tex>. The logic for this was that we could maximize the <fr:link href="/99G8/" title="Model FLOP Utilization (MFU)" uri="https://kellenkanarios.com/99G8/" display-uri="99G8" type="local">MFU</fr:link> by increasing matrix size. Ended up going back to small <fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Rule 2</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {head}} = d_{\text {model}} / \text {num heads}]]></fr:tex>
<html:ul><html:li>I believe the <fr:tex display="inline"><![CDATA[d_{\text {head}}]]></fr:tex> is the output dim of each head.</html:li>
  <html:li>This just says the total output dim for <fr:tex display="inline"><![CDATA[n]]></fr:tex> heads is the same as if we did one head.</html:li>
  <html:li>Means there is something important about splitting up the heads.</html:li></html:ul></html:p><html:p><html:strong>Rule 3</html:strong>: Aspect ratio = <fr:tex display="inline"><![CDATA[\frac {d_{\text {model}}}{n_{\text {layer}}} \approx  100-200]]></fr:tex>
<html:ul><html:li>Pipeline dependent: if network speed is fast than parallelizing is easier and shallow networks are better.</html:li>
  <html:li>If poor network speed then pipeline parallel might be more viable and deeper networks would be more parallelizable.</html:li></html:ul></html:p><html:p><html:strong>Rule 4:</html:strong> Vocab size
<html:ul><html:li>For single language models, vocab size is typically <fr:tex display="inline"><![CDATA[30-50k]]></fr:tex></html:li>
  <html:li>For multi language models, vocab size is typically <fr:tex display="inline"><![CDATA[100-250k]]></fr:tex></html:li></html:ul></html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Regularization and Dropout">Regularization and Dropout</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Since there is so much data, it is not feasible to train your model for multiple epochs. This is actually nice in the sense that we do not have to worry about overfitting and performing regularization.</html:p>
<html:p>However, many of the large models are still trained with weight decay. Tatsu claims that this is not to do with regularization but actually due to some weird interaction with the learning rate schedule. I am not sure I entirely understood this part.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Stability Tricks">Stability Tricks</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link> is ill-behaved due to the exponentials. In the transformer, we have two softmaxes: one at the end and one in the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>.
<html:ul><html:li>For the first, note <fr:tex display="block"><![CDATA[\log  \sigma (\mathbf {x})_i = \log (x_i) - \log  \underbrace {\sum _{j=1}^{d} e^{x_j}}_{D(\mathbf {x})}]]></fr:tex>
   The only problem is the denominator term. The idea is to enforce the <fr:tex display="inline"><![CDATA[D(\mathbf {x}) = 1]]></fr:tex> by regularizing via a penalty on <fr:tex display="inline"><![CDATA[\log  D(\mathbf {x})]]></fr:tex> ie.
<fr:tex display="block"><![CDATA[\mathcal {L}_{\text {aux}} = 10^{-4} \log ^2(D(\mathbf {x}))]]></fr:tex>
With the <fr:tex display="inline"><![CDATA[\nabla  D(\mathbf {x})]]></fr:tex> should be <fr:tex display="inline"><![CDATA[0]]></fr:tex> and we are effectively only considering the non-exponential term.
   </html:li>
   <html:li>For the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>, they primarily operate on the <html:em>logits</html:em> prior to the softmax. One way is via <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> before the <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link>. Another is via <html:em>softcapping</html:em> i.e.
   <fr:tex display="block"><![CDATA[\mathrm {logits} = \mathrm {softcap} \cdot  \tanh  \left ( \frac {\mathrm {logits}}{\mathrm {softcap}} \right ) ]]></fr:tex></html:li></html:ul></html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Attention Variants">Attention Variants</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4YP/</fr:uri><fr:display-uri>W4YP</fr:display-uri><fr:route>/W4YP/</fr:route><fr:title text="Multi Query Attention">Multi Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0085/</fr:uri>
            <fr:display-uri>0085</fr:display-uri>
            <fr:route>/0085/</fr:route>
            <fr:title text="Notebook: Stanford CS336">Notebook: <fr:link href="/stanford/" title="Stanford University" uri="https://kellenkanarios.com/stanford/" display-uri="stanford" type="local">Stanford</fr:link> <fr:link href="https://stanford-cs336.github.io/spring2025/index.htmlschedule" type="external">CS336</fr:link></fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>Formal notebook for my journey in <fr:link href="/stanford/" title="Stanford University" uri="https://kellenkanarios.com/stanford/" display-uri="stanford" type="local">Stanford's</fr:link> <fr:link href="https://stanford-cs336.github.io/spring2025/index.htmlschedule" type="external">CS336</fr:link> as part of my <fr:link href="/003W/" title="LLMs" uri="https://kellenkanarios.com/003W/" display-uri="003W" type="local">LLM</fr:link> self-study endeavors.</html:p>
            <fr:tree show-metadata="false" expanded="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>1</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/0086/</fr:uri>
                <fr:display-uri>0086</fr:display-uri>
                <fr:route>/0086/</fr:route>
                <fr:title text="CS336 Lecture 1"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 1</fr:title>
              </fr:frontmatter>
              <fr:mainmatter>
                <html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (2/66)</html:p>
                <html:p>First they give a brief outline of the course (provided below) along with some of the challenges and problem definitions for each of the sections</html:p>
                <fr:tree show-metadata="false">
                  <fr:frontmatter>
                    <fr:authors />
                    <fr:date>
                      <fr:year>2025</fr:year>
                      <fr:month>7</fr:month>
                      <fr:day>3</fr:day>
                    </fr:date>
                    <fr:uri>https://kellenkanarios.com/DEQT/</fr:uri>
                    <fr:display-uri>DEQT</fr:display-uri>
                    <fr:route>/DEQT/</fr:route>
                    <fr:title text="Course outline">Course outline</fr:title>
                  </fr:frontmatter>
                  <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Basics">Basics</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Tokenization</html:li>
  <html:li>Architecture</html:li>
  <html:li>Loss function</html:li>
  <html:li>Optimizer</html:li>
  <html:li>Learning rate</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Systems">Systems</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> Squeeze the most out of the hardware. </html:p>
<html:ol><html:li><html:strong>Kernels</html:strong></html:li>
  <html:p>Basic idea is that the GPU needs to perform computation, but the data cannot fit on the GPU. GPU is the <html:em>factory</html:em> memory is the <html:em>warehouse</html:em></html:p>
  
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
    <html:em>How do we best organize computation to maximize utilization of GPUs by minimizing data movement?</html:em>
  </html:div>

  <html:li><html:strong>Parallelism</html:strong></html:li>
  <html:p>Beyond one GPU, we must then learn how to scale these ideas to multiple GPUs.</html:p>
  <html:ul><html:li>Sharding</html:li>
    <html:li>{data,tensor,piple,sequence} parallelism</html:li>
    <html:li>Quantization</html:li>
    <html:li>Activation checkpointing</html:li>
    <html:li>CPU offloading</html:li></html:ul>
  <html:li><html:strong>Inference</html:strong></html:li>
  <html:p>Two phases: prefill and decode</html:p>
  <html:ul><html:li>In prefill all tokens are given and you just want to output the next coding (compute bound).</html:li>
    <html:li>In decoding, we need to output one token at a time (memory-bound). </html:li></html:ul></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Scaling Laws">Scaling Laws</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> do experiments at small scale in order to pick hyperparameters for much more expensive runs at large scale.</html:p>
<html:ol><html:li>Scaling sequence</html:li>
  <html:li>Model complexity</html:li>
  <html:li>Loss metric</html:li>
  <html:li>Parametric form</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Data">Data</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> what we want the model to actually do i.e. pick your data for your task.</html:p>
<html:ol><html:li>Evaluation</html:li>
  <html:li>Curation</html:li>
  <html:li>Transformation</html:li>
  <html:li>Filtering</html:li>
  <html:li>Deduplication</html:li>
  <html:li>Mixing</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Alignment">Alignment</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Supervised fine-tuning</html:li>
  <html:li>Reinforcement learning</html:li>
  <html:li>Preference data</html:li>
  <html:li>Synthetic data</html:li>
  <html:li>Verifiers</html:li></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
                </fr:tree>
                <fr:tree show-metadata="false">
                  <fr:frontmatter>
                    <fr:authors>
                      <fr:author>
                        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                      </fr:author>
                    </fr:authors>
                    <fr:date>
                      <fr:year>2025</fr:year>
                      <fr:month>7</fr:month>
                      <fr:day>1</fr:day>
                    </fr:date>
                    <fr:uri>https://kellenkanarios.com/0087/</fr:uri>
                    <fr:display-uri>0087</fr:display-uri>
                    <fr:route>/0087/</fr:route>
                    <fr:title text="Tokenization">Tokenization</fr:title>
                  </fr:frontmatter>
                  <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Introduction">Introduction</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
A <html:em>tokenizer</html:em> is responsible for taking text and turning it into a numerical representation we can pass to a neural network i.e.
<fr:tex display="block"><![CDATA[\mathsf {encode}(\text {hello world}) \mapsto  \begin {bmatrix} 24912, & 2375 \end {bmatrix} ]]></fr:tex>
    However, we also need 
    <fr:tex display="block"><![CDATA[\mathsf {decode}(\begin {bmatrix} 24912, & 2375 \end {bmatrix}) \mapsto  \text {hello world} 
    ]]></fr:tex>
 When defining a "good" tokenizer, we are concerned with the <html:em>compression ratio</html:em> i.e. the # of bytes per token.
    </html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Character-based tokenization">Character-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Trivially, we can do <html:em>character-based tokenization</html:em> i.e. mapping each character to a unique number. For example, <fr:tex display="inline"><![CDATA[\mathsf {chr}(97) = \text {``a''}]]></fr:tex> and <fr:tex display="inline"><![CDATA[\mathsf {ord}(\text {``a''}) = 97]]></fr:tex>. The compression ratio will be <fr:tex display="inline"><![CDATA[> 1]]></fr:tex> because each token corresponds to a character potentially consisting of more than one byte.</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problems:</html:strong>
  <html:ol><html:li>Very large vocabulary</html:li>
    <html:li>Many characters are quite rare.</html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
There is a fundamental <html:strong>tradeoff</html:strong> between compression ratio and vocabulary size. As an example, suppose we map every sequence of <fr:tex display="inline"><![CDATA[n]]></fr:tex> bytes to a token. This would achieve a compression ratio of <fr:tex display="inline"><![CDATA[n]]></fr:tex>. However, this would require <fr:tex display="inline"><![CDATA[256^n]]></fr:tex> tokens in our vocabulary.
</fr:mainmatter></fr:tree>

<html:p>
  Problem 2 was not entirely straight-forward to me. This actually reminds me of an idea from my <fr:link href="/004B/" title="Notebook: Information Theory" uri="https://kellenkanarios.com/004B/" display-uri="004B" type="local">information theory</fr:link> course, where we can assume we are given a "prior" over human language then we want to map low-probability characters to large strings in order to minimize the expected length (see <fr:link href="/005Z/" title="Huffman Code" uri="https://kellenkanarios.com/005Z/" display-uri="005Z" type="local">huffman coding</fr:link>).
</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte-based tokenization">Byte-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>In byte-based tokenization, each byte is a token achieving a compression ratio of exactly <fr:tex display="inline"><![CDATA[1]]></fr:tex>, where a byte <fr:tex display="inline"><![CDATA[\mapsto  [0, 256]]]></fr:tex>
. This is very elegant in the sense that our vocabulary size is fixed at <fr:tex display="inline"><![CDATA[256]]></fr:tex>.
</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong>
However, with this, the length of our encoding is # of bytes in the string, which will become too long.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Word-based tokenization">Word-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
In <html:em>word-based tokenization</html:em>, we simply split the string into the corresponding words then map each of these to a unique integer.
    </html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong> The vocabulary size is "unbounded" i.e. if we see a word we have not seen before then we need to extend the vocabulary size.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte Pair Encoding">Byte Pair Encoding</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>For the <html:em>byte pair encoding</html:em> (BPE), we instead <html:em>train</html:em> the tokenizer on raw text to automatically determine the vocabulary.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In GPT2, they first split the raw text into words and then do this byte pair encoding.
</fr:mainmatter></fr:tree>

<html:p><html:strong>Algorithm:</html:strong></html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
<html:em>while</html:em> <html:strong>true</html:strong>:
<html:ol><html:li>Count occurences of byte pairs. Store in dict <fr:tex display="inline"><![CDATA[D]]></fr:tex> with <fr:tex display="inline"><![CDATA[\{(b_1, b_2): \text {count}\}]]></fr:tex></html:li>
  <html:li>Find <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> with highest count.</html:li>
  <html:li>Merge <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> by <fr:tex display="inline"><![CDATA[(b_{1}, b_{2}) \mapsto  b']]></fr:tex></html:li>
  <html:li>Replace all instances of <fr:tex display="inline"><![CDATA[(b_{1}, b_{2})]]></fr:tex> in string with <fr:tex display="inline"><![CDATA[b']]></fr:tex> i.e. 
  <fr:tex display="block"><![CDATA[b_{1} b_{2} b_{3} b_{4} \mapsto  b'b_{3} b_{4}]]></fr:tex></html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  There are a few additional implementation details to consider for project 1.
<html:ol><html:li><fr:tex display="inline"><![CDATA[\mathsf {encode}]]></fr:tex> loops over all merges. Only loop over merges that matter.</html:li>
  <html:li>Detect and preserve special tokens (e.g. &lt;|endoftext|&gt;)</html:li>
  <html:li>Use pre-tokenization (GPT2 regex)</html:li></html:ol>
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>
</fr:mainmatter>
                </fr:tree>
              </fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false" expanded="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>3</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/DEQS/</fr:uri>
                <fr:display-uri>DEQS</fr:display-uri>
                <fr:route>/DEQS/</fr:route>
                <fr:title text="CS336 Lecture 2"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 2</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (4/66)</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Motivating Questions">Motivating Questions</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>We start by discussing a few essential questions one hopes to answer when they set out to train a large model.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>How long to train a 70b parameter model on 15T tokens on 1024 H100s?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>Mysteriously, we spawn from the ether (will be explained later) that <html:code>total_flops = 6 * 70e9 * 15e12</html:code>. Intuitively, we know that we need to perform some computation for each parameter for each token. However, the 6 is still an ominous mystery. Given the total flops, we then can just take the GPUs FLOP/s multiply by the number of seconds in the day and divide <html:code>total_flops</html:code> by this number i.e. 
<html:code>days = total_flops / flops_per_day</html:code>
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In practice, we must also consider the <html:em>model flop utilization</html:em> (MFU). Basically, our algorithm cannot utilize the entirety of the maximum FLOP/s provided by the hardware due to things like memory bandwidth, etc. To account for this in the calculation, they just take <html:code>flops_per_day / 2</html:code>.
</fr:mainmatter></fr:tree>



    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  What is the largest model that you can train on 8 H100s using AdamW?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  An H100 has 80gb of shared memory i.e. <html:code>h100_bytes = 80e9</html:code>. The calculation then proceeds as
  <html:pre><![CDATA[# parameters, gradients, optimizer state
bytest_per_param = 4 + 4 + (4 + 4)
num_parameters = (h100_bytes * 8) / bytes_per_parameter]]></html:pre>
  what we need each of parameters, gradients, optimizer state will become clear later.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In practice, we also need to account for activations, which will depend on <html:strong>both</html:strong> batch size and sequence length.
</fr:mainmatter></fr:tree>

  </fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:uri>https://kellenkanarios.com/DEQU/</fr:uri><fr:display-uri>DEQU</fr:display-uri><fr:route>/DEQU/</fr:route><fr:title text="Memory Accounting">Memory Accounting</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Tensor Memory</html:strong>:
Tensors are stored as a <html:em>floating point number</html:em>. Memory is determined by (i) number of values in tensor and (ii) data type of each value.
</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:uri>https://kellenkanarios.com/DEQV/</fr:uri><fr:display-uri>DEQV</fr:display-uri><fr:route>/DEQV/</fr:route><fr:title text="Low Precision Data Types">Low Precision Data Types</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:code>float16</html:code>: here, we decrease both the fraction and exponent bits. Importantly, there are only <fr:tex display="inline"><![CDATA[5]]></fr:tex> exponent bits making the effective range <fr:tex display="inline"><![CDATA[(2^{-6}, 2^{6})]]></fr:tex>. The dynamic range for small numbers is pretty bad. This can cause small numbers to "die" effectively killing these neurons for the remainder of learning due to backpropagation.
</html:p><html:p><html:code>bfloat16</html:code>: to fix the problems with <html:code>float16</html:code>, they instead keep <fr:tex display="inline"><![CDATA[8]]></fr:tex> exponent bits and reduce the fraction bits to <fr:tex display="inline"><![CDATA[7]]></fr:tex>. This maintains the same dynamic range as <html:code>float32</html:code> while still only using <fr:tex display="inline"><![CDATA[16]]></fr:tex> bits. 
</html:p><html:p><html:code>fp8</html:code>: Two variants
<html:ol><html:li>E4M3: has 4 exponent bits and 3 mantissa bits</html:li> 
<html:li>E5M2: has 5 exponent bits and 2 mantissa bits</html:li></html:ol>
Requires modern Hopper Architecture i.e. H100 to perform operations. Also supported by TPUs.
</html:p></fr:mainmatter></fr:tree><html:p><html:strong>Training Implications</html:strong>
<html:ol><html:li><html:code>float32</html:code> training works but requires lots of memory</html:li>
  <html:li>Using <fr:link href="/DEQV/" title="Low Precision Data Types" uri="https://kellenkanarios.com/DEQV/" display-uri="DEQV" type="local">lower precision data types</fr:link> allows you to train larger models due to lower memory requirements, but can cause instability</html:li>
  <html:li>To optimize this tradeoff, use <html:em>mixed-precision training</html:em>.</html:li></html:ol></html:p></fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:uri>https://kellenkanarios.com/DYIV/</fr:uri><fr:display-uri>DYIV</fr:display-uri><fr:route>/DYIV/</fr:route><fr:title text="Torch Tensors">Torch Tensors</fr:title></fr:frontmatter><fr:mainmatter><html:p>
In pytorch, tensors are <html:em>contiguous</html:em> blocks of memory, where their shape is controlled by <html:em>strides</html:em>. A stride basically just refers to how far you have to jump in physical memory to correspond to incrementing the logical index i.e. in a (2, 3) matrix each row requires jumping <fr:tex display="inline"><![CDATA[3]]></fr:tex> spots in physical memory. Thus, <html:code>stride[1] = 3</html:code>.
  </html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Calling <html:code>.view((m, n))</html:code> just changes the stride but does <html:strong>not</html:strong> modify the physical memory. If you call <html:code>.view</html:code> that results in a non-contiguous view, then you cannot call <html:code>.view</html:code> again because it assumes contiguous indexing to properly adjust the strides. To deal with this, they have <html:code>.contiguous()</html:code> and <html:code>.reshape()</html:code> is basically <html:code>.contiguous().view()</html:code>.
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
All <html:strong>elementwise</html:strong> operations make a copy i.e. <html:code>x + y</html:code>. A useful function for masked attention is <html:code>triu()</html:code>.
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:title text="Einops">Einops</fr:title></fr:frontmatter><fr:mainmatter>
Einops is a useful python library to keep track of high-dimensional matrix operations i.e. matmuls over <html:code>batch_size</html:code>, <html:code>seqlen</html:code>, etc. High level overview
<html:ul><html:li>Initialize like <html:code>X[float.tensor, "batch seq hidden"] = torch.tensor((2, 3, 4))</html:code></html:li>
  <html:li>Can perform matmul with named dims like</html:li>
  <html:code>z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2")</html:code>
  <html:li><html:code>reduce</html:code>: <html:code>x = reduce(x, "x y z -&gt; x y 1")</html:code> is just like <html:code>mean(x, axis=-1)</html:code></html:li>
  <html:li><html:code>rearrange</html:code>: <html:code>x = rearrange(x, "... (heads hidden1) -&gt; ... heads hidden 1")</html:code></html:li></html:ul>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Computation">Computation</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>12</fr:day></fr:date><fr:uri>https://kellenkanarios.com/4K5H/</fr:uri><fr:display-uri>4K5H</fr:display-uri><fr:route>/4K5H/</fr:route><fr:title text="Floating Point Operations">Floating Point Operations</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>We refer to the <html:strong>total</html:strong> number of floating point operatings as <html:em>FLOPS</html:em>. Alternatively, we refer to the number of floating point operations per second as <html:em>FLOP/s</html:em>.</html:p></fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>12</fr:day></fr:date><fr:uri>https://kellenkanarios.com/99G8/</fr:uri><fr:display-uri>99G8</fr:display-uri><fr:route>/99G8/</fr:route><fr:title text="Model FLOP Utilization (MFU)">Model FLOP Utilization (MFU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>Given a promised <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOP/s</fr:link>, the <html:em>model flop utilization</html:em> is given by
<fr:tex display="block"><![CDATA[\frac {\text {actual FLOP/s}}{\text {promised FLOP/s}}]]></fr:tex></html:p></fr:mainmatter></fr:tree>

   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
Performing a matrix multiplication <fr:tex display="inline"><![CDATA[A \cdot  B]]></fr:tex> for <fr:tex display="inline"><![CDATA[A \in  \mathbb {R}^{(B \times  D)}]]></fr:tex> and <fr:tex display="inline"><![CDATA[B \in  \mathbb {R}^{(D \times  K)}]]></fr:tex>, requires <fr:tex display="inline"><![CDATA[2 \times  B \times  D \times  K]]></fr:tex> <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link></fr:mainmatter></fr:tree>
 

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Optimizers">Optimizers</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>I hope to do a formal blog touring optimizers because I have written a paper on deep learning optimizers for my <fr:link href="/0033/" title="Notebook: Optimization Theory" uri="https://kellenkanarios.com/0033/" display-uri="0033" type="local">optimization course</fr:link>. Due to this, I will leave this blank for now. A brief list of the covered concepts are below:</html:p>
<html:ul><html:li>momentum</html:li>
  <html:li>adagrad</html:li>
  <html:li>RMSProp</html:li>
  <html:li>Adam</html:li></html:ul>
<html:p>A key formula to remember:
<fr:tex display="block"><![CDATA[\text {total memory} \approx  4 \times  (\text {params} + \text {activations} + \text {grad size} + \text {optimizer state size})]]></fr:tex></html:p>
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Another useful trick is to use <fr:link href="/DEQV/" title="Low Precision Data Types" uri="https://kellenkanarios.com/DEQV/" display-uri="DEQV" type="local">low precision</fr:link> for forward pass activations, then use <html:code>float32</html:code> for gradients and network parameters.
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false" expanded="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>13</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/ESQ3/</fr:uri>
                <fr:display-uri>ESQ3</fr:display-uri>
                <fr:route>/ESQ3/</fr:route>
                <fr:title text="CS336 Lecture 3"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 3</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate practice</fr:link> (5/66).</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4WB/</fr:uri><fr:display-uri>W4WB</fr:display-uri><fr:route>/W4WB/</fr:route><fr:title text="Architecture Variations">Architecture Variations</fr:title></fr:frontmatter><fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Norms">Norms</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:strong>Pre-norm vs. Post-norm</html:strong>: The first architecture variation discussed is when to apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link>. As can be seen in the figure below, rather than apply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> in the residual stream. They instead place it before the FFN and MHA layers.</html:p>
<html:figure><html:img width="40%" src="/bafkrmiawz4eenenwuvnx3pzxjniwghlortvgc5ytxzq5z5oyznugozl22y.png" />
<html:figcaption>Post norm (a) vs. pre norm (b).</html:figcaption></html:figure>
<html:p>They actually found that adding <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer normalization</fr:link> before and after the MHA and FFN works the best. This is known as <html:em>double norm</html:em>.</html:p>
<html:p><html:strong><fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> vs. <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link></html:strong>:
Another useful trick is to use the <fr:link href="/1XNO/" title="RMS Norm" uri="https://kellenkanarios.com/1XNO/" display-uri="1XNO" type="local">RMSNorm</fr:link> instead of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">LayerNorm</fr:link>.
</html:p>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/1XNO/</fr:uri><fr:display-uri>1XNO</fr:display-uri><fr:route>/1XNO/</fr:route><fr:title text="RMS Norm">RMS Norm</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The RMS norm is simply the <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> without the added bias term and centering i.e. 
<fr:tex display="block"><![CDATA[\mathrm {RMSNorm}: \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \frac {\mathbf {x}}{\sqrt {\mathrm {Var}[\mathbf {x}_i] + \epsilon }}*\gamma .]]></fr:tex>
This is used to spare memory movement of <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layernorm</fr:link>, where the bias term <fr:tex display="inline"><![CDATA[\beta  \in  \mathbb {R}^d]]></fr:tex> and has been found to be empirically almost if not as good.
</html:p></fr:mainmatter></fr:tree>
<html:p>One might wonder if matrix multiplies are the only thing that matters what can such a small change really accomplish.</html:p>
<html:ul><html:li>Due to memory movement, despite being .17<![CDATA[%]]> of <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link>, provided 25<![CDATA[%]]> speedup in runtime!!</html:li></html:ul>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Most people apparently just drop the bias term and keep the centering i.e. subtracting the mean. This makes sense because computing the mean does not require loading any additional information back to memory.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Activations">Activations</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Despite a long list of activations, the two focused on are <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> and <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link>.</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/BEA2/</fr:uri><fr:display-uri>BEA2</fr:display-uri><fr:route>/BEA2/</fr:route><fr:title text="Rectified Linear Unit (ReLU)">Rectified Linear Unit (ReLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>rectified linear unit</html:em>(ReLU) is defined as
<fr:tex display="block"><![CDATA[\mathrm {ReLU} : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \max (0, \mathbf {x}),]]></fr:tex>
where the <fr:tex display="inline"><![CDATA[\max ]]></fr:tex> is done elementwise i.e. <fr:tex display="inline"><![CDATA[\mathrm {ReLU}(\mathbf {x})_i = \max (0, x_i)]]></fr:tex>
This provides "nice" gradients, making it common when training neural networks.
</html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/5SPM/</fr:uri><fr:display-uri>5SPM</fr:display-uri><fr:route>/5SPM/</fr:route><fr:title text="Gaussian Error Linear Unit (GELU)">Gaussian Error Linear Unit (GELU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Gaussian Error Linear Unit</html:em> (GELU) is a slight modification to the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> to account for the non-differentiability at <fr:tex display="inline"><![CDATA[0]]></fr:tex>. Namely,
<fr:tex display="block"><![CDATA[\mathrm {GELU}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} \cdot  \psi (\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[\psi (\mathbf {x}) = \mathrm {CDF}(\mathcal {N}(\mathbf {x}, \mathbf {I}))]]></fr:tex></html:p><html:figure><html:img width="70%" src="/bafkrmian7h7ke2dp6nqdq4624wx4off5lk346f7d5pp6mv7bbimcy7azni.png" />
<html:figcaption>GELU vs. ReLU activation and derivative. Taken from <fr:link href="https://www.baeldung.com/cs/gelu-activation-function" type="external">here</fr:link>.</html:figcaption></html:figure></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/OZD8/</fr:uri><fr:display-uri>OZD8</fr:display-uri><fr:route>/OZD8/</fr:route><fr:title text="Swish Activation">Swish Activation</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>The <html:em>Swish</html:em> activation is given by
<fr:tex display="block"><![CDATA[\mathrm {Swish}(\mathbf {x}) : \mathbb {R}^d \to  \mathbb {R}^d, \quad  \mathbf {x} \mapsto  \mathbf {x} s(\mathbf {x}),]]></fr:tex>
where <fr:tex display="inline"><![CDATA[s : \mathbb {R}^d \to  \mathbb {R}^d]]></fr:tex> is the <fr:link href="/NNDS/" title="Sigmoid" uri="https://kellenkanarios.com/NNDS/" display-uri="NNDS" type="local">sigmoid function</fr:link>.
</html:p></fr:mainmatter></fr:tree>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  The extra computation required by <fr:link href="/5SPM/" title="Gaussian Error Linear Unit (GELU)" uri="https://kellenkanarios.com/5SPM/" display-uri="5SPM" type="local">GELU</fr:link> or <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> is a non-factor because <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> are dominated by matrix multiplication and there is no increase in memory pressure.
  </fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/K2N7/</fr:uri><fr:display-uri>K2N7</fr:display-uri><fr:route>/K2N7/</fr:route><fr:title text="Gated Linear Unit (GLU)">Gated Linear Unit (GLU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>A <html:em>gated linear unit</html:em> is an activation function combined with an elementwise multiplication i.e. 
<html:ul><html:li><fr:tex display="inline"><![CDATA[\mathrm {ReGLU} = \mathrm {ReLU}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li>
  <html:li><fr:tex display="inline"><![CDATA[\mathrm {SwiGLU} = \mathrm {Swish}(\mathbf {x}) \otimes  \mathbf {V} \mathbf {x},]]></fr:tex></html:li></html:ul>
where <fr:tex display="inline"><![CDATA[\mathrm {ReLU}]]></fr:tex> is the <fr:link href="/BEA2/" title="Rectified Linear Unit (ReLU)" uri="https://kellenkanarios.com/BEA2/" display-uri="BEA2" type="local">ReLU</fr:link> activation and <fr:tex display="inline"><![CDATA[\mathrm {Swish}]]></fr:tex> is the <fr:link href="/OZD8/" title="Swish Activation" uri="https://kellenkanarios.com/OZD8/" display-uri="OZD8" type="local">Swish</fr:link> activation respectively.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The matrix <fr:tex display="inline"><![CDATA[\mathbf {V}]]></fr:tex> introduces additional learnable parameters. Therefore, an architecture that uses gating typically reduces their other parameters by a factor of <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>.
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Serial vs. Parallel">Serial vs. Parallel</fr:title></fr:frontmatter><fr:mainmatter>
Another small trick is to parallelize computation. Traditionally, one computes the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> output and then passes this to the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> i.e.
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x} + \mathrm {MHA}(\mathrm {LN}(\mathbf {x})))).]]></fr:tex>
However, this requires waiting for the <fr:tex display="inline"><![CDATA[\mathrm {MHA}]]></fr:tex> to complete before performing the <fr:tex display="inline"><![CDATA[\mathrm {FFN}]]></fr:tex> computation. It has been found that performing these in parallel does not cause any severe degradation in performance. Explicitly, instead they do
<fr:tex display="block"><![CDATA[\mathbf {y} = \mathbf {x} + \mathrm {FFN}(\mathrm {LN}(\mathbf {x})) + \mathrm {MHA}(\mathrm {LN}(\mathbf {x}))]]></fr:tex>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/TOD6/</fr:uri><fr:display-uri>TOD6</fr:display-uri><fr:route>/TOD6/</fr:route><fr:title text="Rotary Position Embeddings (ROPE)">Rotary Position Embeddings (ROPE)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Notation</html:strong>:
For this section, our notion of an <html:em>embedding</html:em> is a function <fr:tex display="inline"><![CDATA[f]]></fr:tex> that takes a token and a position i.e. <fr:tex display="inline"><![CDATA[f(x, i)]]></fr:tex> is the embedding of token <fr:tex display="inline"><![CDATA[x]]></fr:tex>, occurring at position <fr:tex display="inline"><![CDATA[i]]></fr:tex>.
</html:p><html:p><html:strong>Idea:</html:strong> the <fr:link href="/B1RI/" title="Inner Product" uri="https://kellenkanarios.com/B1RI/" display-uri="B1RI" type="local">inner product</fr:link> of two embeddings should only need relative positioning i.e. for any two <fr:tex display="inline"><![CDATA[\mathbf {x}]]></fr:tex>, <fr:tex display="inline"><![CDATA[\mathbf {y}]]></fr:tex> with positions <fr:tex display="inline"><![CDATA[i]]></fr:tex> and <fr:tex display="inline"><![CDATA[j]]></fr:tex> respectively, there should exist some <fr:tex display="inline"><![CDATA[g]]></fr:tex>, such that
<fr:tex display="block"><![CDATA[\langle  f(\mathbf {x}, i), f(\mathbf {y}, j) \rangle  = g(\mathbf {x}, \mathbf {y}, i - j)]]></fr:tex>
Namely, this can be written as a function of just the relative positioning between the embeddings.
</html:p><html:p>The simplest transformation that only preserves this relative information are <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link>.</html:p><html:figure><html:img width="50%" src="/bafkrmiba4muc7qqk2agccjalqpm24hxw7u4gkfpozmnhz7if6ncagxpsgi.png" />
<html:figcaption>Rotating embeddings does not change relative position.</html:figcaption></html:figure><html:p>However, <fr:link href="/10H9/" title="Rotation matrix" uri="https://kellenkanarios.com/10H9/" display-uri="10H9" type="local">rotations</fr:link> are only easily defined for <fr:tex display="inline"><![CDATA[\mathbb {R}^2]]></fr:tex>. To get around this, they simply partition their input 2d slices and apply the rotation via
<fr:tex display="block"><![CDATA[\mathrm {ROT} = \begin {bmatrix} \cos  m \theta _{1} & - \sin  m \theta _{1} & & & \\
\sin  m \theta _{1} & \cos  m \theta _{1} & & & \\
& & \cos  m \theta _{2} & -\sin  m \theta _{2} & \\
& & \sin  m \theta _{2} & \cos  m \theta _{2} & \\
& & & & \ddots 
\end {bmatrix} ]]></fr:tex></html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The <fr:tex display="inline"><![CDATA[\theta _i]]></fr:tex>'s are fixed and chosen according to some schedule to capture different "frequencies". 
</fr:mainmatter></fr:tree>
<html:p>The embedding are used by appling them to the query and key matrices separately i.e. 
<fr:tex display="block"><![CDATA[W_K = \mathrm {ROT}(W_K), \quad  W_Q = \mathrm {ROT}(W_Q)]]></fr:tex>
These are then what is used in self-attention.
</html:p></fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/9VZH/</fr:uri><fr:display-uri>9VZH</fr:display-uri><fr:route>/9VZH/</fr:route><fr:title text="Hyperparameter Rules (CS336)">Hyperparameter Rules (<fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link>)</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Rule 1</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {ff}} = d_{\text {model}}]]></fr:tex>
<html:ul><html:li><fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex> is the input dimension</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d_{\text {ff}}]]></fr:tex> is the hidden dimension</html:li></html:ul></html:p><html:p><html:strong>Exception 1</html:strong>: in the case of <fr:link href="/K2N7/" title="Gated Linear Unit (GLU)" uri="https://kellenkanarios.com/K2N7/" display-uri="K2N7" type="local">GLUs</fr:link>, we recall that to account for extra parameters we scale <fr:tex display="inline"><![CDATA[\frac {2}{3}]]></fr:tex>. Therefore, <fr:tex display="inline"><![CDATA[d_{\text {ff}} = \frac {8}{3}d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Exception 2</html:strong>: In T5, they use <fr:tex display="inline"><![CDATA[d_{\text {ff}} = 64d_{\text {model}}]]></fr:tex>. The logic for this was that we could maximize the <fr:link href="/99G8/" title="Model FLOP Utilization (MFU)" uri="https://kellenkanarios.com/99G8/" display-uri="99G8" type="local">MFU</fr:link> by increasing matrix size. Ended up going back to small <fr:tex display="inline"><![CDATA[d_{\text {model}}]]></fr:tex>.</html:p><html:p><html:strong>Rule 2</html:strong>: <fr:tex display="inline"><![CDATA[d_{\text {head}} = d_{\text {model}} / \text {num heads}]]></fr:tex>
<html:ul><html:li>I believe the <fr:tex display="inline"><![CDATA[d_{\text {head}}]]></fr:tex> is the output dim of each head.</html:li>
  <html:li>This just says the total output dim for <fr:tex display="inline"><![CDATA[n]]></fr:tex> heads is the same as if we did one head.</html:li>
  <html:li>Means there is something important about splitting up the heads.</html:li></html:ul></html:p><html:p><html:strong>Rule 3</html:strong>: Aspect ratio = <fr:tex display="inline"><![CDATA[\frac {d_{\text {model}}}{n_{\text {layer}}} \approx  100-200]]></fr:tex>
<html:ul><html:li>Pipeline dependent: if network speed is fast than parallelizing is easier and shallow networks are better.</html:li>
  <html:li>If poor network speed then pipeline parallel might be more viable and deeper networks would be more parallelizable.</html:li></html:ul></html:p><html:p><html:strong>Rule 4:</html:strong> Vocab size
<html:ul><html:li>For single language models, vocab size is typically <fr:tex display="inline"><![CDATA[30-50k]]></fr:tex></html:li>
  <html:li>For multi language models, vocab size is typically <fr:tex display="inline"><![CDATA[100-250k]]></fr:tex></html:li></html:ul></html:p></fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Regularization and Dropout">Regularization and Dropout</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Since there is so much data, it is not feasible to train your model for multiple epochs. This is actually nice in the sense that we do not have to worry about overfitting and performing regularization.</html:p>
<html:p>However, many of the large models are still trained with weight decay. Tatsu claims that this is not to do with regularization but actually due to some weird interaction with the learning rate schedule. I am not sure I entirely understood this part.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Stability Tricks">Stability Tricks</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link> is ill-behaved due to the exponentials. In the transformer, we have two softmaxes: one at the end and one in the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>.
<html:ul><html:li>For the first, note <fr:tex display="block"><![CDATA[\log  \sigma (\mathbf {x})_i = \log (x_i) - \log  \underbrace {\sum _{j=1}^{d} e^{x_j}}_{D(\mathbf {x})}]]></fr:tex>
   The only problem is the denominator term. The idea is to enforce the <fr:tex display="inline"><![CDATA[D(\mathbf {x}) = 1]]></fr:tex> by regularizing via a penalty on <fr:tex display="inline"><![CDATA[\log  D(\mathbf {x})]]></fr:tex> ie.
<fr:tex display="block"><![CDATA[\mathcal {L}_{\text {aux}} = 10^{-4} \log ^2(D(\mathbf {x}))]]></fr:tex>
With the <fr:tex display="inline"><![CDATA[\nabla  D(\mathbf {x})]]></fr:tex> should be <fr:tex display="inline"><![CDATA[0]]></fr:tex> and we are effectively only considering the non-exponential term.
   </html:li>
   <html:li>For the <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link>, they primarily operate on the <html:em>logits</html:em> prior to the softmax. One way is via <fr:link href="/LVV4/" title="Layer Normalization" uri="https://kellenkanarios.com/LVV4/" display-uri="LVV4" type="local">layer norm</fr:link> before the <fr:link href="/C00V/" title="Softmax" uri="https://kellenkanarios.com/C00V/" display-uri="C00V" type="local">softmax</fr:link>. Another is via <html:em>softcapping</html:em> i.e.
   <fr:tex display="block"><![CDATA[\mathrm {logits} = \mathrm {softcap} \cdot  \tanh  \left ( \frac {\mathrm {logits}}{\mathrm {softcap}} \right ) ]]></fr:tex></html:li></html:ul></html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>13</fr:day></fr:date><fr:title text="Attention Variants">Attention Variants</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/W4YP/</fr:uri><fr:display-uri>W4YP</fr:display-uri><fr:route>/W4YP/</fr:route><fr:title text="Multi Query Attention">Multi Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0084/</fr:uri>
            <fr:display-uri>0084</fr:display-uri>
            <fr:route>/0084/</fr:route>
            <fr:title text="Notebook: Three Easy Pieces">Notebook: <fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">Three Easy Pieces</fr:link></fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>Part of my <fr:link href="/004H/" title="The Computer in Computer Science" uri="https://kellenkanarios.com/004H/" display-uri="004H" type="local">revisiting CS fundamentals journey</fr:link></html:p>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>6</fr:month>
                  <fr:day>23</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/007V/</fr:uri>
                <fr:display-uri>007V</fr:display-uri>
                <fr:route>/007V/</fr:route>
                <fr:title text="TPE Processes"><fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">TPE</fr:link> Processes</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (1/66)</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:title text="The Abstraction">The Abstraction</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:em>A process</html:em> consists of a few primary components
<html:ol><html:li><html:strong>Memory</html:strong>: each process requires its own portion of memory for (i) to store the instructions of the program to execute and (ii) for the memory required by the calling program. This is referred to as the <html:em>address space</html:em>.</html:li>
  <html:li><html:strong>Registers</html:strong>: for those familiar with computers, instructions typically deal with registers, where information is loaded from memory into registers to perform computation. Additionally, there are special registers reserved for certain mechanisms i.e.
  <html:ul><html:li><html:strong>Program Counter (PC)</html:strong>: This register is in charge of telling us what instruction we are currently on in the executing program.</html:li>
    <html:li><html:strong>Stack and Frame Pointer:</html:strong> Used to manage where we currently are on the stack.</html:li></html:ul></html:li></html:ol></html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:title text="Process API">Process API</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>In order to implement a process, one must support each of the following operations</html:p>
  <html:ol><html:li><html:strong>Create:</html:strong> <html:p>The operating system must implement a mechanism to create a process. This consists of loading the program from disk, initializing memory (i.e. stack and heap), and jumping to <html:code>main</html:code> (typically).</html:p></html:li>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The stack is allocated up front and does not change. However, typically a small amount of memory is allocated for the heap and it grows via <html:code>malloc</html:code>. This is non-trivial because if <html:code>malloc</html:code> is called without available heap memory then memory must be allocated by the OS. I need to look into <fr:link href="https://gee.cs.oswego.edu/dl/html/malloc.html" type="external">how</fr:link> this is done more.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  The OS will also initialize the parameters to <html:code>main</html:code> i.e. <html:code>argc</html:code> or <html:code>argv</html:code>.
</fr:mainmatter></fr:tree>

    <html:li><html:strong>Destroy:</html:strong> Clearly, if a process is misbehaved or the user just does not want it to run any longer, there must be some mechanism to <html:em>kill</html:em> the process.
      </html:li>
    <html:li><html:strong>Wait:</html:strong> In the case of dependencies (piping?), it may be useful to wait for a process to complete.
      </html:li>
    <html:li><html:strong>Miscellaneous Control:</html:strong> The example provided in the book is to <html:em>suspend</html:em> a process.
      </html:li>
    <html:li><html:strong>Status:</html:strong> It is helpful to be able to get information about running processes.
    </html:li></html:ol>
<html:p>There are also process <html:em>states</html:em>. These states are <html:strong>running</html:strong>, <html:strong>ready</html:strong>, and <html:strong>blocked</html:strong>. These states are pretty self-explanatory.
  </html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:title text="Data Structures">Data Structures</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>It seems throughout the book we will be seeing implementation examples from the smaller <fr:link href="https://github.com/mit-pdos/xv6-public" type="external">xv6</fr:link> OS.</html:p>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  In addition to the process implementation below, there is also a data structure that must keep track of all the processes. This is the <html:strong>process list</html:strong> or <html:strong>process control block</html:strong>.
  </fr:mainmatter></fr:tree>

<html:p><html:pre><![CDATA[// the registers xv6 will save and restore
// to stop and subsequently restart a process
struct context {
  int eip;
  int esp;
  int ebx;
  int ecx;
  int edx;
  int esi;
  int edi;
  int ebp;
};
// the different states a process can be in
enum proc_state { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE };
// the information xv6 tracks about each process
// including its register context and state
struct proc {
  char *mem; // Start of process memory
  uint sz; // Size of process memory
  char *kstack; // Bottom of kernel stack for this process
  enum proc_state state; // Process state
  int pid; // Process ID
  struct proc *parent; // Parent process
  void *chan; // If non-zero, sleeping on chan
  int killed; // If non-zero, have been killed
  struct file *ofile[NOFILE]; // Open files
  struct inode *cwd; // Current directory
  struct context context; // Switch here to run process
  struct trapframe *tf; // Trap frame for the current interrupt
};]]></html:pre></html:p>
</fr:mainmatter></fr:tree>

  

  <fr:tree show-metadata="false" expanded="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:title text="Homework">Homework</fr:title></fr:frontmatter><fr:mainmatter>
  <html:ol><html:li>Run <html:code>process.py</html:code> with the flags <html:code>-l 5:100, 5:100</html:code>. What should the CPU utilization be?
    
    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>The flags correspond to running two processes each with <fr:tex display="inline"><![CDATA[5]]></fr:tex> instructions where each instruction has probability one of using the CPU. The output from the program is 
    <html:pre><![CDATA[  Process 0
    cpu
    cpu
    cpu
    cpu
    cpu

  Process 1
    cpu
    cpu
    cpu
    cpu
    cpu]]></html:pre>
    with this, we can conclude that the utilization is 100%. We will execute one instruction for the first process until it completes, then the five instructions of the next process. If both are waiting on IO, then I do not think this would be the case? 
    </fr:mainmatter></fr:tree>

    
 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
    Using the solution flags <html:code>-cp</html:code>, there is 100% utilization. 
    </fr:mainmatter></fr:tree></html:li>
    <html:li>Run <html:code>process.py</html:code> with the flags <html:code>-l 4:100, 1:0</html:code>. How long does it take to complete both processes?
    
    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>The flags correspond to running one process with <fr:tex display="inline"><![CDATA[4]]></fr:tex> instructions with each instruction having probability one of using the CPU and one process with one instruction that uses the IO. The output from the program is 
    <html:pre><![CDATA[Process 0
  cpu
  cpu
  cpu
  cpu

Process 1
  io
  io_done]]></html:pre>
It will take <fr:tex display="inline"><![CDATA[4 + \text {time of IO}]]></fr:tex>. Since the process with <fr:tex display="inline"><![CDATA[4]]></fr:tex> non-IO-dependent instructions will run each of those instructions and then the next process will begin which must wait for the entire duration of IO.
    </fr:mainmatter></fr:tree>

    
 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>Using the <html:code>-cp</html:code> flags it takes <fr:tex display="inline"><![CDATA[11]]></fr:tex> timesteps to complete which is <fr:tex display="inline"><![CDATA[\approx  4 + \text {time of IO}]]></fr:tex>.
  </fr:mainmatter></fr:tree></html:li>
<html:li>
Now switch the order of the processes: <html:code>./process-run.py -l
1:0,4:100</html:code>. What happens now? Does switching the order mat-
ter? Why?
  
    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
The output of the code is
    <html:pre><![CDATA[Process 0
  io
  io_done

Process 1
  cpu
  cpu
  cpu
  cpu]]></html:pre>
    Depending on the time of IO (<html:code>the -L flag</html:code>), it should take <fr:tex display="inline"><![CDATA[\max (5, 1 + \text {time IO})]]></fr:tex>. Switching the order does matter because we can run the CPU instructions while waiting for non-blocking IO. This motivates the idea of scheduling.
  </fr:mainmatter></fr:tree>

  
 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
    It only takes <fr:tex display="inline"><![CDATA[7]]></fr:tex> timesteps because we are able to perform the IO operations off the CPU as expected.
  </fr:mainmatter></fr:tree></html:li>
  <html:li>
We’ll now explore some of the other flags. One important flag is <html:code>-S</html:code>, which determines how the system reacts when a process issues an I/O. With the flag set to <html:code>SWITCH_ON_END</html:code>, the system will NOT switch to another process while one is doing I/O, instead waiting until the process is completely finished. What happens when you run the following two processes, one doing I/O and the other doing CPU work? (<html:code>-l 1:0,4:100 -c -S SWITCH_ON_END</html:code>)

    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  The output of the code is:
<html:pre><![CDATA[Time        PID: 0        PID: 1           CPU           IOs
  1         RUN:io         READY             1          
  2        BLOCKED         READY                           1
  3        BLOCKED         READY                           1
  4        BLOCKED         READY                           1
  5        BLOCKED         READY                           1
  6        BLOCKED         READY                           1
  7*   RUN:io_done         READY             1          
  8           DONE       RUN:cpu             1          
  9           DONE       RUN:cpu             1          
 10           DONE       RUN:cpu             1          
 11           DONE       RUN:cpu             1]]></html:pre>
As expected, it runs the IO process, blocking the CPU process until it is completed.
</fr:mainmatter></fr:tree></html:li>
    <html:li>Now, run the same processes, but with the switching behavior set
to switch to another process whenever one is <html:code>WAITING</html:code> for I/O (<html:code>-l 1:0,4:100 -c -S SWITCH ON IO</html:code>). What happens now? Use <html:code>-c</html:code>
and <html:code>-p</html:code> to confirm that you are right</html:li>

    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  I assume that at timestep <fr:tex display="inline"><![CDATA[1]]></fr:tex> we will still <html:code>RUN:io</html:code>. However, we will then switch the <html:code>PID: 1</html:code> for the next <fr:tex display="inline"><![CDATA[4]]></fr:tex> timesteps until it is completed. After one additional <html:code>BLOCKED</html:code> timestep, we will run <html:code>RUN:io_done</html:code> and complete both processes. 
</fr:mainmatter></fr:tree>


 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  The output of the code is:
  <html:pre><![CDATA[Time        PID: 0        PID: 1           CPU           IOs
  1         RUN:io         READY             1          
  2        BLOCKED       RUN:cpu             1             1
  3        BLOCKED       RUN:cpu             1             1
  4        BLOCKED       RUN:cpu             1             1
  5        BLOCKED       RUN:cpu             1             1
  6        BLOCKED          DONE                           1
  7*   RUN:io_done          DONE             1]]></html:pre>
</fr:mainmatter></fr:tree>
 

<html:li>
One other important behavior is what to do when an I/O completes. With -I IO RUN LATER, when an I/O completes, the process that issued it is not necessarily run right away; rather, whatever was running at the time keeps running. What happens when you run this combination of processes? (<html:code>./process-run.py -l 3:0,5:100,5:100,5:100 -S SWITCH ON IO -I IO RUN LATER -c -p</html:code>) Are system resources being effectively utilized?

    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  The resources are not efficiently utilized because we end up waiting until all of our CPU-bound processes finish before returning to the IO-bound process, which calls another <html:code>IO:run</html:code> and we do not get any concurrency benefit.
</fr:mainmatter></fr:tree>


 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  The code output is pretty long but the end output is
  <html:pre><![CDATA[Stats: Total Time 31
Stats: CPU Busy 21 (67.74%)
Stats: IO Busy  15 (48.39%)]]></html:pre>
</fr:mainmatter></fr:tree></html:li>
<html:li>
Now run the same processes, but with <html:code>-I IO RUN IMMEDIATE</html:code> set, which immediately runs the process that issued the I/O. How does this behavior differ? Why might running a process that just completed an I/O again be a good idea?

    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  The behavior differs by returning to the IO-bound process, which can then immediately call the next <html:code>IO:run</html:code> before returning access to the CPU-bound processes, improving concurrency. In an attempt to predict the next IO call, it might be a good idea to run the process that just completed IO because it is more likely to make another IO call.
</fr:mainmatter></fr:tree>


 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  The corresponding statistics are
  <html:pre><![CDATA[Stats: Total Time 21
Stats: CPU Busy 21 (100.00%)
Stats: IO Busy  15 (71.43%)]]></html:pre>
</fr:mainmatter></fr:tree></html:li>
<html:li>
Now run with some randomly generated processes, e.g., <html:code>-s 1 -l
3:50,3:50, -s 2 -l 3:50,3:50, -s 3 -l 3:50,3:50</html:code>. See
if you can predict how the trace will turn out. What happens when
you use <html:code>-I IO RUN IMMEDIATE</html:code> vs. <html:code>-I IO RUN LATER</html:code>? What hap-
pens when you use <html:code>-S SWITCH ON IO</html:code> vs. <html:code>-S SWITCH ON END</html:code>?

    <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  Since each instruction has a 50% chance of being CPU or I/O, I do not think there will be much benefit to <html:code>IO_RUN_LATER</html:code> vs. <html:code>IO_RUN_IMMEDIATE</html:code>. However, I believe it will be crucial to enable <html:code>-S SWITCH_ON_IO</html:code> over <html:code>-S SWITCH_ON_END</html:code> as the number of processes grow because we should always have waiting CPU instructions to run. In the case of two processes, we still may not improve that much because they will likely both be IO-bound at some point.
</fr:mainmatter></fr:tree>


 
   
   <fr:tree show-metadata="false" expanded="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>6</fr:month><fr:day>23</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  For <html:code>-s 1</html:code> (aka the seed), the observed performance between <html:code>IO_RUN_IMMEDIATE</html:code> and <html:code>IO_RUN_LATER</html:code> is the same. However, we do observe an improvement on both IO and CPU usage when we enable <html:code>SWITCH_ON_IO</html:code>.
</fr:mainmatter></fr:tree></html:li></html:ol>
  </fr:mainmatter></fr:tree>

</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>1</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/007Y/</fr:uri>
                <fr:display-uri>007Y</fr:display-uri>
                <fr:route>/007Y/</fr:route>
                <fr:title text="TPE Process Interlude"><fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">TPE</fr:link> Process Interlude</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (3/66)</html:p><html:p>This was a very short chapter just introducing the actual API for some very important system calls. Namely</html:p>
  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="fork()"><html:code>fork()</html:code></fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
The <html:em>fork</html:em> system call allows a program to duplicate itself in a new process from the current instruction. This is quite literally a duplication which entails giving the child its own
<html:ol><html:li>Address space</html:li>
  <html:li>Registers</html:li>
  <html:li>Program counter</html:li></html:ol></html:p>
<html:p>
A fork is invoked in the parent process like
  <html:pre><![CDATA[  int main()
    int rc = fork();
    if (rc < 0) 
      printf("error with fork");
    else if (rc == 0)
      printf("in child");
    else
      printf("in parent");]]></html:pre></html:p>
<html:p>
Importantly, you identify whether you are in the child or the parent via the return value of <html:code>rc</html:code> (0 for child), (PID of child for parent).
</html:p>

    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Which process is run first (the parent or the child) is non-deterministic and depends on the CPU scheduling algorithm.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="wait()"><html:code>wait()</html:code></fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
  Due to the remark, we need some way to enforce ordering among parent and child. This is the motivation for the <html:em>wait</html:em> system call. Namely, when the parent calls <html:code>wait()</html:code> it tells the CPU not to schedule it until the child completes. 
  </html:p>
  
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
    From the previous <fr:link href="/007V/" title="TPE Processes" uri="https://kellenkanarios.com/007V/" display-uri="007V" type="local">section</fr:link>, we saw the data structure for a process included the state. I assume <html:code>wait()</html:code> is implemented by adjusting the process state in the PCB somehow?
  </fr:mainmatter></fr:tree>

  
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
    Doing a bit more research, it seems this is on the right track. Additionally, when a child completes it enters the <html:code>ZOMBIE</html:code> state, where it stores all the relevant information of the process, so that the parent process can then use this information. How this is actually coordinated seems to involve <html:em>signals</html:em> and <html:em>wait_entries</html:em> that essentially track all the children of a parent. The OS can then get a signal of completion from a child and find the corresponding parent?
  </fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="exec()"><html:code>exec()</html:code></fr:title></fr:frontmatter><fr:mainmatter>
<html:p>While <html:code>fork()</html:code> is useful in the sense that it allows us to spawn new processes from a parent process, it is still limited in that it can only execute instructions following the line of the <html:code>fork()</html:code> call in the parent process.</html:p>
<html:p>In practice, we do not want to have to compile massive C projects that contain all the necessary files for every possible binary we may want to run. To get around this, we can use the <html:code>exec()</html:code> system call. <html:code>exec()</html:code> allows us to transform a process into the desired executable. What this entails:
<html:ol><html:li>Loading the code and static data for the executable and overwriting the current coad and static data.</html:li>
  <html:li>Re-initializing stack and heap memory.</html:li>
  <html:li>Running the program, where the arguments passed to <html:code>exec()</html:code> are treated as <html:code>argv</html:code> for the program.</html:li></html:ol>
For a parent process to invoke <html:code>ls</html:code>, they could do something like
  <html:pre><![CDATA[  int main()
    int rc = fork();
    if (rc < 0) 
      printf("error with fork");
    else if (rc == 0)
      printf("in child");
      exec(ls);
    else
      printf("in parent");]]></html:pre>
  This spawns a child process and overwrites it with the <html:code>ls</html:code> executable.
</html:p>
</fr:mainmatter></fr:tree>

   
   <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
Combining <html:code>fork()</html:code> and <html:code>exec()</html:code> is where the true power lies. In particular, with this combination you can practically implement an entire shell. Below is some (hybrid) pseudocode.
<html:pre><![CDATA[int main()
  while (true) {
    command = getcommand();
    int rc = fork();
    if (rc < 0) 
      printf("error with fork");
    else if (rc == 0)
      exec(command);
    else
      int rc = wait(NULL);
  }]]></html:pre>
You can also implement redirection by opening a file prior to calling <html:code>exec()</html:code> i.e.
<html:pre><![CDATA[int main()
  while (true) {
    command = getcommand();
    int rc = fork();
    if (rc < 0) 
      printf("error with fork");
    else if (rc == 0)
      close(STD_OUT_FILENO);
      open(output.txt);
      exec(command);
    else
      int rc = wait(NULL);
  }]]></html:pre>
</fr:mainmatter></fr:tree>
 

  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Signals">Signals</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>They briefly mention <html:em>signals</html:em>: what I expect to be an important topic in the future. Namely, you can send a signall to a process via <html:code>kill()</html:code> and the recipient program can "catch" this signal via the system call <html:code>signal()</html:code>.
</html:p>
<html:p>This naturally leads to the introduction of a <html:em>user</html:em>. The <html:em>user</html:em> determines which processes they are allowed to signal, where the <html:em>superuser</html:em> can arbitrarily signal all processes.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Coding Homework">Coding Homework</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (6/66).</html:p>
  <html:ol><html:li>Write a program that calls <html:code>fork()</html:code>. Before calling <html:code>fork()</html:code>, have the main process access a variable (e.g., <html:code>x</html:code>) and set its value to something (e.g., 100). What value is the variable in the child process? What happens to the variable when both the child and parent change the value of <html:code>x</html:code></html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      Both the parent and the child get a copy of the variable values i.e. <fr:tex display="inline"><![CDATA[x]]></fr:tex>. When they modify <fr:tex display="inline"><![CDATA[x]]></fr:tex> it only affects the copy of <fr:tex display="inline"><![CDATA[x]]></fr:tex> in their local address space.
    </fr:mainmatter></fr:tree>

    <html:li>
Write a program that opens a file (with the <html:code>open()</html:code> system call) and then calls <html:code>fork()</html:code> to create a new process. Can both the child and parent access the file descriptor returned by <html:code>open()</html:code>? What happens when they are writing to the file concurrently, i.e., at the same time?
      </html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      Both do have access to the same file descriptor. I believe that they can interleave writes if writing at the same time but for me whichever process got ran first always finished writing prior the other starting. This may be a scheduling thing, where one process is able to complete before the other is scheduled?
    </fr:mainmatter></fr:tree>

    <html:li>
Write another program using <html:code>fork()</html:code>. The child process should
print “hello”; the parent process should print “goodbye”. You should
try to ensure that the child process always prints first; can you do
this without calling <html:code>wait()</html:code> in the parent?
    </html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      Using the previous problem, in the child, we can write something to a file. Then, in the parent, we can enforce an ordering by making the parent wait in a <html:code>while</html:code> loop until it reads the value written by the child. However, this is terribly inconvenient (and probably why <html:code>wait()</html:code> exists).
    </fr:mainmatter></fr:tree>

    <html:li>
Now write a program that uses <html:code>wait()</html:code> to wait for the child process to finish in the parent. What does <html:code>wait()</html:code> return? What happens if you use <html:code>wait()</html:code> in the child?
      </html:li>
      
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
        <html:code>wait()</html:code> returns the pid of the child process that died. Using <html:code>wait()</html:code> in the child just immediately returns <fr:tex display="inline"><![CDATA[-1]]></fr:tex>.
      </fr:mainmatter></fr:tree>

      <html:li>
Write a program that calls <html:code>fork()</html:code> and then calls some form of <html:code>exec()</html:code> to run the program <html:code>/bin/ls</html:code>. See if you can try all of the variants of <html:code>exec()</html:code>, including <html:code>execl()</html:code>, <html:code>execle()</html:code>, <html:code>execlp()</html:code>, <html:code>execv()</html:code>, <html:code>execvp()</html:code>, and <html:code>execvP()</html:code>. Why do you think there are so many variants of the same basic call?
        </html:li>
        
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
          It seems that some of them support different types of arguments i.e. <html:code>execv</html:code> family allows variable size argument list. Additionally, there is a distinction in the <html:em>environment</html:em>. This seems to be a list of environment variable values that <html:code>exec{l,v}e()</html:code> will search over. The <fr:tex display="inline"><![CDATA[p]]></fr:tex> family use the same search as the shell i.e. the <html:code>PATH</html:code>.
        </fr:mainmatter></fr:tree>

    <html:li>
Write a slight modification of the previous program, this time us- ing <html:code>waitpid()</html:code> instead of <html:code>wait()</html:code>. When would <html:code>waitpid()</html:code> be useful?
    </html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      If a parent process has multiple children, then <html:code>wait()</html:code> will only wait until the first one dies. <html:code>waitpid()</html:code> lets you specify waiting on a specific child process.
    </fr:mainmatter></fr:tree>

    <html:li> Write a program that creates a child process, and then in the child closes standard output (<html:code>STDOUT_FILENO</html:code>). What happens if the child calls <html:code>printf()</html:code> to print some output after closing the descriptor? </html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      Nothing prints in the child process. However, stdout is still open in the parent process and able to print.
    </fr:mainmatter></fr:tree>

    <html:li>
Write a program that creates two children, and connects the standard output of one to the standard input of the other, using the <html:code>pipe()</html:code> system call
      </html:li>
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
      This required <html:code>fflush()</html:code> for me. I am not sure if this is true in general?
    </fr:mainmatter></fr:tree></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>19</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/VE8C/</fr:uri>
                <fr:display-uri>VE8C</fr:display-uri>
                <fr:route>/VE8C/</fr:route>
                <fr:title text="TPE Limited Direct Execution"><fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">TPE</fr:link> Limited Direct Execution</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (7/66)</html:p><html:p>The contents of this chapter were revolved around two fundamental questions:</html:p>
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:ol><html:li><html:em>How do we make sure a program doesn't do what we don't want it to do?</html:em></html:li>
    <html:li><html:em>How do we enable timesharing?</html:em></html:li></html:ol>
</html:div>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:title text="modes">modes</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>For the first question, we introduce two modes <html:em>user mode</html:em> and <html:em>kernel mode</html:em>. In <html:em>user mode</html:em>, we are restricted with what sort of operations we can run.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
More importantly, in user mode, we are limited with what we can <html:em>see</html:em>. We will see later on in memory, but in user mode we have a limited address space allocated for the current process. However, in kernel mode, the address space is the entire computer and can therefore be very dangerous if we are not careful.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  How do we switch from user and kernel mode without allowing processes to run instructions in kernel mode?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  We must have some cooperation from the hardware. Namely, on boot, the OS code is run and sets up a <html:em>trap table</html:em>.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:title text="trapping">trapping</fr:title></fr:frontmatter><fr:mainmatter>
The hardware has some responsibility when switching to kernel mode. Namely, it must store two types of registers (1) user registers (2) kernel register.
<html:ol><html:li>The user registers are registers that contain the current program context i.e. stack variables, such as <html:code>x = 1</html:code>.</html:li>
  <html:li>The kernel registers hold more meta-information, such as the PC, the syscall number of the calling syscall, etc.</html:li></html:ol>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Each time we return to a process or switch between processes the hardware must perform a <html:em>context switch</html:em>. This is when they store all of the context of the current program before replacing it with the context of another program. The cost of such a switch is the tradeoff when performing <html:em>scheduling</html:em> of processes.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:title text="syscall implementation">syscall implementation</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
In order to implement the syscall, there needs to be some pre-agreed upon arrangement on where certain things go. For example, the syscall number must be stored in a pre-defined register. The general procedure (I think?) is as follows:
  </html:p>
<html:ol><html:li>syscall calls trap instruction defined by hardware.</html:li>
  <html:li>hardware stores all information necessary to restore processes in pre-determined location.</html:li>
  <html:li>hardware switches to kernel mode from user mode.</html:li>
  <html:li>hardware jumps to code in OS to handle syscall (trap handler / interrupt handler?).</html:li>
  <html:li>OS code checks pre-arranged register for the syscall number</html:li>
  <html:li>OS executes corresponding syscall code.</html:li>
  <html:li>OS restores program.</html:li></html:ol>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Due to relying on pre-determined registers, the syscalls are typically hand coded in assembly.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The OS must also perform checks regarding the safety of the requested operation i.e. we cannot let a process read the kernel code...
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
There is a similar mechanism for interrupts via an interrupt handler.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:title text="timesharing">timesharing</fr:title></fr:frontmatter><fr:mainmatter>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  If the CPU is running a process, then how can the kernel decide to stop running it?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
It can't... Instead the hardware performs period interrupts that trap to the OS code. The OS code can then decide what process to run via the <html:em>scheduler</html:em> (next section).
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>19</fr:day></fr:date><fr:title text="booting up">booting up</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>This section is a summary of my findings:</html:p>
<html:ol><html:li>Power signals reset of CPU state.</html:li>
  <html:li>CPU runs BIOS, which checks that hardware works.</html:li>
  <html:li>BIOS then runs bootloader (i.e. grub).</html:li>
  <html:li>Bootloader places kernel code in right place and runs it.</html:li>
  <html:li>Kernel then specifies things like trap table location (which is a special hardware instruction). Also must set timer.</html:li></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>26</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/B8GC/</fr:uri>
                <fr:display-uri>B8GC</fr:display-uri>
                <fr:route>/B8GC/</fr:route>
                <fr:title text="TPE Scheduling"><fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">TPE</fr:link> Scheduling</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>Toward the end of the previous section we alluded to the ominuous entity known as the <html:em>scheduler</html:em>. There is only one question that needs answering</html:p>
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:em>How should we pick what process to run at any given time?</html:em>
</html:div>
<html:p>It turns out there is no right answer to this question (at least that is my takeaway from these three sections in the book). The reason for this is the conflicting priorities of various metrics. Namely,
<html:ol><html:li><html:strong>turnaround time:</html:strong> the time to complete a process i.e.
  <fr:tex display="block"><![CDATA[T_{\text {completion}} - T_{\text {arrival}}]]></fr:tex>
  <html:li><html:strong>fairness:</html:strong> this is computed according to some arbitrary index. However, the main idea is that worst case jobs should not be neglected</html:li>
  <html:li><html:strong>response time:</html:strong> how long it takes from issuing the command to it being scheduled.</html:li></html:li></html:ol></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:title text="Naive First Attempts">Naive First Attempts</fr:title></fr:frontmatter><fr:mainmatter>
<html:p><html:strong>FIFO:</html:strong> A naive and fair scheduler is to just serve each job in a <html:em>first-in-first-out</html:em> manner. However this suffers from the <html:em>convoy effect</html:em>. Namely, if we consider the case where the first job is very long. Then subsequent much shorter jobs will have much longer service times than they should.</html:p>

<html:p><html:strong>SJF:</html:strong> To counteract this, we can try the <html:em>shortest job first</html:em>. If all jobs arrive at the same time, then SJF actually provably minimizes turnaround time. However, all jobs do not arrive at the same time and if the same problematic sequence for FIFO arrives sequentially it will also break SJF.</html:p>
<html:p><html:strong>STCF:</html:strong> Up to this point, we have assumed that all jobs must be run to completion. However, we can relax this assumption and allow the OS to <html:em>preempt</html:em> a job (pausing it and starting a new job). With this, we introduce <html:em>shortest to completion first</html:em>, where we periodically preempt the current running job and then schedule the job with the shortest amount of time remaining until completion.</html:p>
<html:p><html:strong>RR:</html:strong> While the previous algorithm is optimal with respect to turnaround time, it is not very good with respect to response time. In particular, long jobs may not be serviced for a very long time. The best we can do with respect to response time is just <html:em>round robin</html:em>, where we periodically randomly switch between processes. The intervals between switching are referred to as <html:em>strides</html:em> or <html:em>time slices</html:em> or <html:em>quantum lengths</html:em>.</html:p>
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Each time we switch between processes, we incur the cost of a context switch. Choosing larger time slices allows us to <html:em>amortize</html:em> this cost.
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Prior to this, we had been ignoring I/O of the processes. A very simple way to not suffer substantial performance loss due to I/O is to simply divide the process up into its non I/O components and schedule them individually.
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:title text="Partial Homework Solutions">Partial Homework Solutions</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>For what types of workloads does SJF deliver the same turnaround as FIFO?</html:li>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  For workloads that arrive in increasing order of total job time.
  </fr:mainmatter></fr:tree>

  <html:li>For what types of workloads and quantum lengths does SJF deliver the same response times as RR?</html:li>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  For shorter jobs and longer quantum lengths, RR and SJF should deliver similar response times.
  </fr:mainmatter></fr:tree>

  <html:li>What happens to response time with SJF as job lengths increase?</html:li>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
    Response time degrades because the longest jobs have to wait for each of the shorter jobs to finish. If these shorter jobs become longer than the response time grows linearly(?).
  </fr:mainmatter></fr:tree>

  <html:li>What happens to response time with RR as quantum lengths increase? Can you write an equation that gives the worst-case response time, given <fr:tex display="inline"><![CDATA[N]]></fr:tex> jobs?</html:li>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
    The response time is the average over stride length number of preceding jobs. The formula can be written as
    <fr:tex display="block"><![CDATA[\mathrm {RT} = \frac {1}{N} \sum _{i = 1}^{N} (i - 1)S = \frac {S(N - 1) N}{N} = S(N - 1)]]></fr:tex>
  </fr:mainmatter></fr:tree></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>26</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/HN3M/</fr:uri>
                <fr:display-uri>HN3M</fr:display-uri>
                <fr:route>/HN3M/</fr:route>
                <fr:title text="TPE Scheduling cont."><fr:link href="/arpaci2018operating/" title="Operating Systems: Three Easy Pieces" uri="https://kellenkanarios.com/arpaci2018operating/" display-uri="arpaci2018operating" type="local">TPE</fr:link> Scheduling cont.</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>In the previous section, we discussed algorithms that minimize turnaround time OR response time. Here, we will look at how to find a happy middle ground</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:title text="Multi-Level Feedback Queue">Multi-Level Feedback Queue</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The main idea is to maintain multiple queues with varying levels of priority. Each queue will then be scheduled round-robin within priority groups and lower priorities will not be scheduled until all processes of higher priority are completed. This introduces a few problems that we must remedy</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:em>How do we assign priorities?</html:em>
</html:div>

<html:p>One approach is the idea of <html:em>allotment</html:em>. Essentially, each process is given an alloted amount of time at each priority. Once the process has run for the alloted amount of time at a given priority it moves down to the next priority.
</html:p>
<html:p>
Clearly, this achieves a good response time because it essentially reduces to round robin. This also does not tank performance because, if we recall that STCF is the optimal algorithm for turnaround time, we see that short jobs will still finish relatively quickly because they will spend all of their time at high priority (assuming a high enough allotment).</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
We also saw in the previous section that one way to handle I/O is by breaking it into multiple jobs. However, this can result in gaming an allotment scheduler by I/O-ing right before you reach allotment essentially ensuring your process always remains the highest priority. To deal with this, we can count the total time spent by each of the jobs toward the allotment.
</fr:mainmatter></fr:tree>


 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:em>How do we prevent starvation of lower priority queues?</html:em>
</html:div>

<html:p>It seems the easiest way for this is to just periodically move all processes back to the highest priority.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:title text="Proportional Share">Proportional Share</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The key idea here is rather than maintain queues of priorities we instead assign each job a certain <html:em>proportion</html:em> of the CPU time. The obvious question:</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:em>How do we ensure the jobs actually match the desired proportion?</html:em>
</html:div>

The first approach is the concept of <html:em>ticket currency</html:em>. The idea is to give each user a finite amount of tickets and have them allocate them amongs their processes. 

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
They also allow <html:em>ticket transfer</html:em> and <html:em>ticket inflation</html:em>, where a process can transfer tickets i.e. client / server, or temporarily increase their number of tickets.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The benefit of the <html:em>ticket currency</html:em> approach is the simplicity of the implementation. You simply draw a random number in the range of the total number of tickets and select the process with that number.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The natural question of how to actually assign tickets is very difficult and not solved.
</fr:mainmatter></fr:tree>

<html:p><html:strong>Deterministic version:</html:strong> The ambitious observer might wonder if there is a deterministic algorithm to achieve the same effect. Luckily, the answer is actually yes! Rather than randomly sample tickets, we set the <html:em>stride</html:em> to be inversely proportional to the ticket count. We then track the <html:em>pass</html:em> (a running sum for each process that is incremented by the processes stride). To formalize-ish,
<html:ol><html:li>Choose process with lowest <html:em>pass</html:em>.</html:li>
  <html:li>Increment the chosen process pass by the process stride.</html:li>
  <html:li>Repeat.</html:li></html:ol>
We see that a low stride (high ticket count) results in being scheduled more frequently.
</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
There is still an advantage of lottery-based scheduling: no global state. If new processes join, it is unclear what to set their initial pass at. 
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>26</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
On linux, the user can manually set priorities using <html:code>nice</html:code> and setting the <html:em>niceness</html:em> of a process in <![CDATA[[]]>-20, 20<![CDATA[)]]>.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>29</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/SAYD/</fr:uri>
                <fr:display-uri>SAYD</fr:display-uri>
                <fr:route>/SAYD/</fr:route>
                <fr:title text="Memory Intro">Memory Intro</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>
We are now moving on from scheduling into the wonderful world of all things memory.
  </html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Address Spaces">Address Spaces</fr:title></fr:frontmatter><fr:mainmatter>
If we think about what we have done so far, we have an assortment of processes that are constantly being switched between. However, each of these process has their own assortment of register states, but more importantly they each have their own state in <html:em>memory</html:em>.
<html:ol><html:li>For register states, the OS has its own reserved memory, where it can easily store and switch between register states.</html:li>
  <html:li>For memory, when we switch processes, are we expected to clear out memory for the next process by storing to disk? <html:strong>NO!</html:strong></html:li></html:ol>
This motivates the idea behind <html:em>address spaces</html:em>. Namely, each process is given its own address space (chunk in memory) to use at its own discretion. This will then ensure that other processes do not overwrite the program state of one another. The address space of a process typically consists of three components
<html:ol><html:li>Code of the program</html:li>
  <html:li>Stack: local var, arguments, return values</html:li>
  <html:li>Heap: dynamically allocated memory</html:li></html:ol>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
As we will see later, every address we see is actually <html:em>virtual</html:em>, meaning it is not the location in physical memory.
</fr:mainmatter></fr:tree>

<html:p><html:strong>API:</html:strong> We only really need two methods, a way to get memory and a way to release memory. These are implemented via <html:code>malloc</html:code> and <html:code>free</html:code>. However, the use of these is usually accompanied with a handful of common errors:</html:p>
<html:ol><html:li>Forget to initialize mem.</html:li>
  <html:li>Forget to free mem.</html:li>
  <html:li>Freeing memory early: <html:em>dangling pointer</html:em>.</html:li>
  <html:li>Freeing the same memory twice: <html:em>double free</html:em>.</html:li>
  <html:li>Passing something not allocated earlier: <html:em>invalid free</html:em>.</html:li></html:ol>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
When a process dies, the OS reclaims all of that process memory. This means that as long as your program memory is bounded, leakage will not do any sort of catastrophic damage. However, it is still not good practice.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
<html:code>malloc</html:code> and <html:code>free</html:code> are not syscalls. The heap is pre-allocated with some allotment of memory. If the program exceeds this memory then <html:code>malloc</html:code> will call a syscall <html:code>brk</html:code> / <html:code>sbrk</html:code> to increment / decrement heap mem pointer.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
You can actually initialize and use your own heap-like memory through <html:code>mmap</html:code>. This memory is not associated with any particular file and is an <html:em>anonymous</html:em> memory region within your program.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Address Translation">Address Translation</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>The next question is once each program has their own address space, how do we unify the experience for the user.</html:p>
  <html:ol><html:li>We want every process to have the <html:em>illusion</html:em> that is starts at address <fr:tex display="inline"><![CDATA[0]]></fr:tex></html:li>
    <html:li>We want every process to have the <html:em>illusion</html:em> of a large address space.</html:li></html:ol>
  <html:p>However, we also do not want to give up the protection / speed of the address space abstraction. The overarching idea is <html:em>virualizing</html:em> the address space. This means that the address the user sees is not the true address in physical memory. How we achieve this will be a longstanding topic for the next few entries of these notes.</html:p>
  <html:p><html:strong>Base and Bounds:</html:strong> The first approach is <html:em>base and bounds</html:em>. For this approach, the hardware has a dedicated base and bound register to indicate where the address space for a given process begins and ends. It is the OSes job to ensure that these registers are filled with the correct values for the current process.
  <html:ul><html:li>The hardware must provide protected instructions to allow the OS to modify the base and bound registers.</html:li></html:ul></html:p>
  <html:p>An <html:em>address translation</html:em> is then performed by the hardware by adding the base to the virtual address and checking it remains within the bounds. This is done by the <html:em>memory management unit</html:em> (MMU). A common theme we will see is that hardware support is crucial to achieve good virtualization.</html:p>
  
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
    <html:strong>Problem:</html:strong> <html:em>to support a large addres space with base and bounds we suffer significant <html:em>internal fragmentation</html:em> i.e. in between the base and bound there is a lot of unused memory.</html:em>
  </html:div>

  <html:p>To better understand why fragmentation is such an issue: if we have 16kb total memory and each process requests 4kb but only uses 2kb, then we can only maintain 4 processes despite having the memory to service 8! To attempt to remedy this, we will introduce the idea of <html:em>segmentation</html:em>.</html:p> 
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Segmentation">Segmentation</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Instead of pre-allocating the memory between the heap and stack, why don't we <html:em>allocate on demand</html:em>. To do so, we can treat the code, heap, and stack as their own <html:em>segments</html:em> with their own base and bound registers. An address could then be decomposed as 
<fr:tex display="block"><![CDATA[\underbrace {0 1}_{\text {segment}}\underbrace {0 1 \cdots  11}_{\text {offset}}]]></fr:tex>
This also enables an idea of <html:em>memory sharing</html:em>, where process can share a portion of memory (such as the code) to save memory on the system.
<html:ul><html:li>Need to add a protection bit to indicate whether a process can modify certain addrs.</html:li></html:ul></html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
This limits flexibility in the sense that we have no control over where the middle region was allocated and therefore we cannot grow beyond the initial bound.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The hardware also has to support the stack growing backwards when it checks the bounds. To do so, it has to perform an additional check based on the segment number.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>29</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/67X5/</fr:uri>
                <fr:display-uri>67X5</fr:display-uri>
                <fr:route>/67X5/</fr:route>
                <fr:title text="Free-Space Management">Free-Space Management</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>We have been talking a lot about how to abstract physical address to provide ease of use for the programmer. However, we have yet to discuss how to actually allocate these physical addresses in the first place. When allocating memory, there are a few issues we need to be careful about
<html:ol><html:li><html:em>External fragmentation</html:em>: lots of small unallocated non-contiguous chunks of memory.</html:li>
  <html:li><html:em>Internal fragmentation</html:em>: allocated memory that is unused in the process it is allocated for.</html:li></html:ol>
To start, we make a few simplifying assumptions:
<html:ol><html:li>After memory has been allocated, it cannot be moved.</html:li>
  <html:li>The size of memory is fixed.</html:li>
  <html:li>The user does not specify the size in <html:code>free</html:code>.</html:li></html:ol>
First, we can quickly deal with the last problem by introducing a <html:em>header</html:em>. The OS stores additional metadata, such as the size of the allocated memory in a header directly preceding the returned pointer in memory.
<html:ul><html:li>If <html:code>int* x = malloc(sizeof(int))</html:code> then <html:code>header = (header_t)* x - 1</html:code>.</html:li>
  <html:li>OS needs to find memory for the requested memory + size of the header.</html:li></html:ul></html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Malloc Policies">Malloc Policies</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>The general allocation algorithm consists of maintaining some <html:em>free-list</html:em> that contains the address and length of free memory. An allocation is done by some policy over this list. When memory is re-claimed, the list is searched for adjacent memory and if it is found then it is <html:em>coalesced</html:em> into a larger chunk of free memory.</html:p>
<html:ol><html:li><html:em>Best fit</html:em>: find smallest chunk that is big enough</html:li>
  <html:ul><html:li>leaves lots of small chunks</html:li></html:ul>
  <html:li><html:em>Worst fit</html:em>: find largest chunk</html:li></html:ol>
Both of these require searching the entire list, so instead we can introduce some heuristics.
<html:ol><html:li><html:em>First fit</html:em>: find first block that can service.</html:li>
  <html:li><html:em>Next fit</html:em>: find first block that can service starting from the previous allocation.</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Segregated Lists">Segregated Lists</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Outside of the allocation policy, we can make data-structure level optimization. One such optimization is segmentation lists. The idea is to maintain different lists that correspond to different commonly allocated object types.</html:p>
  <html:ul><html:li>Grow list by requesting segments as multiple size of the objects.</html:li>
    <html:li>Can return to main list when no references.</html:li>
    <html:li>Can also leave objects pre-iitialized on list to eliminate calls to constructor / destructor.</html:li></html:ul>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>29</fr:day></fr:date><fr:title text="Buddy Allocation">Buddy Allocation</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>Assume memory is size <fr:tex display="inline"><![CDATA[2^n]]></fr:tex>. Search through memory to find smallest block that can service request.</html:p>
<html:figure><html:img width="50%" src="/bafkrmigwzou6llta75hya6llhd6ixa6gjzeoyjqecitht2op5xxutkjgnu.png" />
<html:figcaption>Memory <fr:tex display="inline"><![CDATA[2^n]]></fr:tex>.</html:figcaption></html:figure>
<html:p>When block is freed,
<html:ol><html:li>Check if the buddy is free.</html:li>
  <html:li>If the buddy is free.</html:li>
  <html:ul><html:li>Coalesce into larger block</html:li>
    <html:li>Repeat.</html:li></html:ul></html:ol>
Can easily check buddy because address differ by one bit at each level. Just modify the corresponding bit and check the free data structure (bitmap?) if the address is free. If it is then move up and check the next bit.
</html:p>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>31</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/SB61/</fr:uri>
                <fr:display-uri>SB61</fr:display-uri>
                <fr:route>/SB61/</fr:route>
                <fr:title text="Paging">Paging</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>Previously, we have only dealt with memory allocations as variable sized chunks i.e. the amount of memory allocated can be anything. A simple idea to avoid the drastic fragmentation this can induce is to instead divide memory into fixed size chunks. This idea is called <html:em>paging</html:em>, where each chunk is a <html:em>page</html:em>.</html:p>
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Main question:</html:strong> <html:em>how do we virtualize memory with pages?</html:em>
</html:div>
<html:p><html:strong>Idea:</html:strong> we decompose the address into a <html:em>virtual page number</html:em> (VPN) and offset i.e. 
<fr:tex display="block"><![CDATA[\underbrace {01}_{\text {VPN}}\underbrace {110\cdots  01}_{\text {offset}}]]></fr:tex>
we then maintain a datastructure to map <fr:tex display="inline"><![CDATA[\text {VPN} \mapsto  \text {PPN}]]></fr:tex>, where the PPN is the <html:em>physical page number</html:em> (we divide both virtual and physical memory into pages). We call this data structure a <html:em>page table</html:em>.
</html:p><html:p><html:strong>Problem 1:</html:strong> The page table can get ridiculously large.</html:p>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
For now, we consider a linear page table (an array with entry for each VPN). If we support a 32 bit address space with 4kb pages and each entry in the page table is 4 bytes then this is <fr:tex display="inline"><![CDATA[2^{32} / 2^{12} * 4 = 2^{22}]]></fr:tex> bytes of memory for the page table!
</fr:mainmatter></fr:tree>
 
<html:p>Before, we relied on the MMU to translate between physical and virtual addresses via simple operations like base and bound. However, there is no way the MMU could support this many arbitrary translations. Therefore, the page table itself must be stored in physical memory.</html:p><html:p>In addition to the address translation, the page table also keeps track of the following bits:
<html:ol><html:li>
Valid bit: whether this particular VPN has memory allocated to it i.e. mem between stack / heap does not need to be allocated.
  </html:li>
  <html:li>
Protection bit: determines whether the calling program can read / write / execute.
  </html:li>
  <html:li>
Present bit: whether the page currently resides in physical memory.
  </html:li>
  <html:li>
Dirty bit: has the page been modified since being paged in.
  </html:li>
  <html:li>
Reference bit: has the page been accessed (used for eviction policy).
    </html:li></html:ol></html:p><html:p><html:strong>Problem 2:</html:strong> Every time we want to perform an address translation (very often) we must
<html:ol><html:li>Compute VPN from address</html:li>
  <html:li>Access page table entry in physical memory (slow)</html:li>
  <html:li>Compute address with offset + PPN.</html:li>
  <html:li>Access corresponding physical page in physical memory.</html:li></html:ol>
This is very slow!
</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:title text="TLB">TLB</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Due to the previously discussed issues, we need a faster way of performing address translation to make paging a feasible option. A common theme: we will throw hardware at it</html:p>
<html:p>Specifically, we will add an additional cache known as the <html:em>translation lookaside buffer</html:em> that caches address translations. Generally, an address translation proceeds as:
<html:ol><html:li>Extract VPN</html:li>
  <html:li>Check if VPN in TLB</html:li>
  <html:li>If in TLB: get translation</html:li>
  <html:li>Otherwise: TLB miss -&gt; trap to OS (RISC)</html:li></html:ol></html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
We have to be a little careful. The OS code itself is located in memory and is therefore subject to our paging system. We can imagine that on TLB miss the hardware tries to load the TLB miss handler code by checking the TLB, getting the translation, and loading the code. But what if the translation for the TLB miss handler is not in the TLB?!?! To not worry about these things the TLB miss handler gets a permanent spot in physical memory.
</fr:mainmatter></fr:tree>

<html:p>Formally, an entry in the TLB consists of 
<html:ol><html:li>VPN</html:li>
  <html:li>PPN</html:li>
  <html:li>Other bits</html:li>
  <html:ul><html:li>Valid bit: has valid translation (different from PTE valid bit) can be invalid on initialization, eviction, or context switch.</html:li>
    <html:li>Protection bit: whether the requested access can be performed.</html:li></html:ul></html:ol></html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Problem</fr:taxon></fr:frontmatter><fr:mainmatter>
  Each process has different address translations corresponding to the same VPN.
</fr:mainmatter></fr:tree>


 
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  Keep additional <html:em>address space identifier</html:em> (ASID) in TLB entry. We also need an ASID register that we set at every process switch to identify the current running process.
</fr:mainmatter></fr:tree>
 

<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:uri>https://kellenkanarios.com/P9UM/</fr:uri><fr:display-uri>P9UM</fr:display-uri><fr:route>/P9UM/</fr:route><fr:title text="Culler's Law">Culler's Law</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p><html:em>"Ram is not always ram"</html:em>. Referring to the fact that a physical memory access takes a substantial different amount of time depending on whether it hit in the TLB.</html:p></fr:mainmatter></fr:tree>

   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
In a real software managed TLB, the TLB consisted of the following components:
<html:ol><html:li>Global bits for globally shared pages,</html:li>
  <html:li>8 bit ASID,</html:li>
  <html:li>Coherence bits for cache-coherence,</html:li>
  <html:li>32-64 TLB entries,</html:li>
  <html:li>A wired register to indicate how many slots to reserve in the TLB for things like TLB miss handler.</html:li></html:ol>
</fr:mainmatter></fr:tree>
 


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Another important tidbit: when using very fast caches, even an access to the TLB is a CPU bottleneck (accessing TLB is longer than accessing cache). To get around this, there are <html:em>virtually indexed caches</html:em> that circumvent the need for performing an address translation.
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:title text="Smaller Tables">Smaller Tables</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>As previously discussed, as it currently stands, the page table takes up too much memory.</html:p>
<html:p><html:strong>Solution 1:</html:strong> Just use bigger pages! Unfortunately, this will suffer from severe internal fragmentation (processes not using their allocated memory) and so we would prefer to avoid this solution.</html:p>
<html:p><html:strong>Solution 2:</html:strong> Paging + segmentation. Instead of a page table for all of memory, keep 3 page tables: one for code, stack, and heap. This only partially resolves the problem.
<html:ol><html:li>Same problems as segmentation. Variable sized page tables will cause external fragmentation</html:li>
  <html:li>We can do better. Within each heap, there is still a (smaller) version of the same problem: a bunch of unused allocated PTEs.</html:li></html:ol></html:p>
<html:p><html:strong>Multi-level page tables:</html:strong> Finally, we have arrived to the idea of a multi-level page table. The idea can be summarized as</html:p>
<html:ol><html:li>Chop page table up into pages.</html:li>
  <html:li>Do not allocated pages with no valid references (i.e. none of the corresponding are being used).</html:li></html:ol>
<html:figure><html:img width="80%" src="/bafkrmihf7gcws2u2ay4ecuwqwpgnziozltgjbsn7zo6x5bs6ismwq76jby.png" /></html:figure>
<html:p>This has a few advantages:</html:p>
<html:ol><html:li>Fits easily in memory. Allocate one page when an entry is used.</html:li> 
 <html:li>Proportional to the amount of address space actually in use.</html:li></html:ol>
<html:p>The obvious downside is the added complexity. Namely, on every TLB miss we must make two loads of memory (1) the page directory entry and (2) the page table entry.</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:title text="What happens when memory is full?">What happens when memory is full?</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>We reserve space on disk for pages that cannot fit in physical memory. This is referred to as <html:em>swap space</html:em>.</html:p>
<html:p>We need to add an additional <html:em>present bit</html:em> to the PTE. On a TLB miss, the hardware (or software) will check the page table and see the present bit to indicate whether the page is in physical memory. If not, we must trap to the OS and this is known as a <html:em>page fault</html:em>.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>31</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
The hardware <html:strong>never</html:strong> handles page faults because this would require understanding swap space + knowing how to perform I/O with disk.
</fr:mainmatter></fr:tree>

<html:p>Three important cases to consider:</html:p>
<html:ol><html:li>Page is <html:strong>present</html:strong> and <html:strong>valid</html:strong>.</html:li>
  <html:ul><html:li>Hardware can just grab physical address from PTE.</html:li></html:ul>
  <html:li>Page is not <html:strong>present</html:strong> but <html:strong>valid</html:strong>.</html:li>
  <html:ul><html:li>Hardware must call page fault handler to retrieve page.</html:li></html:ul>
  <html:li>Page is not <html:strong>valid</html:strong>.</html:li>
  <html:ul><html:li>Throw exception and likely terminate process as it is accessing invalid memory.</html:li></html:ul></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>3</fr:month>
              <fr:day>22</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/005G/</fr:uri>
            <fr:display-uri>005G</fr:display-uri>
            <fr:route>/005G/</fr:route>
            <fr:title text="Research Bible">Research Bible</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p>I will be reading and reviewing ~2ish paper a week (tentatively). More accurately, I have allocated roughly 8hrs a week to reading papers. However much I accomplish in this time will determine the actual rate of these reviews. These reviews are mainly just for my own understanding. Rather than techinical details, I will aim to cover
<html:ol><html:li>Existing / related work and their issues.</html:li>
  <html:li>Key components of the methodology and how it solves these issues.</html:li>
  <html:li>Limitations / future directions or relationships to my work.</html:li></html:ol></html:p>
            <html:p>For more in-depth coverage of papers I find particularly interesting, see my <fr:link href="/0002/" title="Blog" uri="https://kellenkanarios.com/0002/" display-uri="0002" type="local">blog</fr:link>. There will likely be overlap i.e. most blogs will start here...</html:p>
            <html:p>See <fr:link href="/007N/" title="Upcoming papers of the week" uri="https://kellenkanarios.com/007N/" display-uri="007N" type="local">upcoming papers</fr:link>!</html:p>
            <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>14</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/W4YP/</fr:uri>
                <fr:display-uri>W4YP</fr:display-uri>
                <fr:route>/W4YP/</fr:route>
                <fr:title text="Multi Query Attention">Multi Query Attention</fr:title>
              </fr:frontmatter>
              <fr:mainmatter><html:p>Notes on the paper <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">Fast Transformer Decoding: One Write-Head is All You Need</fr:link>.</html:p><html:p>For, 
<html:ul><html:li><fr:tex display="inline"><![CDATA[b]]></fr:tex> batch dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[n]]></fr:tex> sequence length,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[d]]></fr:tex> hidden dimension,</html:li>
  <html:li><fr:tex display="inline"><![CDATA[h]]></fr:tex> number of heads.</html:li></html:ul>
The total <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link> for <fr:link href="/004G/" title="Self-Attention" uri="https://kellenkanarios.com/004G/" display-uri="004G" type="local">attention</fr:link> are roughly <fr:tex display="inline"><![CDATA[bnd^2]]></fr:tex>. This comes from 
<html:ol><html:li><fr:tex display="inline"><![CDATA[3nd^2]]></fr:tex> FLOPS to compute <fr:tex display="inline"><![CDATA[Q = XW_Q, K = XW_K, V = XW_V]]></fr:tex>.</html:li>
  <html:li>Need to do this <fr:tex display="inline"><![CDATA[b]]></fr:tex> times for each input in batch.</html:li></html:ol>
and the total memory accesses are roughly <fr:tex display="inline"><![CDATA[\underbrace {bnd}_{X} + \underbrace {bhn^2}_{\text {softmax}} + \underbrace {d^2}_{\text {projection}}]]></fr:tex>.
This gives high arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {h}{d} + \frac {1}{bn}\right )^{-1}\right )]]></fr:tex></html:p><html:p>However, if we do incremental inference then we must multiply our total number of memory accesses by <fr:tex display="inline"><![CDATA[n]]></fr:tex> i.e. <fr:tex display="inline"><![CDATA[bn^2d + nd^2]]></fr:tex>. This gives arithmetic intensity
<fr:tex display="block"><![CDATA[O\left (\left (\frac {n}{d} + \frac {1}{b}\right )^{-1}\right ),]]></fr:tex>
which requires large batch and short sequence length.
</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
I believe we just ignore the softmax memory contribution in the inference case because it does not scale with <fr:tex display="inline"><![CDATA[n]]></fr:tex> anymore as we are computing the logits just for the next token. Therefore, it becomes a <fr:tex display="inline"><![CDATA[\frac {d}{h}]]></fr:tex> term, which we can safely ignore?
</fr:mainmatter></fr:tree>
<html:p>The key is that the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term comes from the <fr:tex display="inline"><![CDATA[bn^2d]]></fr:tex> term, where we are loading <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[(b \times  n \times  d / h)]]></fr:tex> <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices <fr:tex display="inline"><![CDATA[n]]></fr:tex> times. Thus, we can improve the <fr:tex display="inline"><![CDATA[\frac {n}{d}]]></fr:tex> term by a factor of <fr:tex display="inline"><![CDATA[h]]></fr:tex> by simply not using a different <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrix for each head. This is the entire idea behind <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link>.</html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Similar to the <fr:link href="https://kellenkanarios.com/TJLA/" type="external">TJLA</fr:link>, we still can use <fr:tex display="inline"><![CDATA[h]]></fr:tex> <fr:tex display="inline"><![CDATA[Q]]></fr:tex> matrices because we do not need to load <fr:tex display="inline"><![CDATA[n]]></fr:tex> of them into memory because only the last one matters for inference.
</fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/XUVV/</fr:uri><fr:display-uri>XUVV</fr:display-uri><fr:route>/XUVV/</fr:route><fr:title text="Group Query Attention">Group Query Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:em>Group query attention</html:em> is the same idea as <fr:link href="/shazeerFastTransformerDecoding2019/" title="Fast Transformer Decoding: One Write-Head is All You Need" uri="https://kellenkanarios.com/shazeerFastTransformerDecoding2019/" display-uri="shazeerFastTransformerDecoding2019" type="local">MQA</fr:link> but instead of using one <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> for every head, they use a <html:em>group</html:em> of them. Where obviously the number of groups needs to be less than the number of heads.</html:p><html:figure><html:img width="80%" src="/bafkrmicmjlgi2o3oetsssjdwk6y2pqce4vrdwiw2ah4wwe3ulnt74upkiu.png" />
<html:figcaption>Grouped query uses subset of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> matrices.</html:figcaption></html:figure></fr:mainmatter></fr:tree><html:p>Also inspired by this idea of reducing the dependence of <fr:tex display="inline"><![CDATA[K]]></fr:tex> and <fr:tex display="inline"><![CDATA[V]]></fr:tex> on the sequence length is sliding window attention.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>14</fr:day></fr:date><fr:uri>https://kellenkanarios.com/WM3M/</fr:uri><fr:display-uri>WM3M</fr:display-uri><fr:route>/WM3M/</fr:route><fr:title text="Sliding Window Attention">Sliding Window Attention</fr:title></fr:frontmatter><fr:mainmatter><html:p>The idea behind <html:em>sliding window attention</html:em> is to ensure the number of weight matrices needed for the keys <fr:tex display="inline"><![CDATA[K]]></fr:tex> and values <fr:tex display="inline"><![CDATA[V]]></fr:tex> does not scale with the sequence length. Intuitively, this means allowing each word to "attend" to only some fixed number of previous words rather than the whole sequence</html:p><html:figure><html:img width="80%" src="/bafkrmidqeoo6wx6s4ry6u74fcelfzxvwreyf7d44whnql4ffnnyd2dwcha.png" />
<html:figcaption>"the" only sees "on" and "sat" rather than the full sentence.</html:figcaption></html:figure></fr:mainmatter></fr:tree></fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kellenkanarios.com/rlc/</fr:uri>
            <fr:display-uri>rlc</fr:display-uri>
            <fr:route>/rlc/</fr:route>
            <fr:title text="Reinforcement Learning Conference">Reinforcement Learning Conference</fr:title>
            <fr:taxon>Conference</fr:taxon>
            <fr:meta name="external">https://rl-conference.cc/</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
