<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Wesley Chung</fr:author>
      <fr:author>Lynn Cherif</fr:author>
      <fr:author>David Meger</fr:author>
      <fr:author>Doina Precup</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>12</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/chungParsevalRegularizationContinual2024/</fr:uri>
    <fr:display-uri>chungParsevalRegularizationContinual2024</fr:display-uri>
    <fr:route>/chungParsevalRegularizationContinual2024/</fr:route>
    <fr:title text="Parseval Regularization for Continual Reinforcement Learning">Parseval Regularization for Continual Reinforcement Learning</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2412.07224</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2412.07224</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{chungParsevalRegularizationContinual2024,
 title = {Parseval {{Regularization}} for {{Continual Reinforcement Learning}}},
 author = {Chung, Wesley and Cherif, Lynn and Meger, David and Precup, Doina},
 year = {2024},
 doi = {10.48550/arXiv.2412.07224},
 urldate = {2024-12-18},
 number = {arXiv:2412.07224},
 publisher = {arXiv},
 file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Chung et al_2024_Parseval Regularization for Continual Reinforcement Learning.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Loss of plasticity, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---all referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.},
 primaryclass = {cs},
 eprint = {2412.07224},
 month = {December}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
