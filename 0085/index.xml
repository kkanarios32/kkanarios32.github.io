<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>7</fr:month>
      <fr:day>1</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/0085/</fr:uri>
    <fr:display-uri>0085</fr:display-uri>
    <fr:route>/0085/</fr:route>
    <fr:title text="Notebook: Stanford CS336">Notebook: <fr:link href="/stanford/" title="Stanford University" uri="https://kellenkanarios.com/stanford/" display-uri="stanford" type="local">Stanford</fr:link> <fr:link href="https://stanford-cs336.github.io/spring2025/index.htmlschedule" type="external">CS336</fr:link></fr:title>
  </fr:frontmatter>
  <fr:mainmatter>
    <html:p>Formal notebook for my journey in <fr:link href="/stanford/" title="Stanford University" uri="https://kellenkanarios.com/stanford/" display-uri="stanford" type="local">Stanford's</fr:link> <fr:link href="https://stanford-cs336.github.io/spring2025/index.htmlschedule" type="external">CS336</fr:link> as part of my <fr:link href="/003W/" title="LLMs" uri="https://kellenkanarios.com/003W/" display-uri="003W" type="local">LLM</fr:link> self-study endeavors.</html:p>
    <fr:tree show-metadata="false" expanded="false">
      <fr:frontmatter>
        <fr:authors>
          <fr:author>
            <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
          </fr:author>
        </fr:authors>
        <fr:date>
          <fr:year>2025</fr:year>
          <fr:month>7</fr:month>
          <fr:day>1</fr:day>
        </fr:date>
        <fr:uri>https://kellenkanarios.com/0086/</fr:uri>
        <fr:display-uri>0086</fr:display-uri>
        <fr:route>/0086/</fr:route>
        <fr:title text="CS336 Lecture 1"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 1</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (2/66)</html:p>
        <html:p>First they give a brief outline of the course (provided below) along with some of the challenges and problem definitions for each of the sections</html:p>
        <fr:tree show-metadata="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>3</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/DEQT/</fr:uri>
            <fr:display-uri>DEQT</fr:display-uri>
            <fr:route>/DEQT/</fr:route>
            <fr:title text="Course outline">Course outline</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Basics">Basics</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Tokenization</html:li>
  <html:li>Architecture</html:li>
  <html:li>Loss function</html:li>
  <html:li>Optimizer</html:li>
  <html:li>Learning rate</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Systems">Systems</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> Squeeze the most out of the hardware. </html:p>
<html:ol><html:li><html:strong>Kernels</html:strong></html:li>
  <html:p>Basic idea is that the GPU needs to perform computation, but the data cannot fit on the GPU. GPU is the <html:em>factory</html:em> memory is the <html:em>warehouse</html:em></html:p>
  
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
    <html:em>How do we best organize computation to maximize utilization of GPUs by minimizing data movement?</html:em>
  </html:div>

  <html:li><html:strong>Parallelism</html:strong></html:li>
  <html:p>Beyond one GPU, we must then learn how to scale these ideas to multiple GPUs.</html:p>
  <html:ul><html:li>Sharding</html:li>
    <html:li>{data,tensor,piple,sequence} parallelism</html:li>
    <html:li>Quantization</html:li>
    <html:li>Activation checkpointing</html:li>
    <html:li>CPU offloading</html:li></html:ul>
  <html:li><html:strong>Inference</html:strong></html:li>
  <html:p>Two phases: prefill and decode</html:p>
  <html:ul><html:li>In prefill all tokens are given and you just want to output the next coding (compute bound).</html:li>
    <html:li>In decoding, we need to output one token at a time (memory-bound). </html:li></html:ul></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Scaling Laws">Scaling Laws</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> do experiments at small scale in order to pick hyperparameters for much more expensive runs at large scale.</html:p>
<html:ol><html:li>Scaling sequence</html:li>
  <html:li>Model complexity</html:li>
  <html:li>Loss metric</html:li>
  <html:li>Parametric form</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Data">Data</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> what we want the model to actually do i.e. pick your data for your task.</html:p>
<html:ol><html:li>Evaluation</html:li>
  <html:li>Curation</html:li>
  <html:li>Transformation</html:li>
  <html:li>Filtering</html:li>
  <html:li>Deduplication</html:li>
  <html:li>Mixing</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Alignment">Alignment</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Supervised fine-tuning</html:li>
  <html:li>Reinforcement learning</html:li>
  <html:li>Preference data</html:li>
  <html:li>Synthetic data</html:li>
  <html:li>Verifiers</html:li></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0087/</fr:uri>
            <fr:display-uri>0087</fr:display-uri>
            <fr:route>/0087/</fr:route>
            <fr:title text="Tokenization">Tokenization</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Introduction">Introduction</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
A <html:em>tokenizer</html:em> is responsible for taking text and turning it into a numerical representation we can pass to a neural network i.e.
<fr:tex display="block"><![CDATA[\mathsf {encode}(\text {hello world}) \mapsto  \begin {bmatrix} 24912, & 2375 \end {bmatrix} ]]></fr:tex>
    However, we also need 
    <fr:tex display="block"><![CDATA[\mathsf {decode}(\begin {bmatrix} 24912, & 2375 \end {bmatrix}) \mapsto  \text {hello world} 
    ]]></fr:tex>
 When defining a "good" tokenizer, we are concerned with the <html:em>compression ratio</html:em> i.e. the # of bytes per token.
    </html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Character-based tokenization">Character-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Trivially, we can do <html:em>character-based tokenization</html:em> i.e. mapping each character to a unique number. For example, <fr:tex display="inline"><![CDATA[\mathsf {chr}(97) = \text {``a''}]]></fr:tex> and <fr:tex display="inline"><![CDATA[\mathsf {ord}(\text {``a''}) = 97]]></fr:tex>. The compression ratio will be <fr:tex display="inline"><![CDATA[> 1]]></fr:tex> because each token corresponds to a character potentially consisting of more than one byte.</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problems:</html:strong>
  <html:ol><html:li>Very large vocabulary</html:li>
    <html:li>Many characters are quite rare.</html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
There is a fundamental <html:strong>tradeoff</html:strong> between compression ratio and vocabulary size. As an example, suppose we map every sequence of <fr:tex display="inline"><![CDATA[n]]></fr:tex> bytes to a token. This would achieve a compression ratio of <fr:tex display="inline"><![CDATA[n]]></fr:tex>. However, this would require <fr:tex display="inline"><![CDATA[256^n]]></fr:tex> tokens in our vocabulary.
</fr:mainmatter></fr:tree>

<html:p>
  Problem 2 was not entirely straight-forward to me. This actually reminds me of an idea from my <fr:link href="/004B/" title="Notebook: Information Theory" uri="https://kellenkanarios.com/004B/" display-uri="004B" type="local">information theory</fr:link> course, where we can assume we are given a "prior" over human language then we want to map low-probability characters to large strings in order to minimize the expected length (see <fr:link href="/005Z/" title="Huffman Code" uri="https://kellenkanarios.com/005Z/" display-uri="005Z" type="local">huffman coding</fr:link>).
</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte-based tokenization">Byte-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>In byte-based tokenization, each byte is a token achieving a compression ratio of exactly <fr:tex display="inline"><![CDATA[1]]></fr:tex>, where a byte <fr:tex display="inline"><![CDATA[\mapsto  [0, 256]]]></fr:tex>
. This is very elegant in the sense that our vocabulary size is fixed at <fr:tex display="inline"><![CDATA[256]]></fr:tex>.
</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong>
However, with this, the length of our encoding is # of bytes in the string, which will become too long.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Word-based tokenization">Word-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
In <html:em>word-based tokenization</html:em>, we simply split the string into the corresponding words then map each of these to a unique integer.
    </html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong> The vocabulary size is "unbounded" i.e. if we see a word we have not seen before then we need to extend the vocabulary size.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte Pair Encoding">Byte Pair Encoding</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>For the <html:em>byte pair encoding</html:em> (BPE), we instead <html:em>train</html:em> the tokenizer on raw text to automatically determine the vocabulary.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In GPT2, they first split the raw text into words and then do this byte pair encoding.
</fr:mainmatter></fr:tree>

<html:p><html:strong>Algorithm:</html:strong></html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
<html:em>while</html:em> <html:strong>true</html:strong>:
<html:ol><html:li>Count occurences of byte pairs. Store in dict <fr:tex display="inline"><![CDATA[D]]></fr:tex> with <fr:tex display="inline"><![CDATA[\{(b_1, b_2): \text {count}\}]]></fr:tex></html:li>
  <html:li>Find <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> with highest count.</html:li>
  <html:li>Merge <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> by <fr:tex display="inline"><![CDATA[(b_{1}, b_{2}) \mapsto  b']]></fr:tex></html:li>
  <html:li>Replace all instances of <fr:tex display="inline"><![CDATA[(b_{1}, b_{2})]]></fr:tex> in string with <fr:tex display="inline"><![CDATA[b']]></fr:tex> i.e. 
  <fr:tex display="block"><![CDATA[b_{1} b_{2} b_{3} b_{4} \mapsto  b'b_{3} b_{4}]]></fr:tex></html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  There are a few additional implementation details to consider for project 1.
<html:ol><html:li><fr:tex display="inline"><![CDATA[\mathsf {encode}]]></fr:tex> loops over all merges. Only loop over merges that matter.</html:li>
  <html:li>Detect and preserve special tokens (e.g. &lt;|endoftext|&gt;)</html:li>
  <html:li>Use pre-tokenization (GPT2 regex)</html:li></html:ol>
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
  </fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>3</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/DEQS/</fr:uri>
            <fr:display-uri>DEQS</fr:display-uri>
            <fr:route>/DEQS/</fr:route>
            <fr:title text="CS336 Lecture 2"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 2</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (4/66)</html:p>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Motivating Questions">Motivating Questions</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>We start by discussing a few essential questions one hopes to answer when they set out to train a large model.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>How long to train a 70b parameter model on 15T tokens on 1024 H100s?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>Mysteriously, we spawn from the ether (will be explained later) that <html:code>total_flops = 6 * 70e9 * 15e12</html:code>. Intuitively, we know that we need to perform some computation for each parameter for each token. However, the 6 is still an ominous mystery. Given the total flops, we then can just take the GPUs FLOP/s multiply by the number of seconds in the day and divide <html:code>total_flops</html:code> by this number i.e. 
<html:code>days = total_flops / flops_per_day</html:code>
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In practice, we must also consider the <html:em>model flop utilization</html:em> (MFU). Basically, our algorithm cannot utilize the entirety of the maximum FLOP/s provided by the hardware due to things like memory bandwidth, etc. To account for this in the calculation, they just take <html:code>flops_per_day / 2</html:code>.
</fr:mainmatter></fr:tree>



    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  What is the largest model that you can train on 8 H100s using AdamW?
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Answer</fr:taxon></fr:frontmatter><fr:mainmatter>
  An H100 has 80gb of shared memory i.e. <html:code>h100_bytes = 80e9</html:code>. The calculation then proceeds as
  <html:pre><![CDATA[# parameters, gradients, optimizer state
bytest_per_param = 4 + 4 + (4 + 4)
num_parameters = (h100_bytes * 8) / bytes_per_parameter]]></html:pre>
  what we need each of parameters, gradients, optimizer state will become clear later.
</fr:mainmatter></fr:tree>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In practice, we also need to account for activations, which will depend on <html:strong>both</html:strong> batch size and sequence length.
</fr:mainmatter></fr:tree>

  </fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:uri>https://kellenkanarios.com/DEQU/</fr:uri><fr:display-uri>DEQU</fr:display-uri><fr:route>/DEQU/</fr:route><fr:title text="Memory Accounting">Memory Accounting</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:strong>Tensor Memory</html:strong>:
Tensors are stored as a <html:em>floating point number</html:em>. Memory is determined by (i) number of values in tensor and (ii) data type of each value.
</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:uri>https://kellenkanarios.com/DEQV/</fr:uri><fr:display-uri>DEQV</fr:display-uri><fr:route>/DEQV/</fr:route><fr:title text="Low Precision Data Types">Low Precision Data Types</fr:title></fr:frontmatter><fr:mainmatter><html:p><html:code>float16</html:code>: here, we decrease both the fraction and exponent bits. Importantly, there are only <fr:tex display="inline"><![CDATA[5]]></fr:tex> exponent bits making the effective range <fr:tex display="inline"><![CDATA[(2^{-6}, 2^{6})]]></fr:tex>. The dynamic range for small numbers is pretty bad. This can cause small numbers to "die" effectively killing these neurons for the remainder of learning due to backpropagation.
</html:p><html:p><html:code>bfloat16</html:code>: to fix the problems with <html:code>float16</html:code>, they instead keep <fr:tex display="inline"><![CDATA[8]]></fr:tex> exponent bits and reduce the fraction bits to <fr:tex display="inline"><![CDATA[7]]></fr:tex>. This maintains the same dynamic range as <html:code>float32</html:code> while still only using <fr:tex display="inline"><![CDATA[16]]></fr:tex> bits. 
</html:p><html:p><html:code>fp8</html:code>: Two variants
<html:ol><html:li>E4M3: has 4 exponent bits and 3 mantissa bits</html:li> 
<html:li>E5M2: has 5 exponent bits and 2 mantissa bits</html:li></html:ol>
Requires modern Hopper Architecture i.e. H100 to perform operations. Also supported by TPUs.
</html:p></fr:mainmatter></fr:tree><html:p><html:strong>Training Implications</html:strong>
<html:ol><html:li><html:code>float32</html:code> training works but requires lots of memory</html:li>
  <html:li>Using <fr:link href="/DEQV/" title="Low Precision Data Types" uri="https://kellenkanarios.com/DEQV/" display-uri="DEQV" type="local">lower precision data types</fr:link> allows you to train larger models due to lower memory requirements, but can cause instability</html:li>
  <html:li>To optimize this tradeoff, use <html:em>mixed-precision training</html:em>.</html:li></html:ol></html:p></fr:mainmatter></fr:tree><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:uri>https://kellenkanarios.com/DYIV/</fr:uri><fr:display-uri>DYIV</fr:display-uri><fr:route>/DYIV/</fr:route><fr:title text="Torch Tensors">Torch Tensors</fr:title></fr:frontmatter><fr:mainmatter><html:p>
In pytorch, tensors are <html:em>contiguous</html:em> blocks of memory, where their shape is controlled by <html:em>strides</html:em>. A stride basically just refers to how far you have to jump in physical memory to correspond to incrementing the logical index i.e. in a (2, 3) matrix each row requires jumping <fr:tex display="inline"><![CDATA[3]]></fr:tex> spots in physical memory. Thus, <html:code>stride[1] = 3</html:code>.
  </html:p>
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Calling <html:code>.view((m, n))</html:code> just changes the stride but does <html:strong>not</html:strong> modify the physical memory. If you call <html:code>.view</html:code> that results in a non-contiguous view, then you cannot call <html:code>.view</html:code> again because it assumes contiguous indexing to properly adjust the strides. To deal with this, they have <html:code>.contiguous()</html:code> and <html:code>.reshape()</html:code> is basically <html:code>.contiguous().view()</html:code>.
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
All <html:strong>elementwise</html:strong> operations make a copy i.e. <html:code>x + y</html:code>. A useful function for masked attention is <html:code>triu()</html:code>.
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:title text="Einops">Einops</fr:title></fr:frontmatter><fr:mainmatter>
Einops is a useful python library to keep track of high-dimensional matrix operations i.e. matmuls over <html:code>batch_size</html:code>, <html:code>seqlen</html:code>, etc. High level overview
<html:ul><html:li>Initialize like <html:code>X[float.tensor, "batch seq hidden"] = torch.tensor((2, 3, 4))</html:code></html:li>
  <html:li>Can perform matmul with named dims like</html:li>
  <html:code>z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2")</html:code>
  <html:li><html:code>reduce</html:code>: <html:code>x = reduce(x, "x y z -&gt; x y 1")</html:code> is just like <html:code>mean(x, axis=-1)</html:code></html:li>
  <html:li><html:code>rearrange</html:code>: <html:code>x = rearrange(x, "... (heads hidden1) -&gt; ... heads hidden 1")</html:code></html:li></html:ul>
</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Computation">Computation</fr:title></fr:frontmatter><fr:mainmatter>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>12</fr:day></fr:date><fr:uri>https://kellenkanarios.com/4K5H/</fr:uri><fr:display-uri>4K5H</fr:display-uri><fr:route>/4K5H/</fr:route><fr:title text="Floating Point Operations">Floating Point Operations</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>We refer to the <html:strong>total</html:strong> number of floating point operatings as <html:em>FLOPS</html:em>. Alternatively, we refer to the number of floating point operations per second as <html:em>FLOP/s</html:em>.</html:p></fr:mainmatter></fr:tree>
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>12</fr:day></fr:date><fr:uri>https://kellenkanarios.com/99G8/</fr:uri><fr:display-uri>99G8</fr:display-uri><fr:route>/99G8/</fr:route><fr:title text="Model FLOP Utilization (MFU)">Model FLOP Utilization (MFU)</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>Given a promised <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOP/s</fr:link>, the <html:em>model flop utilization</html:em> is given by
<fr:tex display="block"><![CDATA[\frac {\text {actual FLOP/s}}{\text {promised FLOP/s}}]]></fr:tex></html:p></fr:mainmatter></fr:tree>

   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Example</fr:taxon></fr:frontmatter><fr:mainmatter>
Performing a matrix multiplication <fr:tex display="inline"><![CDATA[A \cdot  B]]></fr:tex> for <fr:tex display="inline"><![CDATA[A \in  \mathbb {R}^{(B \times  D)}]]></fr:tex> and <fr:tex display="inline"><![CDATA[B \in  \mathbb {R}^{(D \times  K)}]]></fr:tex>, requires <fr:tex display="inline"><![CDATA[2 \times  B \times  D \times  K]]></fr:tex> <fr:link href="/4K5H/" title="Floating Point Operations" uri="https://kellenkanarios.com/4K5H/" display-uri="4K5H" type="local">FLOPS</fr:link></fr:mainmatter></fr:tree>
 

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Optimizers">Optimizers</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>I hope to do a formal blog touring optimizers because I have written a paper on deep learning optimizers for my <fr:link href="/0033/" title="Notebook: Optimization Theory" uri="https://kellenkanarios.com/0033/" display-uri="0033" type="local">optimization course</fr:link>. Due to this, I will leave this blank for now. A brief list of the covered concepts are below:</html:p>
<html:ul><html:li>momentum</html:li>
  <html:li>adagrad</html:li>
  <html:li>RMSProp</html:li>
  <html:li>Adam</html:li></html:ul>
<html:p>A key formula to remember:
<fr:tex display="block"><![CDATA[\text {total memory} \approx  4 \times  (\text {params} + \text {activations} + \text {grad size} + \text {optimizer state size})]]></fr:tex></html:p>
</fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
Another useful trick is to use <fr:link href="/DEQV/" title="Low Precision Data Types" uri="https://kellenkanarios.com/DEQV/" display-uri="DEQV" type="local">low precision</fr:link> for forward pass activations, then use <html:code>float32</html:code> for gradients and network parameters.
</fr:mainmatter></fr:tree>
</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/0086/</fr:uri>
            <fr:display-uri>0086</fr:display-uri>
            <fr:route>/0086/</fr:route>
            <fr:title text="CS336 Lecture 1"><fr:link href="/0085/" title="Notebook: Stanford CS336" uri="https://kellenkanarios.com/0085/" display-uri="0085" type="local">CS336</fr:link> Lecture 1</fr:title>
          </fr:frontmatter>
          <fr:mainmatter>
            <html:p><fr:link href="/007U/" title="Deliberate-Practice" uri="https://kellenkanarios.com/007U/" display-uri="007U" type="local">Deliberate-Practice</fr:link> (2/66)</html:p>
            <html:p>First they give a brief outline of the course (provided below) along with some of the challenges and problem definitions for each of the sections</html:p>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors />
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>3</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/DEQT/</fr:uri>
                <fr:display-uri>DEQT</fr:display-uri>
                <fr:route>/DEQT/</fr:route>
                <fr:title text="Course outline">Course outline</fr:title>
              </fr:frontmatter>
              <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Basics">Basics</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Tokenization</html:li>
  <html:li>Architecture</html:li>
  <html:li>Loss function</html:li>
  <html:li>Optimizer</html:li>
  <html:li>Learning rate</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Systems">Systems</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> Squeeze the most out of the hardware. </html:p>
<html:ol><html:li><html:strong>Kernels</html:strong></html:li>
  <html:p>Basic idea is that the GPU needs to perform computation, but the data cannot fit on the GPU. GPU is the <html:em>factory</html:em> memory is the <html:em>warehouse</html:em></html:p>
  
 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
    <html:em>How do we best organize computation to maximize utilization of GPUs by minimizing data movement?</html:em>
  </html:div>

  <html:li><html:strong>Parallelism</html:strong></html:li>
  <html:p>Beyond one GPU, we must then learn how to scale these ideas to multiple GPUs.</html:p>
  <html:ul><html:li>Sharding</html:li>
    <html:li>{data,tensor,piple,sequence} parallelism</html:li>
    <html:li>Quantization</html:li>
    <html:li>Activation checkpointing</html:li>
    <html:li>CPU offloading</html:li></html:ul>
  <html:li><html:strong>Inference</html:strong></html:li>
  <html:p>Two phases: prefill and decode</html:p>
  <html:ul><html:li>In prefill all tokens are given and you just want to output the next coding (compute bound).</html:li>
    <html:li>In decoding, we need to output one token at a time (memory-bound). </html:li></html:ul></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Scaling Laws">Scaling Laws</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> do experiments at small scale in order to pick hyperparameters for much more expensive runs at large scale.</html:p>
<html:ol><html:li>Scaling sequence</html:li>
  <html:li>Model complexity</html:li>
  <html:li>Loss metric</html:li>
  <html:li>Parametric form</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Data">Data</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p><html:strong>Goal:</html:strong> what we want the model to actually do i.e. pick your data for your task.</html:p>
<html:ol><html:li>Evaluation</html:li>
  <html:li>Curation</html:li>
  <html:li>Transformation</html:li>
  <html:li>Filtering</html:li>
  <html:li>Deduplication</html:li>
  <html:li>Mixing</html:li></html:ol>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>3</fr:day></fr:date><fr:title text="Alignment">Alignment</fr:title></fr:frontmatter><fr:mainmatter>
<html:ol><html:li>Supervised fine-tuning</html:li>
  <html:li>Reinforcement learning</html:li>
  <html:li>Preference data</html:li>
  <html:li>Synthetic data</html:li>
  <html:li>Verifiers</html:li></html:ol>
</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
            <fr:tree show-metadata="false">
              <fr:frontmatter>
                <fr:authors>
                  <fr:author>
                    <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
                  </fr:author>
                </fr:authors>
                <fr:date>
                  <fr:year>2025</fr:year>
                  <fr:month>7</fr:month>
                  <fr:day>1</fr:day>
                </fr:date>
                <fr:uri>https://kellenkanarios.com/0087/</fr:uri>
                <fr:display-uri>0087</fr:display-uri>
                <fr:route>/0087/</fr:route>
                <fr:title text="Tokenization">Tokenization</fr:title>
              </fr:frontmatter>
              <fr:mainmatter>
  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Introduction">Introduction</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
A <html:em>tokenizer</html:em> is responsible for taking text and turning it into a numerical representation we can pass to a neural network i.e.
<fr:tex display="block"><![CDATA[\mathsf {encode}(\text {hello world}) \mapsto  \begin {bmatrix} 24912, & 2375 \end {bmatrix} ]]></fr:tex>
    However, we also need 
    <fr:tex display="block"><![CDATA[\mathsf {decode}(\begin {bmatrix} 24912, & 2375 \end {bmatrix}) \mapsto  \text {hello world} 
    ]]></fr:tex>
 When defining a "good" tokenizer, we are concerned with the <html:em>compression ratio</html:em> i.e. the # of bytes per token.
    </html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Character-based tokenization">Character-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>Trivially, we can do <html:em>character-based tokenization</html:em> i.e. mapping each character to a unique number. For example, <fr:tex display="inline"><![CDATA[\mathsf {chr}(97) = \text {``a''}]]></fr:tex> and <fr:tex display="inline"><![CDATA[\mathsf {ord}(\text {``a''}) = 97]]></fr:tex>. The compression ratio will be <fr:tex display="inline"><![CDATA[> 1]]></fr:tex> because each token corresponds to a character potentially consisting of more than one byte.</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problems:</html:strong>
  <html:ol><html:li>Very large vocabulary</html:li>
    <html:li>Many characters are quite rare.</html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
There is a fundamental <html:strong>tradeoff</html:strong> between compression ratio and vocabulary size. As an example, suppose we map every sequence of <fr:tex display="inline"><![CDATA[n]]></fr:tex> bytes to a token. This would achieve a compression ratio of <fr:tex display="inline"><![CDATA[n]]></fr:tex>. However, this would require <fr:tex display="inline"><![CDATA[256^n]]></fr:tex> tokens in our vocabulary.
</fr:mainmatter></fr:tree>

<html:p>
  Problem 2 was not entirely straight-forward to me. This actually reminds me of an idea from my <fr:link href="/004B/" title="Notebook: Information Theory" uri="https://kellenkanarios.com/004B/" display-uri="004B" type="local">information theory</fr:link> course, where we can assume we are given a "prior" over human language then we want to map low-probability characters to large strings in order to minimize the expected length (see <fr:link href="/005Z/" title="Huffman Code" uri="https://kellenkanarios.com/005Z/" display-uri="005Z" type="local">huffman coding</fr:link>).
</html:p>
</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte-based tokenization">Byte-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>In byte-based tokenization, each byte is a token achieving a compression ratio of exactly <fr:tex display="inline"><![CDATA[1]]></fr:tex>, where a byte <fr:tex display="inline"><![CDATA[\mapsto  [0, 256]]]></fr:tex>
. This is very elegant in the sense that our vocabulary size is fixed at <fr:tex display="inline"><![CDATA[256]]></fr:tex>.
</html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong>
However, with this, the length of our encoding is # of bytes in the string, which will become too long.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Word-based tokenization">Word-based tokenization</fr:title></fr:frontmatter><fr:mainmatter>
  <html:p>
In <html:em>word-based tokenization</html:em>, we simply split the string into the corresponding words then map each of these to a unique integer.
    </html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
  <html:strong>Problem:</html:strong> The vocabulary size is "unbounded" i.e. if we see a word we have not seen before then we need to extend the vocabulary size.
</html:div>

</fr:mainmatter></fr:tree>

  <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Byte Pair Encoding">Byte Pair Encoding</fr:title></fr:frontmatter><fr:mainmatter>
<html:p>For the <html:em>byte pair encoding</html:em> (BPE), we instead <html:em>train</html:em> the tokenizer on raw text to automatically determine the vocabulary.</html:p>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
In GPT2, they first split the raw text into words and then do this byte pair encoding.
</fr:mainmatter></fr:tree>

<html:p><html:strong>Algorithm:</html:strong></html:p>

 <html:div class="jms-indent-block" style="
  margin-left: 1em;
  margin-bottom: 1em;
  border-left: 4px solid black;
  padding-left: 12px;
  background-color: whitesmoke;
  ">
<html:em>while</html:em> <html:strong>true</html:strong>:
<html:ol><html:li>Count occurences of byte pairs. Store in dict <fr:tex display="inline"><![CDATA[D]]></fr:tex> with <fr:tex display="inline"><![CDATA[\{(b_1, b_2): \text {count}\}]]></fr:tex></html:li>
  <html:li>Find <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> with highest count.</html:li>
  <html:li>Merge <fr:tex display="inline"><![CDATA[(b_1, b_2)]]></fr:tex> by <fr:tex display="inline"><![CDATA[(b_{1}, b_{2}) \mapsto  b']]></fr:tex></html:li>
  <html:li>Replace all instances of <fr:tex display="inline"><![CDATA[(b_{1}, b_{2})]]></fr:tex> in string with <fr:tex display="inline"><![CDATA[b']]></fr:tex> i.e. 
  <fr:tex display="block"><![CDATA[b_{1} b_{2} b_{3} b_{4} \mapsto  b'b_{3} b_{4}]]></fr:tex></html:li></html:ol>
</html:div>


    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>
  There are a few additional implementation details to consider for project 1.
<html:ol><html:li><fr:tex display="inline"><![CDATA[\mathsf {encode}]]></fr:tex> loops over all merges. Only loop over merges that matter.</html:li>
  <html:li>Detect and preserve special tokens (e.g. &lt;|endoftext|&gt;)</html:li>
  <html:li>Use pre-tokenization (GPT2 regex)</html:li></html:ol>
</fr:mainmatter></fr:tree>

</fr:mainmatter></fr:tree>
</fr:mainmatter>
            </fr:tree>
          </fr:mainmatter>
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2025</fr:year>
              <fr:month>7</fr:month>
              <fr:day>1</fr:day>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/003W/</fr:uri>
            <fr:display-uri>003W</fr:display-uri>
            <fr:route>/003W/</fr:route>
            <fr:title text="LLMs">LLMs</fr:title>
          </fr:frontmatter>
          <fr:mainmatter><html:p>This will be my backlog accumulator / dump of related resources for my journey through the dubious waters of <fr:link href="/stanford/" title="Stanford University" uri="https://kellenkanarios.com/stanford/" display-uri="stanford" type="local">Stanford's</fr:link> <fr:link href="https://stanford-cs336.github.io/spring2025/index.htmlschedule" type="external">CS336</fr:link> and the broader journey toward understanding industry-level LLMs.</html:p>


  <fr:tree show-metadata="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kellenkanarios.com/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>1</fr:day></fr:date><fr:title text="Reading List / Useful Resources">Reading List / Useful Resources</fr:title></fr:frontmatter><fr:mainmatter>
    <html:ul><html:li><fr:link href="https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361" type="external">PPO + LLM Tricks</fr:link></html:li></html:ul>
</fr:mainmatter></fr:tree>

</fr:mainmatter>
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kellenkanarios.com/stanford/</fr:uri>
            <fr:display-uri>stanford</fr:display-uri>
            <fr:route>/stanford/</fr:route>
            <fr:title text="Stanford University">Stanford University</fr:title>
            <fr:taxon>Institute</fr:taxon>
            <fr:meta name="external">https://www.stanford.edu/</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
