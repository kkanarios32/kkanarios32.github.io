<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors />
    <fr:date>
      <fr:year>2025</fr:year>
      <fr:month>7</fr:month>
      <fr:day>9</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/DZ1D/</fr:uri>
    <fr:display-uri>DZ1D</fr:display-uri>
    <fr:route>/DZ1D/</fr:route>
    <fr:title text="FM Gradient Equivalence"><fr:link href="/lipmanFlowMatchingGenerative2023/" title="Flow Matching for Generative Modeling" uri="https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/" display-uri="lipmanFlowMatchingGenerative2023" type="local">FM</fr:link> Gradient Equivalence</fr:title>
    <fr:taxon>Theorem</fr:taxon>
  </fr:frontmatter>
  <fr:mainmatter>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2025</fr:year><fr:month>7</fr:month><fr:day>9</fr:day></fr:date><fr:taxon>Proof</fr:taxon></fr:frontmatter><fr:mainmatter>

</fr:mainmatter></fr:tree>
 

</fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>Yaron Lipman</fr:author>
              <fr:author>Ricky T. Q. Chen</fr:author>
              <fr:author>Heli Ben-Hamu</fr:author>
              <fr:author>Maximilian Nickel</fr:author>
              <fr:author>Matt Le</fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2023</fr:year>
              <fr:month>2</fr:month>
            </fr:date>
            <fr:uri>https://kellenkanarios.com/lipmanFlowMatchingGenerative2023/</fr:uri>
            <fr:display-uri>lipmanFlowMatchingGenerative2023</fr:display-uri>
            <fr:route>/lipmanFlowMatchingGenerative2023/</fr:route>
            <fr:title text="Flow Matching for Generative Modeling">Flow Matching for Generative Modeling</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="doi">10.48550/arXiv.2210.02747</fr:meta>
            <fr:meta name="external">https://arxiv.org/abs/2210.02747</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{lipmanFlowMatchingGenerative2023,
 title = {Flow {{Matching}} for {{Generative Modeling}}},
 author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},
 year = {2023},
 doi = {10.48550/arXiv.2210.02747},
 urldate = {2025-06-20},
 number = {arXiv:2210.02747},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/WMMQDW4B/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/home/kellen/Downloads/pdfs/storage/R693NNL4/2210.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
 primaryclass = {cs},
 eprint = {2210.02747},
 month = {February}
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
