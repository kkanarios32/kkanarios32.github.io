<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Sungryull Sohn</fr:author>
      <fr:author>Junhyuk Oh</fr:author>
      <fr:author>Honglak Lee</fr:author>
    </fr:authors>
    <fr:uri>https://kellenkanarios.com/sohnHierarchicalReinforcementLearning/</fr:uri>
    <fr:display-uri>sohnHierarchicalReinforcementLearning</fr:display-uri>
    <fr:route>/sohnHierarchicalReinforcementLearning/</fr:route>
    <fr:title text="Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies">Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="bibtex"><![CDATA[@article{sohnHierarchicalReinforcementLearning,
 title = {Hierarchical {{Reinforcement Learning}} for {{Zero-shot Generalization}} with {{Subtask Dependencies}}},
 author = {Sohn, Sungryull and Oh, Junhyuk and Lee, Honglak},
 file = {/home/kellen/Downloads/pdfs/storage/6TK6ATBU/Sohn et al. - Hierarchical Reinforcement Learning for Zero-shot .pdf},
 langid = {english},
 abstract = {We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradientbased policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
