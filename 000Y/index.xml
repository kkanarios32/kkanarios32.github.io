<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors />
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>11</fr:month>
      <fr:day>22</fr:day>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/000Y/</fr:uri>
    <fr:display-uri>000Y</fr:display-uri>
    <fr:route>/000Y/</fr:route>
    <fr:title text="Markov Decision Processes">Markov Decision Processes</fr:title>
  </fr:frontmatter>
  <fr:mainmatter><html:p>
In reinforcement learning, the interactions between the agent and the environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP).
</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:uri>https://kellenkanarios.com/000Z/</fr:uri><fr:display-uri>000Z</fr:display-uri><fr:route>/000Z/</fr:route><fr:title text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>
A Markov Decision Process <fr:tex display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<html:ul><html:li>A state space <fr:tex display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</html:li>
  <html:li>A action space <fr:tex display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex display="inline"><![CDATA[A]]></fr:tex> is finite. </html:li>
  <html:li>A transition function <fr:tex display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex display="inline"><![CDATA[s]]></fr:tex>.</html:li>
  <html:li>A reward function <fr:tex display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex display="inline"><![CDATA[a]]></fr:tex>, <fr:tex display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</html:li>
  <html:li>A discount factor <fr:tex display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</html:li></html:ul></html:p></fr:mainmatter></fr:tree><html:p>From an MDP and a policy <fr:tex display="inline"><![CDATA[\pi  : S \to  A]]></fr:tex>, we can define a Value function for <fr:tex display="inline"><![CDATA[\pi ]]></fr:tex>.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:uri>https://kellenkanarios.com/0010/</fr:uri><fr:display-uri>0010</fr:display-uri><fr:route>/0010/</fr:route><fr:title text="Value Function">Value Function</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>
For a fixed policy and a starting state <fr:tex display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the value function <fr:tex display="inline"><![CDATA[V_{M}^{s}:S\to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards

<fr:tex display="block"><![CDATA[V_{M}^{s}(s)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ]]]></fr:tex></html:p></fr:mainmatter></fr:tree><html:p>The expectation is over the randomness of the the transitions <fr:tex display="inline"><![CDATA[P]]></fr:tex> and if the policy <fr:tex display="inline"><![CDATA[\pi ]]></fr:tex> is stochastic. When determining a policy we are more interested with the value of an action in a specific state rather than just the state itself. To get this, we can define a <fr:tex display="inline"><![CDATA[Q]]></fr:tex>-function in a similar way.</html:p><fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:uri>https://kellenkanarios.com/0011/</fr:uri><fr:display-uri>0011</fr:display-uri><fr:route>/0011/</fr:route><fr:title text="Action Value Function">Action Value Function</fr:title><fr:taxon>Definition</fr:taxon></fr:frontmatter><fr:mainmatter><html:p>
For a fixed policy and a starting state <fr:tex display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the action value function <fr:tex display="inline"><![CDATA[Q_{M}^{s}:S \times  A \to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards after taking action <fr:tex display="inline"><![CDATA[A]]></fr:tex>.

<fr:tex display="block"><![CDATA[Q_{M}^{s}(s, a)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi , s_{0}=s, a_{0} = a\Big ]]]></fr:tex></html:p></fr:mainmatter></fr:tree><html:p>If the reward <fr:tex display="inline"><![CDATA[r(s,a)]]></fr:tex> is bounded by some <fr:tex display="inline"><![CDATA[R_{\text {max}}]]></fr:tex>. Then we can trivially bound both the value and action value function by <fr:tex display="inline"><![CDATA[(R_{max}/1 - \gamma )]]></fr:tex></html:p>
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:taxon>Proof</fr:taxon></fr:frontmatter><fr:mainmatter>
    This is just a geometric series i.e. 
<fr:tex display="block"><![CDATA[\begin {align*}
V_{M}^{s}(s)&=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ] \\
&\leq  \mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}R_{\text {max}}\mid \pi ,s_{0}=s\Big ] \\
&= R_{\text {max}} \sum _{t=0}^{\infty }\gamma ^{t} \\
&= R_{\text {max}}/ 1 - \gamma 
    \end {align*}
  ]]></fr:tex>
  </fr:mainmatter></fr:tree>
 

</fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
