<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
      </fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>11</fr:month>
      <fr:day>24</fr:day>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/0012/</fr:uri>
    <fr:display-uri>0012</fr:display-uri>
    <fr:route>/0012/</fr:route>
    <fr:title text="Notes on Reinforcement Learning: An Introduction">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:title>
  </fr:frontmatter>
  <fr:mainmatter><html:p>This won't really be notes, my lab has begun taking a few undergrads who are not familiar with RL. I will be onboarding them by going through <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link>. Specifically, I will be following the <fr:link href="/richardsutton/" title="Richard Sutton" uri="https://kkanarios32.github.io/richardsutton/" display-uri="richardsutton" type="local">Richard Sutton</fr:link> flavor of RL through his <fr:link href="https://drive.google.com/drive/folders/0B3w765rOKuKANmxNbXdwaE1YU1k?resourcekey=0-JZz-noRuJgogNsg1ljgV8w" type="external">CMPUT 609</fr:link> course. You know what the say: the best way to learn is to teach! Here, I will try to solve all the problems as we go and prepare some questions for the undergrads for when we meet. Additionally, I will document my foray into more advanced exploration of topics introduced in the textbook.</html:p>
  
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Multi-armed Bandits">Multi-armed Bandits</fr:title></fr:frontmatter><fr:mainmatter>

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.1]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.1" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.1]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  In the case of two actions, (assuming that <fr:tex display="inline"><![CDATA[Q_*(a_1) > Q_*(a_2)]]></fr:tex>) the probability of the greedy action being selected is <fr:tex display="inline"><![CDATA[1 - \epsilon ]]></fr:tex> or 0.5.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.2]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.2" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.2]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  In the long run <fr:tex display="inline"><![CDATA[\epsilon  = 0.01]]></fr:tex>, would achieve the highest reward. The expected reward can be written as
<fr:tex display="block"><![CDATA[
    \begin {align*}
      R^* = \max _{a} q^*(a) + \frac {\epsilon }{n}\sum _{i \neq  a}^{n} q^*(i)
    \end {align*}
  ]]></fr:tex>
  The exception is that if <fr:tex display="inline"><![CDATA[\epsilon  = 0]]></fr:tex>, then there are no guarantees. For example, if your initialization is zeros. Then you could have an empirical average of a suboptimal arm that is always positive and you would never deviate. Thus, to ensure you converge to the true optimal arm you need <fr:tex display="inline"><![CDATA[\epsilon  > 0]]></fr:tex>.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.3]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.3" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.3]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  TODO

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.4]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.4" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.4]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Following the steps in 2.6,
  <fr:tex display="block"><![CDATA[
      \begin {align*}
        Q_{n + 1} &= Q_n + \alpha _n [R_n - Q_n] \\
        &= Q_{n - 1} + \alpha _{n - 1}[R_{n - 1} - Q_{n - 1}] + \alpha _n \Big [R_n - (Q_{n - 1} + \alpha _{n - 1}[R_{n - 1} - Q_{n - 1}])\Big ] \\
        &\vdots  \\
        & = \left (\prod _{i = 1}^{n} (1 - \alpha _{i})\right ) Q_1 + \sum _{i = 1}^{n} \alpha _n \left (\prod _{i = 1}^{n} (1 - \alpha _{n - i})\right )R_i
      \end {align*}
    ]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.6]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.6" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.6]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Since all of the arms in the testbed have distributions with support less than 5, in the beginning the unpulled arm's optimistic estimates will be higher than the empirical estimates of the previously pulled arms. Therefore, in the early stages all of the arms will be pulled in a round robin fashion, and assuming deterministic rewards the optimal arm will keep being pulled until it falls below the other optimistic values.

</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.8]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.8" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.8]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Since there are 10 arms (presumably with the same initialization), the UCB bonus will enforce that the first 10 pulls are round robin (uniform) pulling of each of the 10 arms. Then on the 11th step each will have the same UCB bonus term but the one with the highest empirical reward will be pulled, leading to a spike. It then drops in subsequent steps because the UCB bonus of that arm will decrease and other (possibly less optimal) arms are pulled in subsequent steps.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.9]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.9" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.9]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  We can just expand the denominator as
<fr:tex display="block"><![CDATA[
    \begin {align*}
      \Pr \{A_t = a\} &= \frac {e^{H_t(a)}}{e^{H_t(a)} + e^{H_t(b)}} \\
      &= \frac {1}{1 + e^{\frac {H_t(b)}{H_t(a)}}} \\
      &= \frac {1}{1 + e^{-\frac {H_t(a)}{H_t(b)}}} \\
    \end {align*}
  ]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 2.10]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="2.10" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 2.10]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  In case 1, the best you can hope to achieve is <fr:tex display="inline"><![CDATA[\max _{a} \mathbb {E} [R(a)]]]></fr:tex>. In this case, we have
<fr:tex display="block"><![CDATA[
    \begin {align*}
      \mathbb {E}[R(1)] &= 0.5 \cdot  10 + 0.5 \cdot  90 = 50, \\
      \mathbb {E}[R(2)] &= 0.5 \cdot  20 + 0.5 \cdot  80 = 50
    \end {align*}
  ]]></fr:tex>
  Therefore, the best we can hope to achieve is <fr:tex display="inline"><![CDATA[50]]></fr:tex>. If we are given what case we are in, then we can achieve
<fr:tex display="block"><![CDATA[
    \begin {align*}
      R^* &= 0.5 \cdot  \max _{a}\mathbb {E}[R(a) \mid  x = 1] +
      0.5 \cdot  \max _{a}\mathbb {E}[R(a) \mid  x = 2] \\
      &= 0.5 \cdot  20 + 0.5 \cdot  90 \\
      &= 55
    \end {align*}
  ]]></fr:tex>

</fr:mainmatter></fr:tree>
  

</fr:mainmatter></fr:tree>
  

  
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Markov Decision Processes">Markov Decision Processes</fr:title></fr:frontmatter><fr:mainmatter>

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.3]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.3" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.3]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  I think you draw the line at where you can already execute the necessary behavior to get from one state to another. This is eerily familiar to the line of work known as hierarchical reinforcement learning, where you gradually learn higher level of abstraction by executing subpolicies.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.5]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.5" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.5]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  We need to add the notion of a terminal state. This can be done by just adding some <fr:tex display="inline"><![CDATA[t]]></fr:tex> to the state space <fr:tex display="inline"><![CDATA[S]]></fr:tex>. Then
  <fr:tex display="block"><![CDATA[
      \begin {cases}
        p(t, 0 \mid  s, a) = 1, &\text { if } s = t\\
        \sum _{s^{\prime }\in \mathcal {S}}\sum _{r\in \mathcal {R}}p(s^{\prime },r|s,a)=1,\mathrm {~for~all~}s\in \mathcal {S},a\in \mathcal {A}(s), &\text { if } s \neq  t
      \end {cases}
]]></fr:tex>
I think the idea is just that when you hit a terminal state you no longer can reach other states.
  
</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.6]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.6" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.6]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Let <fr:tex display="inline"><![CDATA[T]]></fr:tex> denote the length of the episode. Then the return <fr:tex display="inline"><![CDATA[G_t]]></fr:tex> would be 
<fr:tex display="block"><![CDATA[
  \begin {align*}
G_t &= R_{t+1} + \gamma  R_{t + 2} + \cdots  + \gamma ^{T - {t + 1}} R_T = - \gamma ^{T - {t + 1}}
  \end {align*}
]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.7]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.7" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.7]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  You have not effectively communicated the task. The agent has no incentive to solve the maze as fast as possible. This means that no matter the duration it took to solve the task the agent would receive the same reward. To fix this, you can give -1 reward at each time step, or use a discount factor to capture the time.

</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.8]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.8" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.8]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Due to the recursive relationship, we will start from the back
<fr:tex display="block"><![CDATA[
    \begin {align*}
      G_5 &= 0 \\
      G_4 &= R_{5} + \gamma  G_5 = 2 \\
      G_3 &= R_{4} + \gamma  G_4 = 4 \\
      G_2 &= R_{3} + \gamma  G_3 = 8 \\
      G_1 &= R_{2} + \gamma  G_2 = 6 \\
      G_0 &= R_{1} + \gamma  G_1 = 2
    \end {align*}
  ]]></fr:tex>

</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.9]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.9" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.9]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
From the definition,
<fr:tex display="block"><![CDATA[
\begin {align*}
  G_0 &= R_{1} + \gamma  R_{2} + \gamma ^2 R_{3} + \cdots  \\
&= 2 + \sum _{i = 1}^{\infty } \gamma ^{i} R_{i} \\
&= 2 + \gamma  \sum _{i = 0}^{\infty } \gamma ^{i} R_{i + 2} \\
&= 2 + \frac {7 \gamma }{1 - \gamma } \\
&= 65
\end {align*}
]]></fr:tex>
and 
<fr:tex display="block"><![CDATA[
\begin {align*}
  G_1 &= R_{2} + \gamma  R_{2} + \gamma ^2 R_{3} + \cdots  \\
&= 7 + \sum _{i = 1}^{\infty } \gamma ^{i} R_{i} \\
&= 7 + \gamma  \sum _{i = 0}^{\infty } \gamma ^{i} R_{i + 3} \\
&= 7 + \frac {7 \gamma }{1 - \gamma } \\
&= 70
\end {align*}
]]></fr:tex>
  
</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.11]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.11" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.11]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  By just expanding,
<fr:tex display="block"><![CDATA[
    \begin {align*}
      \mathbb {E}[R_{t + 1} \mid  S_{t} = s] = \sum _{a} \pi (a \mid  s) \sum _{s'} \sum _{r} p(s', r \mid  s, a) \cdot  r
    \end {align*}
  ]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.12]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.12" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.12]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  <fr:tex display="block"><![CDATA[
      \begin {align*}
        v_{\pi }(s) &= \sum _{a} \pi (a \mid  s) q_{\pi }(s, a)
      \end {align*}
    ]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.12]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.12" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.12]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
<fr:tex display="block"><![CDATA[
\begin {align*}
  q_{\pi }(s, a) = \sum _{s', r} p(s', r \mid  s, a) \big (r + \gamma  v_{\pi }(s')\big )
\end {align*}
  ]]></fr:tex>

</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.15]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.15" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.15]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  Recall that 
<fr:tex display="block"><![CDATA[
    \begin {align*}
      v_{\pi }(s) = \mathbb {E}_{\pi } [\sum _{k = 0}^{\infty } R_{t + k + 1} \mid  S_t = s]
    \end {align*}
  ]]></fr:tex>
  Adding a constant <fr:tex display="inline"><![CDATA[c]]></fr:tex>, we get
<fr:tex display="block"><![CDATA[
    \begin {align*}
      v_{\pi }'(s) &= \mathbb {E}_{\pi } [\sum _{k = 0}^{\infty } \gamma ^{k} R_{t + k + 1} + c \mid  S_t = s] \\
      &= \mathbb {E}_{\pi } [\sum _{k = 0}^{\infty } R_{t + k + 1}\mid  S_t = s] +
\sum _{k = 0}^{\infty } \gamma ^{k} c \\
      &= v_{\pi }(s) + \frac {c}{1 - \gamma }
    \end {align*}
  ]]></fr:tex>
  Therefore, <fr:tex display="inline"><![CDATA[v_c = \frac {c}{1 - \gamma }]]></fr:tex>.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.16]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.16" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.16]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  In the episodic case, this would not be the same. In the maze running example, <fr:tex display="inline"><![CDATA[v_c]]></fr:tex> would be larger for longer episodes. This would then incentivize the agent to actually take LONGER, where as we want the agent to solve the maze as fast as possible.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 3.18]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="3.18" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 3.18]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  This is similar to exercise 3.12. Namely, 
<fr:tex display="block"><![CDATA[v_{\pi }(s) = \mathbb {E}_{\pi }[q_{\pi }(s, a)] = \sum _{a} \pi (a \mid  s) q_{\pi }(s, a)]]></fr:tex>

</fr:mainmatter></fr:tree>
  

</fr:mainmatter></fr:tree>
  

  
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Dynamic Programming">Dynamic Programming</fr:title></fr:frontmatter><fr:mainmatter>
  
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter><html:strong>Value Iteration as Linear Algebra:</html:strong> During my brief stint at <fr:link href="https://www.ipam.ucla.edu/" type="external">IPAM</fr:link>, I spent a lot of time focused on <fr:link href="/kanariosdas2023/" title="Parallel Algebraic Multigrid for Higher Order PDEs" uri="https://kkanarios32.github.io/kanariosdas2023/" display-uri="kanariosdas2023" type="local">solving large systems for discretized PDEs</fr:link>. From this, I learned about iterative methods, such as Gauss-Jacobi and Gauss-Seidel. Interestingly, these methods can be used to interpret some of RL's most fundamental algorithms. If we define the following matrices,
<fr:tex display="block"><![CDATA[
  \begin {align*}
        R &= \begin {bmatrix}
        \mathbb {E}[r(s = 1)]\\
        \vdots  \\
        \mathbb {E}[r(s = n)]
        \end {bmatrix}, \quad  P = \begin {bmatrix}
        P_{\pi }(s'=1 | s=1) & \cdots  & P_{\pi }(s'=n | s=1) \\
        \vdots  & \ddots  & \vdots  \\
        P_{\pi }(s'=1 | s=n) & \cdots  & P_{\pi }(s'=n | s=n)
        \end {bmatrix}, \\  
        V_k &= \begin {bmatrix}
        V_k(s = 1)\\
        \vdots  \\
        V_k(s = n)
        \end {bmatrix}
  \end {align*}
      ]]></fr:tex>
      Then we can write the value function <fr:tex display="inline"><![CDATA[V_{\pi }]]></fr:tex> as the solution to the linear system
      <fr:tex display="block"><![CDATA[
          V_{\pi } = R + PV_{\pi } \iff  V_{\pi } = (1 - P)^{-1} R
        ]]></fr:tex>
    This is just inverting a matrix! Updating one state with the canonical update
<fr:tex display="block"><![CDATA[
    \begin {align*}
      v_{k + 1}(s) = \sum _{a}\pi (a|s)\sum _{s^{\prime },r}p(s^{\prime },r\,|\,s,a)\Big [r+\gamma  v_{k}(s^{\prime })\Big ]
    \end {align*}
]]></fr:tex>
Is actually just one iteration of Gauss-Seidel!
    </fr:mainmatter></fr:tree>

    
    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>(Number of updates as pseudo discount factor). </fr:mainmatter></fr:tree>

  
  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 4.1]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="4.1" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 4.1]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
    We have that
<fr:tex display="block"><![CDATA[
      \begin {align*}
        q_{\pi }(11, \mathrm {down}) &= -1,
      \end {align*}
    ]]></fr:tex>
    where there is no recursion because the episode ends.
    Next,
<fr:tex display="block"><![CDATA[
      \begin {align*}
        q_{\pi }(7, \mathrm {down}) &= -1 + v(11) \\
        &= -1 - 14 \\ 
        &= -15
      \end {align*}
    ]]></fr:tex>
  
</fr:mainmatter></fr:tree>
  

  
  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 4.3]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="4.3" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 4.3]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  
</fr:mainmatter></fr:tree>
  


</fr:mainmatter></fr:tree>
  

  
  

  
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Review Question">Review Question</fr:title></fr:frontmatter><fr:mainmatter>
  
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:taxon>Problem</fr:taxon></fr:frontmatter><fr:mainmatter>Review the methods discussed in the first part of the book. What are their strengths and weaknesses? When should they be used?</fr:mainmatter></fr:tree>

  <html:span style="white-space: nowrap">
    
 
   
   <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
      
  <html:table>
        
  <html:tr>
          
  <html:td />

          
  <html:td><html:strong>Strength</html:strong></html:td>

          
  <html:td><html:strong>Weakness</html:strong></html:td>

          
  <html:td><html:strong>Use case</html:strong></html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>Bandits</html:strong></html:td>

          
  <html:td>Simplest case. Can isolate exploration problem.</html:td>

          
  <html:td>No credit assignment. Action only effect next timestep.</html:td>

          
  <html:td>Clinical trials. Things without temporally extended outcomes.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>Dynamic Programming</html:strong></html:td>

          
  <html:td>Exact solution.</html:td>

          
  <html:td>Complexity blows up with state and action space. Model-based.</html:td>

          
  <html:td>Tabular MDPs.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>Monte Carlo</html:strong></html:td>

          
  <html:td>Unbiased estimator. Model-free.</html:td>

          
  <html:td>Extremely high variance. Must wait till end of episode.</html:td>

          
  <html:td>When interacting with environment is inexpensive, episode length is short.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>TD methods</html:strong></html:td>

          
  <html:td>Model-free. Online.</html:td>

          
  <html:td>Biased estimator.</html:td>

          
  <html:td>Environment interaction is cheap but also episode length is long or continuing.</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>N step</html:strong></html:td>

          
  <html:td>Balance bias-variance tradeoff.</html:td>

          
  <html:td>Must choose <fr:tex display="inline"><![CDATA[n]]></fr:tex>.</html:td>

          
  <html:td>Lower variance continuing environments?</html:td>

        </html:tr>

        
  <html:tr>
          
  <html:td><html:strong>Dyna</html:strong></html:td>

          
  <html:td>Sample efficiency.</html:td>

          
  <html:td>Model-based.</html:td>

          
  <html:td>When interacting with environment is costly i.e. driving.</html:td>

        </html:tr>

      </html:table>

    </fr:mainmatter></fr:tree>
 

  </html:span>
</fr:mainmatter></fr:tree>
  


  
    <fr:tree show-metadata="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Planning and Learning with Tabular Methods">Planning and Learning with Tabular Methods</fr:title></fr:frontmatter><fr:mainmatter>

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 8.1]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="8.1" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 8.1]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  If you pick <fr:tex display="inline"><![CDATA[n]]></fr:tex> large enough, then the "first pass" would be just as good. This means that you will be able to update the <fr:tex display="inline"><![CDATA[Q]]></fr:tex>-values of every <fr:tex display="inline"><![CDATA[(s,a)]]></fr:tex> pair you encountered. However, with Dyna you are updating your <fr:tex display="inline"><![CDATA[Q]]></fr:tex>-value based on updated <fr:tex display="inline"><![CDATA[Q]]></fr:tex>-values, meaning that you are no longer limited to the <fr:tex display="inline"><![CDATA[n]]></fr:tex>-step trajectory encountered. Thus, I do not think that <fr:tex display="inline"><![CDATA[n]]></fr:tex>-step methods can match Dyna with a perfect world model.

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 8.5]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="8.5" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 8.5]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  You could modify the algorithm by maintaining an empirical distribution for states and rewards in your model instead of the exact deterministic transition. To handle changing environments, you could then weight the updates to the empirical distribution to value the more recent entries higher i.e. 
  <fr:tex display="block"><![CDATA[\hat {R}_{t + 1}(S,A) = \lambda  \hat {R}_{t}(S, A) + (1 - \lambda )R_{t + 1}(S, A)]]></fr:tex>

</fr:mainmatter></fr:tree>
  


  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 8.6]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="8.6" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 8.6]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  This would strengthen the case for sample updates because the samples would most likely concentrate around the higher probability states. Since these states are much more likely to occur, the computed <fr:tex display="inline"><![CDATA[Q]]></fr:tex>-values will likely end up very similar without the extra computation required by the expected update.

</fr:mainmatter></fr:tree>
  



  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors><fr:author><fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>24</fr:day></fr:date><fr:title text="Notes on Reinforcement Learning: An Introduction ›  [sutton2022, 8.7]"><fr:link href="/0012/" title="Notes on Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/0012/" display-uri="0012" type="local">Notes on <fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">Reinforcement Learning: An Introduction</fr:link></fr:link> ›  <html:span class="link-reference" tid="8.7" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 8.7]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  


</fr:mainmatter></fr:tree>
  

</fr:mainmatter></fr:tree>
  
<fr:tree show-metadata="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:uri>https://kkanarios32.github.io/000G/</fr:uri><fr:display-uri>000G</fr:display-uri><fr:route>/000G/</fr:route><fr:title text="On-policy Prediction with Approximation">On-policy Prediction with Approximation</fr:title></fr:frontmatter><fr:mainmatter><html:p>
In my opinion, the most important part of this chapter is now that when we update say a <fr:tex display="inline"><![CDATA[Q]]></fr:tex> function for a specific state action pair <fr:tex display="inline"><![CDATA[(s,a)]]></fr:tex>, then this update can affect the value of the <fr:tex display="inline"><![CDATA[Q(s', a')]]></fr:tex> due to the reuse of the internal parameters.
  </html:p>
  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:title text=" [sutton2022, 9.1]"> <html:span class="link-reference" tid="9.1" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 9.1]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  <html:p>Show that tabular methods are a special case of linear function approximation. What would the feature vectors be?</html:p>

</fr:mainmatter></fr:tree>
  

 
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
<html:p>As a linear function approximation method, we have two quantities <fr:tex display="inline"><![CDATA[\boldsymbol {w}]]></fr:tex> and <fr:tex display="inline"><![CDATA[\boldsymbol {x}(s)]]></fr:tex>. We make the update
<fr:tex display="block"><![CDATA[w_{t + 1} = w_{t} + \alpha (r_t + \gamma  w_{t}^{\top } x_{t+1} - w_t^{\top } x_t) x_t]]></fr:tex>
In the tabular setting, we just assume that we have the capacity to represent every possible state. This means that <fr:tex display="inline"><![CDATA[x(s) \in  \mathbb {R}^{|S|}]]></fr:tex>. Therefore, if we define the features as <fr:tex display="inline"><![CDATA[x : s_i \mapsto  \boldsymbol {e}_i]]></fr:tex> then we can recover policy evaluation by taking,
<fr:tex display="block"><![CDATA[r_i = \mathbb {E}_{a \sim  \pi , s \sim  p}[r(s_i)], \quad  w_i = v_{\pi }(s_i)]]></fr:tex></html:p>
Substituting, we get 
<fr:tex display="block"><![CDATA[
w_{t + 1} = w_{t} + \alpha (\mathbb {E}_{a \sim  \pi , s \sim  p}[r(s_i)] + v(s_{t + 1}) - v(s_{t})) \cdot  \boldsymbol {e}_i
]]></fr:tex>
Since <fr:tex display="inline"><![CDATA[\boldsymbol {w}]]></fr:tex> is our vector of values, updating the <fr:tex display="inline"><![CDATA[i]]></fr:tex>th entry is exactly performing exactly one update to <fr:tex display="inline"><![CDATA[v_{\pi }(s_i)]]></fr:tex> in the tabular setting.
</fr:mainmatter></fr:tree>
 

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:title text=" [sutton2022, 9.2]"> <html:span class="link-reference" tid="9.2" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 9.2]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  <html:p>Why does (9.17) define <fr:tex display="inline"><![CDATA[(n + 1)^k]]></fr:tex> distinct features for dimension k? </html:p>

</fr:mainmatter></fr:tree>
  

 
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  <html:p>For each <fr:tex display="inline"><![CDATA[s_j]]></fr:tex>, there are <fr:tex display="inline"><![CDATA[n + 1]]></fr:tex> options for <fr:tex display="inline"><![CDATA[c_{i, j}]]></fr:tex>. Since there are <fr:tex display="inline"><![CDATA[k]]></fr:tex>, <fr:tex display="inline"><![CDATA[s_{j}]]></fr:tex>'s, there are <fr:tex display="inline"><![CDATA[(n + 1)^k]]></fr:tex> total possible features for <fr:tex display="inline"><![CDATA[x_i]]></fr:tex>.</html:p>
</fr:mainmatter></fr:tree>
 

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:title text=" [sutton2022, 9.3]"> <html:span class="link-reference" tid="9.3" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 9.3]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
<html:p>What <fr:tex display="inline"><![CDATA[n]]></fr:tex> and <fr:tex display="inline"><![CDATA[c_{i,j}]]></fr:tex> produce the feature vectors <fr:tex display="block"><![CDATA[\mathbf {x}(s)=(1,s_{1},s_{2},s_{1}s_{2},s_{1}^{2},s_{2}^{2},s_{1}^{2}s_{2}^{2},s_{1}s_{2}^{2},s_{1}^{2}s_{2}^{2})^{\top }?]]></fr:tex></html:p>

</fr:mainmatter></fr:tree>
  

 
   
   <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Solution</fr:taxon></fr:frontmatter><fr:mainmatter>
  <html:p><fr:tex display="inline"><![CDATA[n = 2]]></fr:tex> and <fr:tex display="inline"><![CDATA[c_{ij}]]></fr:tex> as 
<fr:tex display="block"><![CDATA[\boldsymbol {c_0} = [0, 0], \quad 
  \boldsymbol {c_1} = [1, 0] \\
  \boldsymbol {c_2} = [0, 1], \quad 
  \boldsymbol {c_3} = [1, 1] \\
  \boldsymbol {c_4} = [2, 0], \quad 
  \boldsymbol {c_5} = [0, 2] \\
  \boldsymbol {c_6} = [2, 2], \quad 
  \boldsymbol {c_7} = [1, 2], \quad 
  \boldsymbol {c_8} = [2, 2]
  ]]></fr:tex></html:p>
</fr:mainmatter></fr:tree>
 

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  <html:ol><html:li>In section 9.5.2, what do they mean when they say you can select <fr:tex display="inline"><![CDATA[n]]></fr:tex> so all the fourier features can be used?
    <html:ol><html:li>Pick <fr:tex display="inline"><![CDATA[n]]></fr:tex> so that <fr:tex display="inline"><![CDATA[(n + 1)^k < mk^2]]></fr:tex>.</html:li>
        <html:li>Pick <fr:tex display="inline"><![CDATA[n]]></fr:tex> so that <fr:tex display="inline"><![CDATA[(n + 1)^k]]></fr:tex> is reasonable.</html:li></html:ol></html:li>
    <html:p>
    In the tabular case, I think (a) is correct. My initial understanding of state representation is as a representation learning i.e. compression type objective. If we assume that <fr:tex display="inline"><![CDATA[s_i \in  [m]]]></fr:tex>, then we do not gain anything in the tabular setting if our value function vector is the same size as the underlying transition kernel. When in the continuous state space regime i.e. <fr:tex display="inline"><![CDATA[s_i \in  [0,1]]]></fr:tex>, there is no amount of features that would overfit the transition kernel. Therefore, it is just about trying to learn as much as possible about the underlying relations of the state dimensions.
    </html:p>
    <html:p>
    In the <fr:tex display="inline"><![CDATA[2]]></fr:tex>-dimensional case, the feature vector <fr:tex display="inline"><![CDATA[c_i = [1,1]]]></fr:tex> would provide information on how <fr:tex display="inline"><![CDATA[s_1]]></fr:tex> and <fr:tex display="inline"><![CDATA[s_2]]></fr:tex> relate not just their specific values. You want the learning algorithm to understand the relationships, so that it can make better use of the internal parameters when learning something like a <fr:tex display="inline"><![CDATA[Q]]></fr:tex> function. This is basically just SVD but instead of classification you are learning a <fr:tex display="inline"><![CDATA[Q]]></fr:tex> function. The compression  
    </html:p></html:ol>
  </fr:mainmatter></fr:tree>

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Question</fr:taxon></fr:frontmatter><fr:mainmatter>
  <html:ol><html:li>In Figure 9.5, why do fourier features outperform polynomial features?
    <html:ol><html:li>The fourier features got lucky on the seeds (lol).</html:li>
        <html:li>The choice of <fr:tex display="inline"><![CDATA[\boldsymbol {c}]]></fr:tex> is not specified. A good choice can provide improvement specific to the problem?</html:li>
        <html:li>Polynomial features range is very large. Can suffer from blowup or vanishing of features.</html:li></html:ol></html:li>
    One of the advantages of fourier features mentioned previously is the ability to select which features to serve as your basis. However, for this setup I assume they just use all of the fourier features. This likely means it is more of an issue with the polynomial features and (c). If you have a large polynomial then even relative similar states <fr:tex display="inline"><![CDATA[s_1 = 1.1]]></fr:tex>, <fr:tex display="inline"><![CDATA[s_2 = 0.9]]></fr:tex>, <fr:tex display="inline"><![CDATA[\ldots ]]></fr:tex> can blow up or vanish making them likely more unstable when using gradient-based methods.
  </html:ol>
  </fr:mainmatter></fr:tree>

  
    
    
    <fr:tree show-metadata="false" toc="false" numbered="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:title text=" [sutton2022, 9.4]"> <html:span class="link-reference" tid="9.4" refid="sutton2022"><fr:link href="/sutton2022/" title="Reinforcement Learning: An Introduction" uri="https://kkanarios32.github.io/sutton2022/" display-uri="sutton2022" type="local">[sutton2022, 9.4]</fr:link></html:span></fr:title><fr:taxon>exercise</fr:taxon></fr:frontmatter><fr:mainmatter>
  
  <html:p>
    You could do anisotropic (big word for asymmetric) tile partitioning. If we consider the two state dimensions as (x,y) coordinates and suppose that we want to only generalize across the <fr:tex display="inline"><![CDATA[y]]></fr:tex>-direction i.e. we want states with the same <fr:tex display="inline"><![CDATA[x]]></fr:tex> coordinate to have similar values then we would tile with long thin tiles. Therefore, states with the same <fr:tex display="inline"><![CDATA[x]]></fr:tex> coordinate would lie in the same vertical tile and if the tiles are very thin any change in <fr:tex display="inline"><![CDATA[x]]></fr:tex>-coordinate would lie in disjoint tiles.
  </html:p>

</fr:mainmatter></fr:tree>
  

    <fr:tree show-metadata="false" toc="false"><fr:frontmatter><fr:authors /><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>5</fr:day></fr:date><fr:taxon>Remark</fr:taxon></fr:frontmatter><fr:mainmatter>I think that RBF as a continuous generalization coarse-coding is a nice intuition I want to remember here. Essentially, you just weight a state by how close it is to the center of the receptive field. This weighting is done via a Gaussian kernel, which I believe is arbitrary and can be any distance measure of choice.</fr:mainmatter></fr:tree>
</fr:mainmatter></fr:tree></fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/kellenkanarios/" title="Kellen Kanarios" uri="https://kkanarios32.github.io/kellenkanarios/" display-uri="kellenkanarios" type="local">Kellen Kanarios</fr:link>
              </fr:author>
            </fr:authors>
            <fr:date>
              <fr:year>2023</fr:year>
              <fr:month>8</fr:month>
              <fr:day>20</fr:day>
            </fr:date>
            <fr:uri>https://kkanarios32.github.io/kanariosdas2023/</fr:uri>
            <fr:display-uri>kanariosdas2023</fr:display-uri>
            <fr:route>/kanariosdas2023/</fr:route>
            <fr:title text="Parallel Algebraic Multigrid for Higher Order PDEs">Parallel Algebraic Multigrid for Higher Order PDEs</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="external">slides/amg_slides.pdf</fr:meta>
            <fr:meta name="external">posters/amg_poster.pptx</fr:meta>
            <fr:meta name="doi">https://www.osti.gov/servlets/purl/2205732/</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors>
              <fr:author>
                <fr:link href="/richardsutton/" title="Richard Sutton" uri="https://kkanarios32.github.io/richardsutton/" display-uri="richardsutton" type="local">Richard Sutton</fr:link>
              </fr:author>
            </fr:authors>
            <fr:uri>https://kkanarios32.github.io/sutton2022/</fr:uri>
            <fr:display-uri>sutton2022</fr:display-uri>
            <fr:route>/sutton2022/</fr:route>
            <fr:title text="Reinforcement Learning: An Introduction">Reinforcement Learning: An Introduction</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="external">http://incompleteideas.net/book/the-book-2nd.html</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kkanarios32.github.io/richardsutton/</fr:uri>
            <fr:display-uri>richardsutton</fr:display-uri>
            <fr:route>/richardsutton/</fr:route>
            <fr:title text="Richard Sutton">Richard Sutton</fr:title>
            <fr:taxon>Person</fr:taxon>
            <fr:meta name="external">http://incompleteideas.net</fr:meta>
            <fr:meta name="institution">
              <fr:link href="https://www.ualberta.ca/en/index.html" type="external">University of Alberta</fr:link>
            </fr:meta>
            <fr:meta name="position">Professor</fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
