<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>Yihua Zhang</fr:author>
      <fr:author>Pingzhi Li</fr:author>
      <fr:author>Junyuan Hong</fr:author>
      <fr:author>Jiaxiang Li</fr:author>
      <fr:author>Yimeng Zhang</fr:author>
      <fr:author>Wenqing Zheng</fr:author>
      <fr:author>Pin-Yu Chen</fr:author>
      <fr:author>Jason D. Lee</fr:author>
      <fr:author>Wotao Yin</fr:author>
      <fr:author>Mingyi Hong</fr:author>
      <fr:author>Zhangyang Wang</fr:author>
      <fr:author>Sijia Liu</fr:author>
      <fr:author>Tianlong Chen</fr:author>
    </fr:authors>
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>5</fr:month>
    </fr:date>
    <fr:uri>https://kellenkanarios.com/zhangRevisitingZerothOrderOptimization2024/</fr:uri>
    <fr:display-uri>zhangRevisitingZerothOrderOptimization2024</fr:display-uri>
    <fr:route>/zhangRevisitingZerothOrderOptimization2024/</fr:route>
    <fr:title text="Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="doi">10.48550/arXiv.2402.11592</fr:meta>
    <fr:meta name="external">https://arxiv.org/abs/2402.11592</fr:meta>
    <fr:meta name="bibtex"><![CDATA[@misc{zhangRevisitingZerothOrderOptimization2024,
 title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}: {{A Benchmark}}},
 author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},
 year = {2024},
 doi = {10.48550/arXiv.2402.11592},
 urldate = {2024-09-08},
 number = {arXiv:2402.11592},
 publisher = {arXiv},
 file = {/home/kellen/Downloads/pdfs/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A Benchmark.pdf},
 keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow \{in size\}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .},
 primaryclass = {cs},
 eprint = {2402.11592},
 month = {May},
 shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
