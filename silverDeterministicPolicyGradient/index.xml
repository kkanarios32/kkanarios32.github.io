<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/David%20Silver/" type="external">David Silver</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Guy%20Lever/" type="external">Guy Lever</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Nicolas%20Heess/" type="external">Nicolas Heess</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Thomas%20Degris/" type="external">Thomas Degris</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Daan%20Wierstra/" type="external">Daan Wierstra</fr:link>
      </fr:author>
      <fr:author>
        <fr:link href="https://kkanarios32.github.io/Martin%20Riedmiller/" type="external">Martin Riedmiller</fr:link>
      </fr:author>
    </fr:authors>
    <fr:uri>https://kkanarios32.github.io/silverDeterministicPolicyGradient/</fr:uri>
    <fr:display-uri>silverDeterministicPolicyGradient</fr:display-uri>
    <fr:route>/silverDeterministicPolicyGradient/</fr:route>
    <fr:title text="Deterministic Policy Gradient Algorithms">Deterministic Policy Gradient Algorithms</fr:title>
    <fr:taxon>Reference</fr:taxon>
    <fr:meta name="bibtex"><![CDATA[@article{silverDeterministicPolicyGradient,
 title = {Deterministic {{Policy Gradient Algorithms}}},
 author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
 file = {/home/kellen/Zotero/storage/HYF79JZL/Silver et al. - Deterministic Policy Gradient Algorithms.pdf},
 langid = {english},
 abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.}
}]]></fr:meta>
  </fr:frontmatter>
  <fr:mainmatter />
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
