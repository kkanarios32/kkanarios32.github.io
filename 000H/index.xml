<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/default.xsl"?>
<fr:tree xmlns:fr="http://www.forester-notes.org" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:xml="http://www.w3.org/XML/1998/namespace" root="false" base-url="/">
  <fr:frontmatter>
    <fr:authors />
    <fr:date>
      <fr:year>2024</fr:year>
      <fr:month>11</fr:month>
      <fr:day>6</fr:day>
    </fr:date>
    <fr:uri>https://kkanarios32.github.io/000H/</fr:uri>
    <fr:display-uri>000H</fr:display-uri>
    <fr:route>/000H/</fr:route>
    <fr:title text="Papers of a Week">Papers of a Week</fr:title>
  </fr:frontmatter>
  <fr:mainmatter>
    <html:p>I am committing to reading (fairly rigorously) one paper a week. Here I will accumulate my notes and thoughts on each of these papers as I go.</html:p>
    <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
      <fr:frontmatter>
        <fr:authors />
        <fr:date>
          <fr:year>2024</fr:year>
          <fr:month>11</fr:month>
          <fr:day>6</fr:day>
        </fr:date>
        <fr:uri>https://kkanarios32.github.io/000J/</fr:uri>
        <fr:display-uri>000J</fr:display-uri>
        <fr:route>/000J/</fr:route>
        <fr:title text="Constrastive Learning as Goal Conditioned Reinforcement Learning">
          <fr:link href="/eysenbach2023/" title="Constrastive Learning as Goal Conditioned Reinforcement Learning" uri="https://kkanarios32.github.io/eysenbach2023/" display-uri="eysenbach2023" type="local">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:link>
        </fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
      <fr:frontmatter>
        <fr:authors />
        <fr:date>
          <fr:year>2024</fr:year>
          <fr:month>11</fr:month>
          <fr:day>6</fr:day>
        </fr:date>
        <fr:uri>https://kkanarios32.github.io/000I/</fr:uri>
        <fr:display-uri>000I</fr:display-uri>
        <fr:route>/000I/</fr:route>
        <fr:title text="Near Optimal Representation Learning for Hierarchical Reinforcement Learning">
          <fr:link href="/nachum2019/" title="Near Optimal Representation Learning for Hierarchical Reinforcement Learning" uri="https://kkanarios32.github.io/nachum2019/" display-uri="nachum2019" type="local">Near Optimal Representation Learning for Hierarchical Reinforcement Learning</fr:link>
        </fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
      <fr:frontmatter>
        <fr:authors />
        <fr:date>
          <fr:year>2024</fr:year>
          <fr:month>11</fr:month>
          <fr:day>6</fr:day>
        </fr:date>
        <fr:uri>https://kkanarios32.github.io/000K/</fr:uri>
        <fr:display-uri>000K</fr:display-uri>
        <fr:route>/000K/</fr:route>
        <fr:title text="Contrastive Difference Predictive Coding">
          <fr:link href="/zheng2024/" title="Contrastive Difference Predictive Coding" uri="https://kkanarios32.github.io/zheng2024/" display-uri="zheng2024" type="local">Contrastive Difference Predictive Coding</fr:link>
        </fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:mainmatter>
  <fr:backmatter>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="References">References</fr:title>
      </fr:frontmatter>
      <fr:mainmatter>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kkanarios32.github.io/eysenbach2023/</fr:uri>
            <fr:display-uri>eysenbach2023</fr:display-uri>
            <fr:route>/eysenbach2023/</fr:route>
            <fr:title text="Constrastive Learning as Goal Conditioned Reinforcement Learning">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="abstract">
In reinforcement learning (RL), it is easier to solve a task if
              given a good representation. While deep RL should automatically
              acquire such good representations, prior work often finds that
              learning representations in an end-to-end fashion is unstable and
              instead equip RL algorithms with additional representation learning
              parts (e.g., auxiliary losses, data augmentation). How can we
              design RL algorithms that directly acquire good representations? In
              this paper, instead of adding representation learning parts to an
              existing RL algorithm, we show (contrastive) representation
              learning methods can be cast as RL algorithms in their own right.
              To do this, we build upon prior work and apply contrastive
              representation learning to action-labeled trajectories, in such a
              way that the (inner product of) learned representations exactly
              corresponds to a goal-conditioned value function. We use this idea
              to reinterpret a prior RL method as performing contrastive learning
              , and then use the idea to propose a much simpler method that
              achieves similar performance. Across a range of goal-conditioned RL
              tasks, we demonstrate that contrastive RL methods achieve higher
              success rates than prior non-contrastive methods, including in the
              offline RL setting. We also show that contrastive RL outperforms
              prior methods on image-based tasks, without using data augmentation
              or auxiliary objectives.
  </fr:meta>
            <fr:meta name="doi">10.48550/arXiv.2206.07568</fr:meta>
            <fr:meta name="bibtex"><![CDATA[@misc{eysenbachContrastiveLearningGoalConditioned2023,
  title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning
           }}},
  author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and
            Levine, Sergey},
  year = {2023},
  month = feb,
  number = {arXiv:2206.07568},
  eprint = {2206.07568},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-06},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning},
  file = {/home/kellen/Zotero/storage/WVQKIMN4/Eysenbach et al. - 2023 -
          Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kkanarios32.github.io/zheng2024/</fr:uri>
            <fr:display-uri>zheng2024</fr:display-uri>
            <fr:route>/zheng2024/</fr:route>
            <fr:title text="Contrastive Difference Predictive Coding">Contrastive Difference Predictive Coding</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="bibtex"><![CDATA[@misc{zheng2024contrastivedifferencepredictivecoding,
      title={Contrastive Difference Predictive Coding}, 
      author={Chongyi Zheng and Ruslan Salakhutdinov and Benjamin Eysenbach},
      year={2024},
      eprint={2310.20141},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.20141}, 
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
        <fr:tree show-metadata="true" expanded="false" toc="false" numbered="false">
          <fr:frontmatter>
            <fr:authors />
            <fr:uri>https://kkanarios32.github.io/nachum2019/</fr:uri>
            <fr:display-uri>nachum2019</fr:display-uri>
            <fr:route>/nachum2019/</fr:route>
            <fr:title text="Near Optimal Representation Learning for Hierarchical Reinforcement Learning">Near Optimal Representation Learning for Hierarchical Reinforcement Learning</fr:title>
            <fr:taxon>Reference</fr:taxon>
            <fr:meta name="bibtex"><![CDATA[@misc{nachum2019nearoptimalrepresentationlearninghierarchical,
      title={Near-Optimal Representation Learning for Hierarchical Reinforcement Learning}, 
      author={Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},
      year={2019},
      eprint={1810.01257},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1810.01257}, 
}]]></fr:meta>
          </fr:frontmatter>
          <fr:mainmatter />
        </fr:tree>
      </fr:mainmatter>
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Context">Context</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Backlinks">Backlinks</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Related">Related</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
    <fr:tree show-metadata="false" hidden-when-empty="true">
      <fr:frontmatter>
        <fr:authors />
        <fr:title text="Contributions">Contributions</fr:title>
      </fr:frontmatter>
      <fr:mainmatter />
    </fr:tree>
  </fr:backmatter>
</fr:tree>
